From 3c4b0f245ebfae2833897df07ca12c6bb9dcf916 Mon Sep 17 00:00:00 2001
From: 7a72 <git@zrlab.org>
Date: Mon, 29 Sep 2025 16:16:25 +0000
Subject: [PATCH 2/3] FROMGIT: treewide: Import oplus module

Co-Authored-by: pswbuild <pswbuild@oppo.com>
---
 drivers/soc/Kconfig                           |     2 +
 drivers/soc/Makefile                          |     2 +
 drivers/soc/oplus/oplus_resctrl/Kbuild        |     8 +
 drivers/soc/oplus/oplus_resctrl/Kconfig       |    14 +
 drivers/soc/oplus/oplus_resctrl/Makefile      |     8 +
 drivers/soc/oplus/oplus_resctrl/iocost.c      |   165 +
 drivers/soc/oplus/oplus_resctrl/resctrl.c     |   232 +
 drivers/soc/oplus/oplus_resctrl/resctrl.h     |   176 +
 drivers/soc/oplus/storage/Kconfig             |     8 +
 drivers/soc/oplus/storage/Makefile            |     5 +
 drivers/soc/oplus/storage/README.md           |    14 +
 drivers/soc/oplus/storage/bp/OplusStorage.h   |    34 +
 .../soc/oplus/storage/bp/OplusStorageDxe.c    |   144 +
 .../soc/oplus/storage/bp/OplusStorageDxe.h    |    84 +
 .../soc/oplus/storage/bp/OplusStorageDxe.inf  |   103 +
 drivers/soc/oplus/storage/common/README.md    |     2 +
 .../soc/oplus/storage/common/fmonitor/Kconfig |     6 +
 .../oplus/storage/common/fmonitor/Makefile    |     7 +
 .../common/fmonitor/oplus_file_monitor.c      |   326 +
 .../oplus/storage/common/io_metrics/Kconfig   |    19 +
 .../oplus/storage/common/io_metrics/Makefile  |    27 +
 .../storage/common/io_metrics/abnormal_io.c   |   896 ++
 .../storage/common/io_metrics/abnormal_io.h   |    15 +
 .../storage/common/io_metrics/block_metrics.c |   666 +
 .../storage/common/io_metrics/block_metrics.h |    73 +
 .../storage/common/io_metrics/f2fs_metrics.c  |   431 +
 .../storage/common/io_metrics/f2fs_metrics.h  |    12 +
 .../common/io_metrics/io_metrics_entry.c      |    73 +
 .../common/io_metrics/io_metrics_entry.h      |    94 +
 .../oplus/storage/common/io_metrics/procfs.c  |   492 +
 .../oplus/storage/common/io_metrics/procfs.h  |    10 +
 .../storage/common/io_metrics/ufs_metrics.c   |   259 +
 .../storage/common/io_metrics/ufs_metrics.h   |    12 +
 drivers/soc/oplus/storage/include/storage.h   |    11 +
 .../storage/storage_feature_in_module/Kconfig |     5 +
 .../storage_feature_in_module/Makefile        |     6 +
 .../common/oplus_f2fslog_storage/Kconfig      |     7 +
 .../common/oplus_f2fslog_storage/Makefile     |     5 +
 .../oplus_f2fslog_storage.c                   |   229 +
 .../common/oplus_uprobe/Kconfig               |     7 +
 .../common/oplus_uprobe/Makefile              |     7 +
 .../common/oplus_uprobe/oplus_uprobe.c        |  1296 ++
 .../common/oplus_uprobe/oplus_uprobe.h        |     4 +
 .../common/storage_log/Kconfig                |     7 +
 .../common/storage_log/Makefile               |     3 +
 .../common/storage_log/storage_log.c          |   240 +
 .../common/ufs_oplus_dbg/Kconfig              |     9 +
 .../common/ufs_oplus_dbg/Makefile             |     2 +
 .../common/ufs_oplus_dbg/ufs-oplus-dbg.c      |  1215 ++
 .../common/ufs_oplus_dbg/ufs-oplus-dbg.h      |   209 +
 .../common/wq_dynamic_priority/Kconfig        |     6 +
 .../common/wq_dynamic_priority/Makefile       |     5 +
 .../oplus_wq_dynamic_priority.c               |   236 +
 .../oplus_wq_dynamic_priority.h               |   174 +
 fs/f2fs/Kconfig                               |    26 +
 fs/f2fs/Makefile                              |     3 +
 fs/f2fs/checkpoint.c                          |    45 +-
 fs/f2fs/compress.c                            |   988 +-
 fs/f2fs/data.c                                |   604 +-
 fs/f2fs/dir.c                                 |    69 +-
 fs/f2fs/extent_cache.c                        |    55 +-
 fs/f2fs/f2fs.h                                |   764 +-
 fs/f2fs/file.c                                | 10350 +++++++++++-----
 fs/f2fs/gc.c                                  |     6 +-
 fs/f2fs/inline.c                              |     5 +-
 fs/f2fs/inode.c                               |   165 +-
 fs/f2fs/namei.c                               |   119 +-
 fs/f2fs/node.c                                |    66 +-
 fs/f2fs/node.h                                |     4 +
 fs/f2fs/recovery.c                            |    12 +-
 fs/f2fs/segment.c                             |   336 +-
 fs/f2fs/segment.h                             |    75 +-
 fs/f2fs/super.c                               |   290 +-
 fs/f2fs/sysfs.c                               |   316 +-
 fs/f2fs/verity.c                              |     9 +
 fs/f2fs/xattr.c                               |    17 +-
 fs/f2fs/xattr.h                               |     1 +
 include/linux/f2fs_fs.h                       |    14 +-
 include/trace/events/f2fs.h                   |   136 +-
 include/uapi/linux/f2fs.h                     |   124 +
 include/uapi/linux/stat.h                     |     1 +
 kernel/locking/Makefile                       |     2 +-
 kernel/locking/locking_main.h                 |   165 +
 kernel/locking/oplus_locking.c                |   885 ++
 kernel/locking/sa_common_struct.h             |   181 +
 85 files changed, 19911 insertions(+), 4024 deletions(-)
 create mode 100644 drivers/soc/oplus/oplus_resctrl/Kbuild
 create mode 100644 drivers/soc/oplus/oplus_resctrl/Kconfig
 create mode 100644 drivers/soc/oplus/oplus_resctrl/Makefile
 create mode 100644 drivers/soc/oplus/oplus_resctrl/iocost.c
 create mode 100644 drivers/soc/oplus/oplus_resctrl/resctrl.c
 create mode 100644 drivers/soc/oplus/oplus_resctrl/resctrl.h
 create mode 100644 drivers/soc/oplus/storage/Kconfig
 create mode 100644 drivers/soc/oplus/storage/Makefile
 create mode 100644 drivers/soc/oplus/storage/README.md
 create mode 100644 drivers/soc/oplus/storage/bp/OplusStorage.h
 create mode 100644 drivers/soc/oplus/storage/bp/OplusStorageDxe.c
 create mode 100644 drivers/soc/oplus/storage/bp/OplusStorageDxe.h
 create mode 100644 drivers/soc/oplus/storage/bp/OplusStorageDxe.inf
 create mode 100644 drivers/soc/oplus/storage/common/README.md
 create mode 100644 drivers/soc/oplus/storage/common/fmonitor/Kconfig
 create mode 100644 drivers/soc/oplus/storage/common/fmonitor/Makefile
 create mode 100644 drivers/soc/oplus/storage/common/fmonitor/oplus_file_monitor.c
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/Kconfig
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/Makefile
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/abnormal_io.c
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/abnormal_io.h
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/block_metrics.c
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/block_metrics.h
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/f2fs_metrics.c
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/f2fs_metrics.h
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/io_metrics_entry.c
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/io_metrics_entry.h
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/procfs.c
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/procfs.h
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/ufs_metrics.c
 create mode 100644 drivers/soc/oplus/storage/common/io_metrics/ufs_metrics.h
 create mode 100644 drivers/soc/oplus/storage/include/storage.h
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/Kconfig
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/Makefile
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/Kconfig
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/Makefile
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/oplus_f2fslog_storage.c
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/Kconfig
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/Makefile
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/oplus_uprobe.c
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/oplus_uprobe.h
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/Kconfig
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/Makefile
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/storage_log.c
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/Kconfig
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/Makefile
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/ufs-oplus-dbg.c
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/ufs-oplus-dbg.h
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/Kconfig
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/Makefile
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/oplus_wq_dynamic_priority.c
 create mode 100644 drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/oplus_wq_dynamic_priority.h
 create mode 100644 kernel/locking/locking_main.h
 create mode 100644 kernel/locking/oplus_locking.c
 create mode 100644 kernel/locking/sa_common_struct.h

diff --git a/drivers/soc/Kconfig b/drivers/soc/Kconfig
index e461c0711..8aac6ceba 100644
--- a/drivers/soc/Kconfig
+++ b/drivers/soc/Kconfig
@@ -27,5 +27,7 @@ source "drivers/soc/ti/Kconfig"
 source "drivers/soc/ux500/Kconfig"
 source "drivers/soc/versatile/Kconfig"
 source "drivers/soc/xilinx/Kconfig"
+source "drivers/soc/oplus/oplus_resctrl/Kconfig"
+source "drivers/soc/oplus/storage/Kconfig"
 
 endmenu
diff --git a/drivers/soc/Makefile b/drivers/soc/Makefile
index 69ba6508c..ea17e8b27 100644
--- a/drivers/soc/Makefile
+++ b/drivers/soc/Makefile
@@ -33,3 +33,5 @@ obj-y				+= ti/
 obj-$(CONFIG_ARCH_U8500)	+= ux500/
 obj-$(CONFIG_PLAT_VERSATILE)	+= versatile/
 obj-y				+= xilinx/
+obj-y				+= oplus/oplus_resctrl/
+obj-y				+= oplus/storage/
diff --git a/drivers/soc/oplus/oplus_resctrl/Kbuild b/drivers/soc/oplus/oplus_resctrl/Kbuild
new file mode 100644
index 000000000..336b96298
--- /dev/null
+++ b/drivers/soc/oplus/oplus_resctrl/Kbuild
@@ -0,0 +1,8 @@
+obj-$(CONFIG_OPLUS_RESCTRL) += oplus_resctrl.o
+
+oplus_resctrl-y := resctrl.o
+oplus_resctrl-$(CONFIG_OPLUS_RESCTRL) += iocost.o
+
+ifeq ($(OPLUS_OUT_OF_TREE_KO),y)
+ccflags-y += -DCONFIG_OPLUS_RESCTRL=1
+endif
diff --git a/drivers/soc/oplus/oplus_resctrl/Kconfig b/drivers/soc/oplus/oplus_resctrl/Kconfig
new file mode 100644
index 000000000..9b4a60470
--- /dev/null
+++ b/drivers/soc/oplus/oplus_resctrl/Kconfig
@@ -0,0 +1,14 @@
+config OPLUS_RESCTRL
+	tristate "oplus resctrl module"
+	depends on BLOCK
+	default y
+	help
+	  Support oplus resctrl
+
+config OPLUS_RESCTRL_DEBUG
+	bool "oplus resctrl debug"
+	depends on OPLUS_RESCTRL
+	default n
+	help
+	  use oplus resctrl debug log
+
diff --git a/drivers/soc/oplus/oplus_resctrl/Makefile b/drivers/soc/oplus/oplus_resctrl/Makefile
new file mode 100644
index 000000000..56a05a66c
--- /dev/null
+++ b/drivers/soc/oplus/oplus_resctrl/Makefile
@@ -0,0 +1,8 @@
+KBUILD_OPTIONS += CONFIG_OPLUS_RESCTRL=m
+KBUILD_OPTIONS += OPLUS_OUT_OF_TREE_KO=y
+
+KERNEL_SRC ?= /lib/modules/$(shell uname -r)/build
+M ?= $(shell pwd)
+
+modules modules_install clean:
+	$(MAKE) -C $(KERNEL_SRC) M=$(M) $(KBUILD_OPTIONS) $(@)
diff --git a/drivers/soc/oplus/oplus_resctrl/iocost.c b/drivers/soc/oplus/oplus_resctrl/iocost.c
new file mode 100644
index 000000000..2e14b1faa
--- /dev/null
+++ b/drivers/soc/oplus/oplus_resctrl/iocost.c
@@ -0,0 +1,165 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+* Copyright (C) 2023 Oplus. All rights reserved.
+*/
+#include "resctrl.h"
+
+#if LINUX_KERNEL_601
+#include <linux/blk-mq.h>
+#endif
+
+#define IOC_DEF_MIN_IOPS 640
+#define IOC_DEF_PPM_HIGH 80
+#define IOC_DEF_PPM_LOW (IOC_DEF_PPM_HIGH >> 2)
+
+static int ioc_ppm_high = 0;
+struct iocost_config iocost_config = {0};
+
+DEFINE_PER_CPU(struct ioc_dist_stat, ioc_dist_stat) = { { 0 } };
+EXPORT_PER_CPU_SYMBOL(ioc_dist_stat);
+
+static void ioc_dist_update_stat(int rw, u64 on_q_us, u64 rq_wait_us,
+				 int segments)
+{
+	int i = 0;
+	int j = 0;
+	while (segments > 0) {
+		if (i >= (IOC_DIST_SEGMENTS - 1))
+			break;
+		i++;
+		segments = segments >> 1;
+	}
+
+	if (rw == RESCTRL_READ) {
+		if (ioc_high_read(on_q_us))
+			this_cpu_inc(ioc_dist_stat.rw[rw].high);
+	}
+	else {
+		if (ioc_high_write(on_q_us))
+			this_cpu_inc(ioc_dist_stat.rw[rw].high);
+	}
+
+	while (on_q_us > 0) {
+		if (j >= (IOC_DIST_TIMESTAT - 1))
+			break;
+		j++;
+		on_q_us = on_q_us >> 1;
+	}
+
+	this_cpu_inc(ioc_dist_stat.rw[rw].dist[i][j]);
+}
+
+void ioc_dist_clear_stat(void)
+{
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		struct ioc_dist_stat *this = &per_cpu(ioc_dist_stat, cpu);
+		memset(this, 0, sizeof(struct ioc_dist_stat));
+	}
+}
+
+void ioc_dist_get_stat(int rw, u64 *request, int time_dist)
+{
+	int cpu;
+	int i;
+
+	for_each_online_cpu(cpu) {
+		struct ioc_dist_stat *this = &per_cpu(ioc_dist_stat, cpu);
+
+		for (i = 0; i < IOC_DIST_SEGMENTS; i++)
+			request[i] += this->rw[rw].dist[i][time_dist];
+	}
+}
+
+void android_vh_blk_account_io_done_handler(void *unused, struct request *rq)
+{
+	unsigned int segments;
+	int rw = RESCTRL_READ;
+	u64 on_q_ns = ktime_get_ns() - rq->start_time_ns;
+	u64 rq_wait_ns = 0;
+	segments = blk_rq_stats_sectors(rq) >> 3;
+
+	switch (req_op(rq) & REQ_OP_MASK) {
+	case REQ_OP_READ:
+		rw = RESCTRL_READ;
+		break;
+	case REQ_OP_WRITE:
+		rw = RESCTRL_WRITE;
+		break;
+	default:
+		resctrl_debug("resctrl android_vh_blk_account_io_done_handler op=%d, error no support!",
+					req_op(rq));
+		return;
+	}
+
+	trace_printk("resctrl rq_qos_done rw=%d req_op=0x%x on_q=%lluus rq_wait=%lluus segments=%u, %d\n",
+				rw, req_op(rq), on_q_ns / 1000, rq_wait_ns / 1000, segments,
+				blk_rq_sectors(rq));
+	/* update the stat */
+	ioc_dist_update_stat(rw, on_q_ns / 1000, rq_wait_ns / 1000, segments);
+	return;
+}
+
+static void ioc_lat_calc(u64 *nr_high, u64 *nr_all)
+{
+	int cpu, i, j, rw;
+
+	for_each_online_cpu(cpu) {
+		struct ioc_dist_stat *this = &per_cpu(ioc_dist_stat, cpu);
+
+		for (rw = RESCTRL_READ; rw <= RESCTRL_WRITE; rw++) {
+			u64 this_all = 0;
+			u64 this_high = this->rw[rw].high;
+
+			for (i = 0; i < IOC_DIST_SEGMENTS; i++) {
+				for (j = 0; j < IOC_DIST_TIMESTAT; j++) {
+					this_all += this->rw[rw].dist[i][j];
+				}
+			}
+
+			nr_all[rw] += this_all - this->rw[rw].last_sum;
+			nr_high[rw] += this_high - this->rw[rw].last_high;
+
+			this->rw[rw].last_sum = this_all;
+			this->rw[rw].last_high = this_high;
+		}
+	}
+}
+
+void ioc_timer_fn(struct timer_list *timer)
+{
+	u64 nr_high[2] = {0};
+	u64 nr_all[2] = {0};
+
+	ioc_lat_calc(nr_high, nr_all);
+	trace_printk("### nr_high=%llu, nr_all=%llu ppm=%d\n", nr_high[0], nr_all[0], ioc_ppm_high);
+	if (nr_all[0] >= IOC_DEF_MIN_IOPS) {
+		if ((nr_high[0]*100/nr_all[0])  >= IOC_DEF_PPM_HIGH) {
+			if (ioc_ppm_high == 0) {
+				printk("resctrl set high ppm\n");
+				ioc_ppm_high = 1;
+			}
+		} else if ((nr_high[0] * 100 /nr_all[0])  < IOC_DEF_PPM_LOW) {
+			if (ioc_ppm_high != 0) {
+				printk("resctrl set low ppm\n");
+				ioc_ppm_high = 0;
+			}
+		}
+	} else {
+		if (ioc_ppm_high != 0) {
+			printk("resctrl clear high ppm\n");
+			ioc_ppm_high = 0;
+		}
+	}
+
+	if (iocost_config.ppm_start) {
+		iocost_config.timer.expires = jiffies + 1*HZ;
+		add_timer(&iocost_config.timer);
+	}
+}
+
+int ioc_iop_read(void)
+{
+	return ioc_ppm_high;
+}
diff --git a/drivers/soc/oplus/oplus_resctrl/resctrl.c b/drivers/soc/oplus/oplus_resctrl/resctrl.c
new file mode 100644
index 000000000..eeb4991f5
--- /dev/null
+++ b/drivers/soc/oplus/oplus_resctrl/resctrl.c
@@ -0,0 +1,232 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+* Copyright (C) 2023 Oplus. All rights reserved.
+*/
+
+#include <linux/string.h>
+#include "resctrl.h"
+#include "linux/proc_fs.h"
+#include "linux/module.h"
+
+#define OPLUS_RESCTRL_PROC "oplus_resctrl"
+#define RESCTRL_BUFSIZE_MAX 4096
+#define RESCTRL_CONFIG_MAX 64
+
+void resctrl_printk(int log_level, const char *func, const char *fmt, ...)
+{
+	/* todo */
+	return;
+}
+
+static ssize_t iocost_ppm_read(struct file *filp, char __user *buff,
+			       size_t count, loff_t *off)
+{
+	char page[32] = { 0 };
+	int len = 0;
+
+	len += sprintf(page, "%d\n", ioc_iop_read());
+
+	return sched_data_to_user(buff, count, off, page, len);
+}
+
+static ssize_t iocost_ppm_write(struct file *filp, const char __user *buff,
+			      size_t count, loff_t *off)
+{
+	char str[RESCTRL_CONFIG_MAX] = {0};
+	u32 new_start;
+	int ret;
+
+	if(count > (RESCTRL_CONFIG_MAX - 1) || count == 0)
+		return -EFAULT;
+
+	if (copy_from_user(str, buff, count))
+		return -EFAULT;
+	ret = kstrtouint(str, 10, &new_start);
+	if (ret)
+		return ret;
+	if (new_start == iocost_config.ppm_start)
+		return count;
+	if (new_start) {
+		iocost_config.ppm_start = new_start;
+		iocost_config.timer.expires = jiffies + 1*HZ;
+		add_timer(&iocost_config.timer);
+	} else {
+		iocost_config.ppm_start = new_start;
+		iocost_config.timer.expires = jiffies + 1*HZ;
+		del_timer(&iocost_config.timer);
+	}
+
+	return count;
+}
+
+static const struct proc_ops proc_iocost_ppm_fops = {
+	.proc_read = iocost_ppm_read,
+	.proc_write = iocost_ppm_write,
+	.proc_lseek = default_llseek,
+};
+
+static ssize_t ioc_dist_read(int rw, char __user *buff, size_t count,
+			     loff_t *off)
+{
+	int len = 0;
+	int ret_len = 0;
+	int i, j;
+	char *const time_str[] = { "1us",      "2us",      "4us",
+				   "8us",      "16us",     "32us",
+				   "64us",     "128us",    "256us",
+				   "512us",    "1024us",   "2048us",
+				   "4096us",   "8192us",   "16384us",
+				   "32768us",  "65536us",  "131072us",
+				   "262144us", "524288us", "other" };
+	char *page = kzalloc(RESCTRL_BUFSIZE_MAX, GFP_KERNEL);
+	if (!page)
+		return -ENOMEM;
+
+	len += sprintf(page + len,
+		       "%-10s%-10s%-10s%-10s%-10s%-10s%-10s%-10s%-10s%-10s"
+		       "%-10s\n",
+		       "latency", "1", "2", "4", "8", "16", "32", "64", "128",
+		       "256", "other");
+
+	for (i = 0; i < IOC_DIST_TIMESTAT; i++) {
+		u64 request[IOC_DIST_SEGMENTS] = { 0 };
+
+		ioc_dist_get_stat(rw, request, i);
+
+		len += sprintf(page + len, "%-10s", time_str[i]);
+		for (j = 0; j < IOC_DIST_SEGMENTS; j++)
+			len += sprintf(page + len, "%-10llu", request[j]);
+		len += sprintf(page + len, "\n");
+	}
+
+	ret_len = sched_data_to_user(buff, count, off, page, len);
+	kfree(page);
+	return ret_len;
+}
+
+static ssize_t ioc_dist_read_read(struct file *filp, char __user *buff,
+				  size_t count, loff_t *off)
+{
+	return ioc_dist_read(RESCTRL_READ, buff, count, off);
+}
+
+static ssize_t ioc_dist_write_read(struct file *filp, char __user *buff,
+				   size_t count, loff_t *off)
+{
+	return ioc_dist_read(RESCTRL_WRITE, buff, count, off);
+}
+
+static void  ioc_parse_config(char *key, char *value)
+{
+	int ret;
+	if (strcmp(key, "read_lat") == 0) {
+		ret = kstrtouint(value, 10, &iocost_config.read_lat);
+		if (ret)
+			printk("resctrl ioc_parse_config err\n");
+		printk("resctrl ioc_parse_config read_lat=%d\n", iocost_config.read_lat);
+	}
+
+	if (strcmp(key, "write_lat") == 0) {
+		ret = kstrtouint(value, 10, &iocost_config.write_lat);
+		if (ret)
+			printk("resctrl ioc_parse_config err\n");
+		printk("resctrl ioc_parse_config write_lat=%d\n", iocost_config.write_lat);
+	}
+}
+
+static ssize_t ioc_dist_write(struct file *filp, const char __user *buff,
+			      size_t count, loff_t *off)
+{
+	char str[RESCTRL_CONFIG_MAX] = {0};
+	char *sptr;
+	char *split = NULL;
+	char *key = NULL;
+
+	if(count > (RESCTRL_CONFIG_MAX - 1) || count == 0)
+		return -EFAULT;
+
+	if (copy_from_user(str, buff, count))
+		return -EFAULT;
+	sptr = str;
+
+	split = strsep(&sptr, " ");
+	while (split != NULL) {
+		key = strsep(&split, "=");
+		if (key && split) {
+			ioc_parse_config(key, split);
+		}
+
+		split = strsep(&sptr, " ");
+	}
+
+	ioc_dist_clear_stat();
+	return count;
+}
+
+static const struct proc_ops proc_ioc_dist_read_fops = {
+	.proc_read = ioc_dist_read_read,
+	.proc_write = ioc_dist_write,
+	.proc_lseek = default_llseek,
+};
+
+static const struct proc_ops proc_ioc_dist_write_fops = {
+	.proc_read = ioc_dist_write_read,
+	.proc_write = ioc_dist_write,
+	.proc_lseek = default_llseek,
+};
+
+static int __init oplus_resctrl_init(void)
+{
+	int ret = 0;
+	struct proc_dir_entry *pentry_dir = NULL;
+	struct proc_dir_entry *pentry = NULL;
+
+	pentry_dir = proc_mkdir(OPLUS_RESCTRL_PROC, NULL);
+	if (!pentry_dir) {
+		resctrl_err("create dir failed.\n");
+		return -ENOENT;
+	}
+
+	pentry = proc_create("iocost_ppm", S_IRUGO | S_IWUGO, pentry_dir,
+			     &proc_iocost_ppm_fops);
+	if (!pentry) {
+		resctrl_err("create iocost_ppm failed.\n");
+		goto err;
+	}
+	pentry = proc_create("ioc_dist_read", S_IRUGO | S_IWUGO, pentry_dir,
+			     &proc_ioc_dist_read_fops);
+	if (!pentry) {
+		resctrl_err("create ioc_dist_read failed.\n");
+		goto err;
+	}
+
+	pentry = proc_create("ioc_dist_write", S_IRUGO | S_IWUGO, pentry_dir,
+			     &proc_ioc_dist_write_fops);
+	if (!pentry) {
+		resctrl_err("create ioc_dist_write failed.\n");
+		goto err;
+	}
+
+	timer_setup(&iocost_config.timer, ioc_timer_fn, 0);
+	iocost_config.ppm_start = 1;
+	iocost_config.timer.expires = jiffies + 1*HZ;
+	add_timer(&iocost_config.timer);
+
+	return ret;
+err:
+	remove_proc_entry(OPLUS_RESCTRL_PROC, NULL);
+	return -ENOENT;
+}
+
+static void __exit oplus_resctrl_exit(void)
+{
+	del_timer(&iocost_config.timer);
+	remove_proc_subtree(OPLUS_RESCTRL_PROC, NULL);
+	printk("oplus resctrl module exit\n");
+}
+
+module_init(oplus_resctrl_init);
+module_exit(oplus_resctrl_exit);
+
+MODULE_DESCRIPTION("oplus_resctrl");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/oplus/oplus_resctrl/resctrl.h b/drivers/soc/oplus/oplus_resctrl/resctrl.h
new file mode 100644
index 000000000..0b3ac4036
--- /dev/null
+++ b/drivers/soc/oplus/oplus_resctrl/resctrl.h
@@ -0,0 +1,176 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+* Copyright (C) 2023 Oplus. All rights reserved.
+*/
+#ifndef _RESCTRL_H_
+#define _RESCTRL_H_
+#include <linux/compiler_types.h>
+#include <linux/uaccess.h>
+#include <linux/kernel.h>
+#include <linux/seq_file.h>
+#include <linux/blkdev.h>
+#include <linux/version.h>
+
+#ifndef KERNEL_VERSION
+#define KERNEL_VERSION(a, b, c)                                                \
+	(((a) << 16) + ((b) << 8) + ((c) > 255 ? 255 : (c)))
+#endif
+
+#ifndef LINUX_VERSION_CODE
+#error "Need LINUX_VERSION_CODE"
+#endif
+
+#define LINUX_KERNEL_601                                                       \
+	((LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)) &&                   \
+	 (LINUX_VERSION_CODE < KERNEL_VERSION(6, 1, 256)))
+#define LINUX_KERNEL_515                                                       \
+	((LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 0)) &&                   \
+	 (LINUX_VERSION_CODE < KERNEL_VERSION(5, 15, 256)))
+#define LINUX_KERNEL_510                                                       \
+	((LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)) &&                   \
+	 (LINUX_VERSION_CODE < KERNEL_VERSION(5, 10, 256)))
+#define LINUX_KERNEL_504                                                       \
+	((LINUX_VERSION_CODE >= KERNEL_VERSION(5, 4, 0)) &&                    \
+	 (LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 256)))
+#define LINUX_KERNEL_419                                                       \
+	((LINUX_VERSION_CODE >= KERNEL_VERSION(4, 19, 0)) &&                   \
+	 (LINUX_VERSION_CODE < KERNEL_VERSION(4, 19, 256)))
+
+#if (!LINUX_KERNEL_601 && !LINUX_KERNEL_515 && !LINUX_KERNEL_510 && !LINUX_KERNEL_419 &&            \
+	 !LINUX_KERNEL_504)
+#error "Kernel version not supported"
+#endif
+
+enum {
+	RESCTRL_LOG_LEVEL_ERROR,
+	RESCTRL_LOG_LEVEL_WARN,
+	RESCTRL_LOG_LEVEL_NOTICE,
+	RESCTRL_LOG_LEVEL_INFO,
+	RESCTRL_LOG_LEVEL_DEBUG,
+	RESCTRL_LOG_LEVEL_TEMP,
+};
+
+/*
+ * we know min is 4K and max is 512K
+ * but we still keep the 1 and other col
+ */
+#define IOC_DIST_TIMESTAT 21
+#define IOC_DIST_SEGMENTS 10
+
+#define RESCTRL_READ 0
+#define RESCTRL_WRITE 1
+
+struct ioc_dist_rw {
+	u64 dist[IOC_DIST_SEGMENTS][IOC_DIST_TIMESTAT];
+	u64 high;
+	u64 last_sum;
+	u64 last_high;
+};
+
+struct ioc_dist_stat {
+	struct ioc_dist_rw rw[2];
+};
+
+struct iocost_config {
+	struct timer_list timer;
+	u32 read_lat;
+	u32 write_lat;
+	u32 ppm_start;
+};
+
+extern struct iocost_config iocost_config;
+
+__printf(3, 4) void resctrl_printk(int log_level, const char *func,
+				   const char *fmt, ...);
+
+#define resctrl_err(fmt, ...)                                                  \
+	resctrl_printk(RESCTRL_LOG_LEVEL_ERROR, __func__, KERN_ERR fmt,        \
+		       ##__VA_ARGS__)
+#define resctrl_warn(fmt, ...)                                                 \
+	resctrl_printk(RESCTRL_LOG_LEVEL_WARN, __func__, KERN_WARNING fmt,     \
+		       ##__VA_ARGS__)
+#ifdef CONFIG_RESCTRL_DEBUG
+#define resctrl_notice(fmt, ...)                                               \
+	resctrl_printk(RESCTRL_LOG_LEVEL_NOTICE, __func__, KERN_NOTICE fmt,    \
+		       ##__VA_ARGS__)
+#define resctrl_info(fmt, ...)                                                 \
+	resctrl_printk(RESCTRL_LOG_LEVEL_INFO, __func__, KERN_INFO fmt,        \
+		       ##__VA_ARGS__)
+#define resctrl_debug(fmt, ...)                                                \
+	resctrl_printk(RESCTRL_LOG_LEVEL_DEBUG, __func__, KERN_DEBUG fmt,      \
+		       ##__VA_ARGS__)
+#define resctrl_temp(fmt, ...)                                                 \
+	resctrl_printk(RESCTRL_LOG_LEVEL_TEMP, __func__, KERN_DEBUG fmt,       \
+		       ##__VA_ARGS__)
+#else
+#define resctrl_notice(sbi, fmt, ...)                                          \
+	do {                                                                   \
+	} while (0)
+#define resctrl_info(sbi, fmt, ...)                                            \
+	do {                                                                   \
+	} while (0)
+#define resctrl_debug(sbi, fmt, ...)                                           \
+	do {                                                                   \
+	} while (0)
+#define resctrl_temp(sbi, fmt, ...)                                            \
+	do {                                                                   \
+	} while (0)
+#endif
+
+#define REGISTER_TRACE_VH(vender_hook, handler)                                \
+{                                                                      \
+	ret = register_trace_##vender_hook(handler, NULL);             \
+	if (ret) {                                                     \
+		pr_info("failed to register_" #vender_hook             \
+			", ret=%d\n",                                  \
+			ret);                                          \
+		return ret;                                            \
+	}                                                              \
+}
+
+#define UNREGISTER_TRACE_VH(vender_hook, handler)                              \
+{                                                                      \
+	ret = unregister_trace_##vender_hook(handler, NULL);           \
+	pr_info("%s handler register success", #handler);              \
+}
+
+static inline ssize_t sched_data_to_user(char __user *buff, size_t count,
+					 loff_t *off, char *format_str, int len)
+{
+	if (len > *off)
+		len -= *off;
+	else
+		len = 0;
+	if (copy_to_user(buff, format_str, (len < count ? len : count)))
+		return -EFAULT;
+	*off += len < count ? len : count;
+
+	return (len < count ? len : count);
+}
+
+void android_vh_blk_account_io_done_handler(void *unused, struct request *rq);
+
+int  ioc_iop_read(void);
+void ioc_dist_clear_stat(void);
+void ioc_dist_get_stat(int rw, u64 *request, int time_dist);
+void ioc_timer_fn(struct timer_list *timer);
+
+static inline bool ioc_high_read(u32 value) {
+	if (iocost_config.read_lat == 0)
+		return false;
+
+	if (value > iocost_config.read_lat)
+		return true;
+	return false;
+}
+
+static inline bool ioc_high_write(u32 value) {
+	if (iocost_config.write_lat == 0)
+		return false;
+
+	if (value > iocost_config.write_lat)
+		return true;
+	return false;
+}
+
+#endif /* _RESCTRL_H_ */
diff --git a/drivers/soc/oplus/storage/Kconfig b/drivers/soc/oplus/storage/Kconfig
new file mode 100644
index 000000000..e52838949
--- /dev/null
+++ b/drivers/soc/oplus/storage/Kconfig
@@ -0,0 +1,8 @@
+
+config OPLUS_FEATURE_STORAGE_IO_METRICS
+	tristate "config storage io metrics"
+	default y
+	help
+	  The function of this module is to collect various metrics on the IO path
+
+source "drivers/soc/oplus/storage/common/fmonitor/Kconfig"
diff --git a/drivers/soc/oplus/storage/Makefile b/drivers/soc/oplus/storage/Makefile
new file mode 100644
index 000000000..260435d19
--- /dev/null
+++ b/drivers/soc/oplus/storage/Makefile
@@ -0,0 +1,5 @@
+GCOV_PROFILE := y
+#ifdef OPLUS_FEATURE_STORAGE_IO_METRICS
+obj-$(CONFIG_OPLUS_FEATURE_STORAGE_IO_METRICS) 	+= common/io_metrics/
+#endif /*OPLUS_FEATURE_STORAGE_IO_METRICS*/
+obj-y	+= common/fmonitor/
diff --git a/drivers/soc/oplus/storage/README.md b/drivers/soc/oplus/storage/README.md
new file mode 100644
index 000000000..4140899c4
--- /dev/null
+++ b/drivers/soc/oplus/storage/README.md
@@ -0,0 +1,14 @@
+# 目录结构说明
+## common
+存放所有平台通用模块代码
+## mtk
+存放MTK平台特有模块
+## qcom
+存放qcom平台特有模块代码
+# 开发feature说明
+1. 独立的feature模块放在对应目录下独立的子目录
+2. 当前目录下的Kconfig、Makefile为in-tree编译方式存在（OKI），out-of-tree解耦后的编译用不到
+3. 匹配不同版本的内核可以采用LINUX_VERSION_CODE、KERNEL_VERSION()去控制
+# 编译说明
+1. 子目录的编译采用out-of-tree方式编译，修改仓库oplus/kernel/build中相应的配置
+2. 子目录的编译采用in-tree方式编译，修改当前目录的Kconfig
diff --git a/drivers/soc/oplus/storage/bp/OplusStorage.h b/drivers/soc/oplus/storage/bp/OplusStorage.h
new file mode 100644
index 000000000..67a98f78a
--- /dev/null
+++ b/drivers/soc/oplus/storage/bp/OplusStorage.h
@@ -0,0 +1,34 @@
+/** @file
+  Oplus Storage protocol as defined in the UEFI 2.0 specification.
+
+  The Oplus StorageO protocol is used to abstract oplus storage drivers
+
+  Copyright (c) 2006 - 2018, OPLUS All rights reserved.<BR>
+  SPDX-License-Identifier: BSD-2-Clause-Patent
+
+ **/
+#ifndef __OPLUS_STORAGE_H__
+#define __OPLUS_STORAGE_H__
+#include "BlockIo.h"
+typedef struct _EFI_BLOCK_IO_PROTOCOL  EFI_BLOCK_IO_PROTOCOL;
+typedef struct __EFI_OPLUS_STORAGE_PROTOCOL EFI_OPLUS_STORAGE_PROTOCOL;
+#define EFI_OPLUS_STORAGE_PROTOCOL_GUID \
+{ \
+	0x62b1b2da, 0xd0df, 0x4531, {0x80, 0xa4, 0xab, 0xe7, 0xa3, 0xff, 0xdf, 0x78 } \
+}
+
+typedef
+EFI_STATUS
+(EFIAPI * EFI_OPLUS_FW_OPERATION)(
+    IN EFI_OPLUS_STORAGE_PROTOCOL *This,
+    IN EFI_BLOCK_IO_PROTOCOL *blockio,
+    VOID *in,
+    VOID *out
+);
+
+struct __EFI_OPLUS_STORAGE_PROTOCOL {
+	EFI_OPLUS_FW_OPERATION  SdccFwOps;
+};
+
+extern EFI_GUID gEfiOplusStorageProtocolGuid;
+#endif //__OPLUS_STORAGE_H__
diff --git a/drivers/soc/oplus/storage/bp/OplusStorageDxe.c b/drivers/soc/oplus/storage/bp/OplusStorageDxe.c
new file mode 100644
index 000000000..dd2fdfb78
--- /dev/null
+++ b/drivers/soc/oplus/storage/bp/OplusStorageDxe.c
@@ -0,0 +1,144 @@
+/**
+* @file    OplusStorageDxe.c
+* @brief   Universal Flash Storage (UFS) Dxe Driver
+*
+*  Copyright (c) 2013 - 2022 OPLUS All rights reserved.
+*/
+
+
+/*=============================================================================
+                               EDIT HISTORY
+
+
+ when            who   what, where, why
+ ----------      ---   -----------------------------------------------------------
+ 2023-5-26	 zhoumaowei Add for decoupling of ufs drivers
+ =============================================================================*/
+#include <Uefi.h>
+#include <Library/BaseLib.h>
+#include <Library/MemoryAllocationLib.h>
+#include <Library/DebugLib.h>
+#include <Library/IoLib.h>
+#include <Library/PcdLib.h>
+#include <Library/UefiBootServicesTableLib.h>
+#include <Library/BaseMemoryLib.h>
+#include <Library/UncachedMemoryAllocationLib.h>
+#include <Library/ArmLib.h>
+#include <Library/SerialPortShLib.h>
+#include <Library/UefiBootServicesTableLib.h>
+#include <Library/StorSecApp.h>
+#include <Library/UefiCfgLib.h>
+#include <Protocol/BlockIo.h>
+#include <Protocol/BlockIo2.h>
+#include <Protocol/BlockIoCrypto.h>
+#include <Protocol/DevicePath.h>
+#include <Protocol/EFICardInfo.h>
+#include <Protocol/EFIClock.h>
+#include <Protocol/EFIHWIO.h>
+#include <Protocol/EFIStorageDeviceMgmt.h>
+#include <Protocol/OplusStorage.h>
+
+#include <Library/GPTListener.h>
+#include <Library/RpmbListener.h>
+#include <Library/RpmbLib.h>
+#include <Protocol/EFIRpmb.h>
+#include <Protocol/EFIEraseBlock.h>
+#include <Protocol/EFIStorageWriteProtect.h>
+#include <Protocol/EFIHALIOMMUProtocol.h>
+
+#include "OplusStorageDxe.h"
+#include "Library/KernelLib.h"
+#include <api/storage/ufs_api.h>
+#include <BootConfig.h>
+#include <api/storage/ufs_upgrade.h>
+
+EFI_OPLUS_STORAGE_PROTOCOL gOplusStorage = {
+	UFSFirmwareOps
+};
+
+EFI_STATUS EFIAPI UFSFirmwareOps(
+    IN EFI_OPLUS_STORAGE_PROTOCOL *This,
+    IN EFI_BLOCK_IO_PROTOCOL *blockio,
+    VOID *in1,
+    VOID *in2
+)
+{
+	int ret = 0;
+	EFI_STATUS Status = EFI_NOT_STARTED;
+	EFI_PHYSICAL_ADDRESS fw_buff_addr = 0;
+	unsigned long long fw_offset = 0;
+	struct ufs_handle *hUFS = NULL;
+	OPLUS_PARTITION_PRIVATE_DATA *PartitionData = NULL;
+        OPLUS_UFS_DEV *UfsDevice = NULL;
+        uint64_t logo_partition_start_lba = 0;
+        
+	if (NULL == This || NULL == blockio)
+	{
+		return EFI_INVALID_PARAMETER;
+	}
+
+	DEBUG ((EFI_D_ERROR, "BOOT: You are calling oplus UFSFirmwareOps function now.0x%x\n", blockio));
+    PartitionData = BASE_CR(blockio, OPLUS_PARTITION_PRIVATE_DATA, BlockIo);
+	if (NULL == PartitionData || NULL == PartitionData->ParentBlockIo)
+	{
+		return EFI_INVALID_PARAMETER;
+	}
+
+    logo_partition_start_lba = PartitionData->Start/PartitionData->BlockSize;
+	UfsDevice = BASE_CR(PartitionData->ParentBlockIo, OPLUS_UFS_DEV, BlkIo);
+	if (!UfsDevice || !(hUFS = (struct ufs_handle *)UfsDevice->DeviceHandle))
+	{
+		DEBUG((EFI_D_ERROR, "UEFI UFS lib is not initilized.\n"));
+		return EFI_NOT_STARTED;
+	}
+	Status = gBS->AllocatePages(AllocateAnyPages, EfiBootServicesCode, 
+							EFI_SIZE_TO_PAGES(FIRMWARE_LENGTH_MAX), &fw_buff_addr);
+	if (EFI_ERROR(Status))
+	{
+		DEBUG((EFI_D_ERROR, "UEFI AllocatePages fail.\n"));
+		return EFI_NOT_STARTED;
+	}
+
+	Status = EFI_NOT_STARTED;
+	in2 = (VOID*)&logo_partition_start_lba;
+	fw_offset = UNIFIED_RESERVE_UFS_FW_OFFSET;
+	if (need_upgrade_ufs_fimware(hUFS, (in1 ? (*(unsigned long long*)in1) : INVALID_LBA),
+						*(unsigned long long*)in2,
+						fw_offset, fw_buff_addr) ||
+						need_upgrade_ufs_fimware(hUFS, (in1 ? (*(unsigned long long*)in1) : INVALID_LBA),
+						*(unsigned long long*)in2,
+						UNIFIED_RESERVE_UFS_FW_OFFSET2,
+						fw_buff_addr))
+	{
+		ret = ufs_firmware_update(hUFS, fw_buff_addr);
+		if (ret == 0)
+		{
+			DEBUG((EFI_D_ERROR, "ufs FW upgrade success.\n"));
+			Status = EFI_SUCCESS;
+		}
+	}
+	gBS->FreePages(fw_buff_addr, EFI_SIZE_TO_PAGES(FIRMWARE_LENGTH_MAX));
+	return Status;
+}
+
+EFI_STATUS
+EFIAPI
+OplusStorageEntry (
+	IN EFI_HANDLE ImageHandle,
+	IN EFI_SYSTEM_TABLE *SystemTable
+)
+{
+	EFI_HANDLE handle = NULL;
+	EFI_STATUS status;
+        DEBUG((EFI_D_ERROR, "start init oplus storage entry.\n"));
+	status = gBS->InstallMultipleProtocolInterfaces(&handle,
+						&gEfiOplusStorageProtocolGuid,
+						(void **)&gOplusStorage,
+						NULL, 
+						NULL,
+						NULL);
+
+	return status;
+}
+
+
diff --git a/drivers/soc/oplus/storage/bp/OplusStorageDxe.h b/drivers/soc/oplus/storage/bp/OplusStorageDxe.h
new file mode 100644
index 000000000..0c95673c8
--- /dev/null
+++ b/drivers/soc/oplus/storage/bp/OplusStorageDxe.h
@@ -0,0 +1,84 @@
+/**
+@file    OplusStorageDxe.h
+@brief   Universal Flash Storage (UFS) DXE Header File
+
+This file contains the definitions of the constants, data structures,
+and interfaces for the OplusStorage driver in UEFI.
+
+ Copyright (c) 2013 - 2018, 2021,2022 OPLUS All rights reserved.
+**/
+
+ /*=============================================================================
+                               EDIT HISTORY
+
+
+ when            who   what, where, why
+ ----------      ---   -----------------------------------------------------------
+2023-5-26   zhoumaowei Add for decoupling of ufs drivers
+ =============================================================================*/
+
+#ifndef _OPLUSSTORAGEDXE_H_
+#define _OPLUSSTORAGEDXE_H_
+#include <Protocol/ComponentName.h>
+#include <Protocol/DevicePath.h>
+#include <Protocol/DriverBinding.h>
+#include <Protocol/DiskIo.h>
+#include <Protocol/PartitionInfo.h>
+#include <Protocol/DiskIo2.h>
+typedef struct _EFI_BLOCK_IO_PROTOCOL  EFI_BLOCK_IO_PROTOCOL;
+#define OPLUS_UFS_DEV_SIGNATURE SIGNATURE_32 ('u', 'f', 's', ' ')   /**< -- UFS  */
+
+typedef struct {
+  UINT64                       Signature;
+
+  EFI_HANDLE                   Handle;
+  EFI_DEVICE_PATH_PROTOCOL     *DevicePath;
+  EFI_BLOCK_IO_PROTOCOL        BlockIo;
+  EFI_BLOCK_IO2_PROTOCOL       BlockIo2;
+  EFI_BLOCK_IO_MEDIA           Media;
+  EFI_BLOCK_IO_MEDIA           Media2;//For BlockIO2
+  EFI_PARTITION_INFO_PROTOCOL  PartitionInfo;
+
+  EFI_DISK_IO_PROTOCOL         *DiskIo;
+  EFI_DISK_IO2_PROTOCOL        *DiskIo2;
+  EFI_BLOCK_IO_PROTOCOL        *ParentBlockIo;
+  EFI_BLOCK_IO2_PROTOCOL       *ParentBlockIo2;
+  UINT64                       Start;
+  UINT64                       End;
+  UINT32                       BlockSize;
+  BOOLEAN                      InStop;
+
+  EFI_GUID                     TypeGuid;
+#if defined (QCOM_EDK2_PATCH)
+  EFI_PARTITION_ENTRY          PartEntry;
+
+  EFI_ERASE_BLOCK_PROTOCOL    *ParentEraseBlock;
+  EFI_ERASE_BLOCK_PROTOCOL    BlockErase;
+#endif
+} OPLUS_PARTITION_PRIVATE_DATA;
+
+typedef struct {
+  UINT32                        Signature;
+  EFI_HANDLE                    ClientHandle;  /**< -- Client Handle */
+  EFI_HANDLE                    DeviceHandle;  /**< -- Device Handle */
+  EFI_BLOCK_IO_PROTOCOL         BlkIo;         /**< -- Block I/O */
+  EFI_BLOCK_IO_CRYPTO_PROTOCOL  BlkIoCrypto;   /**< -- Block I/O Crypto */
+  EFI_BLOCK_IO2_PROTOCOL        BlkIo2;        /**< -- Block I/O 2 */
+  EFI_SDCC_RPMB_PROTOCOL        RPMB;          /**< -- RPMB protocol */
+  EFI_MEM_CARDINFO_PROTOCOL     CardInfo;      /**< -- Memory card into */
+  EFI_ERASE_BLOCK_PROTOCOL      EraseBlk;      /**< -- Erase block */
+  EFI_STORAGE_WP_PROTOCOL       WriteProtect;  /**< -- Write Protect */
+  EFI_STORAGE_DEV_MGMT_PROTOCOL DeviceMgmt;    /**< -- Device Management */
+} OPLUS_UFS_DEV;
+
+EFI_STATUS EFIAPI UFSFirmwareOps(
+  IN EFI_OPLUS_STORAGE_PROTOCOL *This,
+  IN EFI_BLOCK_IO_PROTOCOL *blockio,
+  VOID *in1,
+  VOID *in2
+);
+
+#define OPLUS_UFS_DEV_FROM_BLOCKIO(a)   CR (a, OPLUS_UFS_DEV, BlkIo, OPLUS_UFS_DEV_SIGNATURE)
+
+#endif /* _UFS_H_ */
+
diff --git a/drivers/soc/oplus/storage/bp/OplusStorageDxe.inf b/drivers/soc/oplus/storage/bp/OplusStorageDxe.inf
new file mode 100644
index 000000000..3bfb05265
--- /dev/null
+++ b/drivers/soc/oplus/storage/bp/OplusStorageDxe.inf
@@ -0,0 +1,103 @@
+#/** @file UFSDxe.inf
+#
+#  UFSDxe INF file
+#
+#  Copyright (c) 2013, 2015-18, 2020-21,2022 OPLUS
+#  All Rights Reserved.
+#
+#**/
+
+#==============================================================================
+#                              EDIT HISTORY
+#
+#
+# when         who     what, where, why
+# ----------   ---     ---------------------------------------------------------
+# 2023/05/26   zhoumaowei   Add OplusStorageDxe for ufs drivers
+#
+#==============================================================================
+
+[Defines]
+  INF_VERSION                       = 0x00010005
+  BASE_NAME                         = OplusStorageDxe
+  FILE_GUID                         = 454bf7e3-2a68-4732-b41d-7ef50a592c93
+  MODULE_TYPE                       = UEFI_DRIVER
+  VERSION_STRING                    = 1.0
+  ENTRY_POINT                       = OplusStorageEntry
+
+
+[BuildOptions.AARCH64]
+  GCC:*_*_*_CC_FLAGS = -Wno-missing-field-initializers -Wno-missing-braces -Wno-unused-but-set-variable
+
+[Sources.common]
+  OplusStorageDxe.c
+
+[Packages]
+  MdePkg/MdePkg.dec
+  EmbeddedPkg/EmbeddedPkg.dec
+  ArmPkg/ArmPkg.dec
+  QcomPkg/QcomPkg.dec
+  QcomSdkPkg/QcomSdkPkg.dec
+
+[LibraryClasses]
+  BaseLib
+  UefiRuntimeServicesTableLib
+  UefiLib
+  UefiBootServicesTableLib
+  BaseMemoryLib
+  DebugLib
+  UefiDriverEntryPoint
+  IoLib
+  UncachedMemoryAllocationLib
+  UefiCfgLib
+  SerialPortLib
+  PrintLib
+  ArmLib
+  MemoryAllocationLib
+  CacheMaintenanceLib
+  TimerLib
+  PcdLib
+  UfsCommonLib
+  RpmbListenerLib
+  BootConfigLib
+  KernelLib
+  LockLib
+  BlkCryptoLib
+
+[Guids]
+  gEfiEventExitBootServicesGuid 
+
+[Protocols]
+  gEfiBlockIoProtocolGuid
+  gEfiBlockIoCryptoProtocolGuid
+  gEfiBlockIo2ProtocolGuid
+  gEfiCpuArchProtocolGuid
+  gEfiDevicePathProtocolGuid
+  gEfiClockProtocolGuid
+  gEfiHwioProtocolGuid
+  gEfiSdccRpmbProtocolGuid
+  gQcomScmProtocolGuid
+  gEfiMemCardInfoProtocolGuid
+  gEfiEraseBlockProtocolGuid
+  gEfiStorageWpProtocolGuid
+  gEfiHalIommuProtocolGuid
+  gEfiStorageDevMgmtProtocolGuid
+  gEfiDtbExtnProtocolGuid
+  gQcomAcpiPlatformProtocolGuid
+  gEfiOplusStorageProtocolGuid
+  gEfiDiskIoProtocolGuid  
+# [Pcd.common]
+#  gQcomTokenSpaceGuid.UFSSlotNumber
+  gEfiDiskIo2ProtocolGuid
+  gEfiDiskIoProtocolGuid
+  gEfiPartitionInfoProtocolGuid
+
+[Depex]
+	TRUE
+#  gEfiClockProtocolGuid
+#  AND
+#  gEfiTimerArchProtocolGuid
+
+[Pcd.common]
+  gQcomTokenSpaceGuid.PcdQTimerEnabled
+
diff --git a/drivers/soc/oplus/storage/common/README.md b/drivers/soc/oplus/storage/common/README.md
new file mode 100644
index 000000000..33340ecca
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/README.md
@@ -0,0 +1,2 @@
+# io_metrics
+存储IO各项性能统计模块，在目录/proc/oplus_storage/io_metrics下会导出相应指标的性能数据
diff --git a/drivers/soc/oplus/storage/common/fmonitor/Kconfig b/drivers/soc/oplus/storage/common/fmonitor/Kconfig
new file mode 100644
index 000000000..b3936ac75
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/fmonitor/Kconfig
@@ -0,0 +1,6 @@
+config OPLUS_FEATURE_FILE_MONITOR
+    tristate "file monitor"
+    default y
+    help
+      Monitor User Mode File Modification.
+
diff --git a/drivers/soc/oplus/storage/common/fmonitor/Makefile b/drivers/soc/oplus/storage/common/fmonitor/Makefile
new file mode 100644
index 000000000..f473f4a8a
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/fmonitor/Makefile
@@ -0,0 +1,7 @@
+# SPDX-License-Identifier: GPL-2.0-only
+# Copyright (C) 2022-2025 Oplus. All rights reserved.
+
+GCOV_PROFILE := y
+LINUXINCLUDE += -I$(srctree)/
+obj-$(CONFIG_OPLUS_FEATURE_FILE_MONITOR)	:= oplus_file_monitor.o
+
diff --git a/drivers/soc/oplus/storage/common/fmonitor/oplus_file_monitor.c b/drivers/soc/oplus/storage/common/fmonitor/oplus_file_monitor.c
new file mode 100644
index 000000000..ae3175470
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/fmonitor/oplus_file_monitor.c
@@ -0,0 +1,326 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2018-2020 Oplus. All rights reserved.
+ */
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/limits.h>
+#include <linux/printk.h>
+#include <linux/mutex.h>
+#include <linux/fcntl.h>
+#include <linux/version.h>
+#include <linux/proc_fs.h>
+#include <linux/atomic.h>
+#include <linux/f2fs_fs.h>
+#include <linux/sched.h>
+#include <linux/pid.h>
+#include <linux/seq_file.h>
+#include <linux/compiler.h>
+#include "fs/f2fs/f2fs.h"
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+#include "trace/events/android_fs.h"
+#else
+#include <linux/fs.h>
+#define MAX_TRACE_PATHBUF_LEN	256
+static inline char *
+android_fstrace_get_pathname(char *buf, int buflen, struct inode *inode)
+{
+	char *path;
+	struct dentry *d;
+	/*
+	 * d_obtain_alias() will either iput() if it locates an existing
+	 * dentry or transfer the reference to the new dentry created.
+	 * So get an extra reference here.
+	 */
+	ihold(inode);
+	d = d_obtain_alias(inode);
+	if (likely(!IS_ERR(d))) {
+		path = dentry_path_raw(d, buf, buflen);
+		if (unlikely(IS_ERR(path))) {
+			strcpy(buf, "ERROR");
+			path = buf;
+		}
+		dput(d);
+	} else {
+		strcpy(buf, "ERROR");
+		path = buf;
+	}
+	return path;
+}
+#endif
+#include <trace/events/f2fs.h>
+static struct proc_dir_entry *file_monitor_procfs = NULL;
+static struct proc_dir_entry *path_filter_file = NULL;
+static struct proc_dir_entry *remove_event_file = NULL;
+static struct proc_dir_entry *unlink_switch_file = NULL;
+static char pfliter_buf[1024] = {0};
+static char pfliter_front[512] = {0};
+static char pfliter_back[512] = {0};
+static char remove_buf[1024] = {0};
+static int g_unlink_switch = 0;
+static int rlen = 0;
+#define FILE_MONITOR_LOG_TAG "[file_monitor]"
+
+static ssize_t remove_proc_write(struct file *file, const char __user *buf,
+		size_t count, loff_t *off)
+{
+	memset(remove_buf, 0, sizeof(remove_buf));
+
+	if (count > sizeof(remove_buf) - 1) {
+		count = sizeof(remove_buf) - 1;
+	}
+
+	if (copy_from_user(remove_buf, buf, count)) {
+		pr_err(FILE_MONITOR_LOG_TAG "%s: read proc input error.\n", __func__);
+		return count;
+	}
+
+	if (count > 0) {
+		remove_buf[count-1] = '\0';
+	}
+
+	return count;
+}
+
+static ssize_t remove_proc_read(struct file *file, char __user *buf,
+		size_t count, loff_t *ppos)
+{
+	return simple_read_from_buffer(buf, count, ppos, remove_buf, strlen(remove_buf));
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+static struct proc_ops remove_event_fops = {
+	.proc_read = remove_proc_read,
+	.proc_write = remove_proc_write,
+	.proc_lseek = default_llseek,
+};
+#else
+static struct file_operations remove_event_fops = {
+	.read = remove_proc_read,
+	.write = remove_proc_write,
+	.llseek = default_llseek,
+};
+#endif
+
+static ssize_t pfliter_proc_write(struct file *file, const char __user *buf,
+		size_t count, loff_t *off)
+{
+	char *ptr;
+	memset(pfliter_buf, 0, sizeof(pfliter_buf));
+
+	if (count > sizeof(pfliter_buf) - 1) {
+		count = sizeof(pfliter_buf) - 1;
+	}
+
+	if (copy_from_user(pfliter_buf, buf, count)) {
+		pr_err(FILE_MONITOR_LOG_TAG "%s: read proc input error.\n", __func__);
+		return count;
+	}
+
+	if (count > 0) {
+		pfliter_buf[count-1] = '\0';
+	}
+
+	ptr = strstr(pfliter_buf, "*");
+	if (ptr) {
+		size_t len1 = ptr - pfliter_buf;
+		size_t len2 = count - len1 - 2;
+
+		strncpy(pfliter_front, pfliter_buf, len1);
+		strncpy(pfliter_back, ptr + 1, len2);
+
+		pfliter_front[len1] = '\0';
+		pfliter_back[len2] = '\0';
+	}
+
+	return count;
+}
+
+static ssize_t pfliter_proc_read(struct file *file, char __user *buf,
+		size_t count, loff_t *ppos)
+{
+	char bpath[100];
+	size_t len = 0;
+
+	len = snprintf(bpath, sizeof(bpath), "%s", pfliter_buf);
+	return simple_read_from_buffer(buf, count, ppos, bpath, len);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+static struct proc_ops path_filter_fops = {
+	.proc_read = pfliter_proc_read,
+	.proc_write = pfliter_proc_write,
+	.proc_lseek = default_llseek,
+};
+#else
+static struct file_operations path_filter_fops = {
+	.read = pfliter_proc_read,
+	.write = pfliter_proc_write,
+	.llseek = default_llseek,
+};
+#endif
+
+static ssize_t uswitch_proc_write(struct file *file, const char __user *buf,
+		size_t count, loff_t *off)
+{
+	char str[3] = {0};
+
+	if (count > 2 || count < 1) {
+		pr_err(FILE_MONITOR_LOG_TAG "unlink switch invalid parameter\n");
+		return -EINVAL;
+	}
+
+	if (copy_from_user(str, buf, count)) {
+		pr_err(FILE_MONITOR_LOG_TAG "copy_from_user failed\n");
+		return -EFAULT;
+	}
+
+	if (unlikely(!strncmp(str, "1", 1))) {
+		pr_info(FILE_MONITOR_LOG_TAG "unlink switch enabled\n");
+		g_unlink_switch = 1;
+	} else {
+		pr_info(FILE_MONITOR_LOG_TAG "unlink switch disabled\n");
+		g_unlink_switch = 0;
+	}
+
+	return (ssize_t)count;
+}
+
+static int unlink_switch_show(struct seq_file *s, void *data)
+{
+	if (g_unlink_switch == 1)
+		seq_printf(s, "%d\n", 1);
+	else
+		seq_printf(s, "%d\n", 0);
+
+	return 0;
+}
+
+static int uswitch_proc_open(struct inode *inodp, struct file *filp)
+{
+	return single_open(filp, unlink_switch_show, inodp);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+static struct proc_ops unlink_switch_fops = {
+	.proc_open = uswitch_proc_open,
+	.proc_read = seq_read,
+	.proc_write = uswitch_proc_write,
+	.proc_lseek = default_llseek,
+};
+#else
+static struct file_operations unlink_switch_fops = {
+	.open = uswitch_proc_open,
+	.read = seq_read,
+	.write = uswitch_proc_write,
+	.llseek = default_llseek,
+};
+#endif
+
+static void cb_f2fs_unlink_enter(void *ignore, struct inode *dir,
+                                          struct dentry *dentry)
+{
+	int written = 0;
+	char *file;
+	struct inode *inode = d_inode(dentry);
+	char *path, pathbuf[MAX_TRACE_PATHBUF_LEN];
+
+	if (g_unlink_switch == 0) {
+		return;
+	}
+
+	path = android_fstrace_get_pathname(pathbuf,
+										MAX_TRACE_PATHBUF_LEN,
+										inode);
+	if(strlen(pfliter_front)) {
+		if(strstr(path, pfliter_front) && strstr(path, pfliter_back)) {
+			rlen = strlen(remove_buf);
+			if (rlen >= sizeof(remove_buf) - 1) {
+				memset(remove_buf, 0, sizeof(remove_buf));
+				rlen = 0;
+			}
+			file = strrchr(path, '/');
+			written = snprintf(remove_buf + rlen, sizeof(remove_buf) - rlen -1, "F:%s T:%s \n", file, current->comm);
+			rlen += written;
+		}
+	} else {
+		if(strstr(path, pfliter_buf)) {
+			rlen = strlen(remove_buf);
+			if (rlen >= sizeof(remove_buf) - 1) {
+				memset(remove_buf, 0, sizeof(remove_buf));
+				rlen = 0;
+			}
+			file = strrchr(path, '/');
+			written = snprintf(remove_buf + rlen, sizeof(remove_buf) - rlen -1, "F:%s T:%s \n", file, current->comm);
+			rlen += written;
+		}
+	}
+};
+
+
+static void file_monitor_register_tracepoints(void)
+{
+	int ret;
+
+	ret = register_trace_f2fs_unlink_enter(cb_f2fs_unlink_enter, NULL);
+
+	return;
+}
+
+static void f2fs_unregister_tracepoint_probes(void)
+{
+	unregister_trace_f2fs_unlink_enter(cb_f2fs_unlink_enter, NULL);
+}
+
+int __init file_monitor_init(void)
+{
+	file_monitor_procfs = proc_mkdir("oplus_storage/file_monitor", NULL);
+	if (file_monitor_procfs == NULL) {
+		pr_err(FILE_MONITOR_LOG_TAG" Failed to create file_monitor procfs\n");
+		return -EFAULT;
+	}
+
+	path_filter_file = proc_create("path_filter", 0644, file_monitor_procfs, &path_filter_fops);
+	if (path_filter_file == NULL) {
+		pr_err(FILE_MONITOR_LOG_TAG" Failed to create file oplus_storage/file_monitor/path_filter\n");
+		return -EFAULT;
+	}
+
+	remove_event_file = proc_create("remove_event", 0644, file_monitor_procfs, &remove_event_fops);
+	if (remove_event_file == NULL) {
+		pr_err(FILE_MONITOR_LOG_TAG" Failed to create file oplus_storage/file_monitor/remove_event\n");
+		return -EFAULT;
+	}
+
+	unlink_switch_file = proc_create("unlink_switch", 0644, file_monitor_procfs, &unlink_switch_fops);
+	if (unlink_switch_file == NULL) {
+		pr_err(FILE_MONITOR_LOG_TAG" Failed to create file oplus_storage/file_monitor/unlink_switch\n");
+		return -EFAULT;
+	}
+
+	memset(pfliter_buf, 0, sizeof(pfliter_buf));
+	memset(remove_buf, 0, sizeof(remove_buf));
+	strcpy(pfliter_buf, "file_monitor");
+	file_monitor_register_tracepoints();
+
+	return 0;
+}
+
+void __exit file_monitor_exit(void)
+{
+	remove_proc_entry("path_filter", file_monitor_procfs);
+	remove_proc_entry("remove_event", file_monitor_procfs);
+	remove_proc_entry("unlink_switch", file_monitor_procfs);
+
+	f2fs_unregister_tracepoint_probes();
+}
+
+module_init(file_monitor_init);
+module_exit(file_monitor_exit);
+
+MODULE_DESCRIPTION("oplus file monitor");
+MODULE_VERSION("1.0");
+MODULE_LICENSE("GPL v2");
+
diff --git a/drivers/soc/oplus/storage/common/io_metrics/Kconfig b/drivers/soc/oplus/storage/common/io_metrics/Kconfig
new file mode 100644
index 000000000..bd9296a49
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/Kconfig
@@ -0,0 +1,19 @@
+config OPLUS_FEATURE_STORAGE_IO_METRICS_QCOM
+  tristate "config oplus_bsp_storage_io_metrics"
+  help
+    define this config to compile oplus_bsp_storage_io_metrics for device register
+
+config OPLUS_FEATURE_STORAGE_IO_METRICS_MTK
+  tristate "config oplus_bsp_storage_io_metrics"
+  help
+    define this config to compile oplus_bsp_storage_io_metrics for device register
+
+config OPLUS_FEATURE_STORAGE_IO_METRICS_DEBUG
+  tristate "config oplus_bsp_storage_io_metrics"
+  help
+    define this config to compile oplus_bsp_storage_io_metrics for device register
+
+config OPLUS_FEATURE_STORAGE_IO_METRICS
+  tristate "config oplus_bsp_storage_io_metrics"
+  help
+    define this config to compile oplus_bsp_storage_io_metrics for device register
diff --git a/drivers/soc/oplus/storage/common/io_metrics/Makefile b/drivers/soc/oplus/storage/common/io_metrics/Makefile
new file mode 100644
index 000000000..5504297bc
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/Makefile
@@ -0,0 +1,27 @@
+LINUXINCLUDE += -I$(srctree)/
+
+ifeq ($(CONFIG_OPLUS_FEATURE_STORAGE_IO_METRICS_QCOM),y)
+ccflags-y += -DSOC_PLATFORM_IS_QCOM
+else ifeq ($(CONFIG_OPLUS_FEATURE_STORAGE_IO_METRICS_MTK),y)
+ccflags-y += -DSOC_PLATFORM_IS_MTK
+endif
+
+ifeq ($(CONFIG_OPLUS_FEATURE_STORAGE_IO_METRICS_DEBUG),y)
+ccflags-y += -DCONFIG_OPLUS_FEATURE_STORAGE_IO_METRICS_DEBUG=1
+endif
+
+ifeq ($(OPLUS_OUT_OF_TREE_KO),y)
+ccflags-y += -DCONFIG_OPLUS_FEATURE_STORAGE_IO_METRICS=1
+endif
+
+ifeq ($(CONFIG_OPLUS_FEATURE_STORAGE_IO_METRICS),y)
+CONFIG_OPLUS_FEATURE_STORAGE_F2FS=y
+endif
+
+obj-$(CONFIG_OPLUS_FEATURE_STORAGE_IO_METRICS) += oplus_bsp_storage_io_metrics.o
+oplus_bsp_storage_io_metrics-y += procfs.o
+oplus_bsp_storage_io_metrics-y += io_metrics_entry.o
+oplus_bsp_storage_io_metrics-y += block_metrics.o
+oplus_bsp_storage_io_metrics-$(CONFIG_OPLUS_FEATURE_STORAGE_F2FS) += f2fs_metrics.o
+oplus_bsp_storage_io_metrics-y += ufs_metrics.o
+oplus_bsp_storage_io_metrics-y += abnormal_io.o
diff --git a/drivers/soc/oplus/storage/common/io_metrics/abnormal_io.c b/drivers/soc/oplus/storage/common/io_metrics/abnormal_io.c
new file mode 100644
index 000000000..97dbc4d4e
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/abnormal_io.c
@@ -0,0 +1,896 @@
+#include "io_metrics_entry.h"
+#include "abnormal_io.h"
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+#include "fs/f2fs/f2fs.h"
+#include "fs/f2fs/segment.h"
+#include "fs/f2fs/node.h"
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+#include "drivers/scsi/ufs/ufs.h"
+#else
+#include <ufs/ufshcd.h>
+#endif
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+#include <trace/events/f2fs.h>
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+#include <trace/events/block.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 4, 0)
+#include <trace/events/ufs.h>
+#endif
+#include <linux/preempt.h>
+#include <linux/fs.h>
+#include <linux/blk_types.h>
+
+#define BUFFER_SIZE  (1<<20)
+#define CACHELINE_SIZE 64
+#define COUNTER_BIT_SIZE 20
+#define TP_BIT_SIZE 8
+
+#define DAY_TO_SECONDS(n) ((n) * 24 * 3600LL)
+#define DUMP_MINIMUM_INTERVAL (5 * 60)
+#define DUMP_LIMIT_1_DAY 10
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0))
+#define RWBS_PAD 2
+#else
+#define RWBS_PAD 0
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+
+atomic_t abnormal_io_enabled = ATOMIC_INIT(0);
+bool abnormal_io_trigger;
+atomic_t multiple_dump;
+int abnormal_io_dump_min_interval_s;
+int abnormal_io_dump_limit_1_day;
+static time64_t last_dump_seconds;
+static time64_t base_dump_seconds;
+
+enum TP_t {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    F2FS_SUBMIT_PAGE_WRITE = 0,
+    F2FS_SUBMIT_PAGE_BIO,
+    F2FS_FILEMAP_FAULT,
+    F2FS_SUBMIT_READ_BIO,
+    F2FS_SUBMIT_WRITE_BIO,
+    F2FS_MAP_BLOCKS,
+    F2FS_DATAREAD_START,
+    F2FS_DATAREAD_END,
+    F2FS_DATAWRITE_START,
+    F2FS_DATAWRITE_END,
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+    BLOCK_RQ_ISSUE,
+    BLOCK_RQ_COMPLETE,
+    BLOCK_GETRQ,
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    BLOCK_BIO_FRONTMERGE,
+    BLOCK_BIO_BACKMERGE,
+    BLOCK_BIO_REMAP,
+    UFSHCD_COMMAND
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+};
+
+/* 24Byte */
+struct entry_head_t {
+    u64 magic; /* 0x0a63697274656d6f69 or "iometrics" */
+    u64 timestamp;
+    u32 counter: COUNTER_BIT_SIZE;
+    u32 in_nmi: 1;
+    u32 in_hardirq: 1;
+    u32 in_softirq: 1;
+    u32 in_atomic: 1;
+    u32  tp_t: TP_BIT_SIZE;
+    int pid;
+};
+
+/* 当前结构对tb中具体struct的修改不能超过40B，不然影响性能*/
+struct entry_t {
+    struct entry_head_t head;
+#pragma pack(push, 1)
+    union {
+        char pad[CACHELINE_SIZE - sizeof(struct entry_head_t)];
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+        struct {
+            int major;
+            int minor;
+            u64 ino;
+            u64 index;
+            u64 old_blkaddr;
+            u64 new_blkaddr;
+        } f2fs_submit_page_write; /* (struct page *page, struct f2fs_io_info *fio) */
+        struct {
+            int major;
+            int minor;
+            u64 ino;
+            u64 index;
+            int op;
+            int op_flags;
+            u8 type;
+            u8 temp;
+            u8 pad[6];
+        } f2fs_submit_page_bio;   /* (struct page *page, struct f2fs_io_info *fio) */
+        struct {
+            int major;
+            int minor;
+            u64 ino;
+            u64 index;
+            u64 ret;
+            u64 pad;
+        } f2fs_filemap_fault;     /* (struct inode *inode, pgoff_t index, u64 ret) */
+        struct {
+            int major;
+            int minor;
+            sector_t sector;
+            int op;
+            int op_flags;
+            int type;
+            u32 size;
+            u8 pad[8];
+        } f2fs_submit_read_bio;   /* (struct super_block *sb, int type, struct bio *bio) */
+        struct {
+            int major;
+            int minor;
+            sector_t sector;
+            int op;
+            int op_flags;
+            int type;
+            u32 size;
+            u8 pad[8];
+        } f2fs_submit_write_bio;  /* (struct super_block *sb, int type, struct bio *bio) */
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+#define PATHNAME_LEN    16
+        struct {
+            u64 offset;
+            u64 ino;
+            int bytes;
+            pid_t pid;
+            u8 pathname[PATHNAME_LEN];
+        } f2fs_dataread_start;   /* (struct inode *inode, loff_t offset, int bytes, pid_t pid,
+                                    char *pathname, char *command) */
+        struct {
+            u64 offset;
+            u64 ino;
+            int bytes;
+            u8 pad[20];
+        } f2fs_dataread_end;     /* (struct inode *inode, loff_t offset, int bytes) */
+        struct {
+            u64 offset;
+            u64 ino;
+            int bytes;
+            pid_t pid;
+            u8 pathname[PATHNAME_LEN];
+        } f2fs_datawrite_start;  /* (struct inode *inode, loff_t offset, int bytes, pid_t pid,
+                                    char *pathname, char *command) */
+        struct {
+            u64 offset;
+            u64 ino;
+            int bytes;
+            u8 pad[20];
+        } f2fs_datawrite_end;    /* (struct inode *inode, loff_t offset, int bytes) */
+#endif
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+        struct {
+            int major;
+            int minor;
+            sector_t sector;
+            u8 rwbs[RWBS_LEN];   /* 8B */
+            u32 bytes;
+            u8 pad[12 - RWBS_PAD];
+        } block_rq_issue;        /* (struct request *rq) */
+        struct {
+            int major;
+            int minor;
+            sector_t sector;
+            u8 rwbs[RWBS_LEN];   /* 8B */
+            u64 elapsed;
+            u32 bytes;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+            int error;
+#else
+            u8 pad[4 - RWBS_PAD];
+#endif
+        } block_rq_complete;     /* (struct request *rq, int error, u32 nr_bytes) */
+        struct {
+            sector_t sector;
+            u8 comm[TASK_COMM_LEN];
+            u8 rwbs[RWBS_LEN];   /* 8B */
+            u32 bytes;
+            u8 pad[4 - RWBS_PAD];
+        } block_getrq;           /* (struct bio *bio) */
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+        struct {
+            sector_t sector;
+            u8 rwbs[RWBS_LEN];   /* 8B */
+            u32 bytes;
+            u8 pad[20 - RWBS_PAD];
+        } block_bio_frontmerge;  /* (struct bio *bio) */
+        struct {
+            sector_t sector;
+            u8 rwbs[RWBS_LEN];   /* 8B */
+            u32 bytes;
+            u8 pad[20 - RWBS_PAD];
+        } block_bio_backmerge;   /* (struct bio *bio) */
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+        struct {
+            int old_major;
+            int old_minor;
+            int new_major;
+            int new_minor;
+            sector_t old_sector;
+            sector_t new_sector;
+            u32 bytes;
+            u8 pad[4];
+        } block_bio_remap;       /* (struct bio *bio, dev_t dev, sector_t from) */
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 4, 0)
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+        struct {
+            u64 lba;
+            u64 elapsed;
+            u32 tag;
+            u32 doorbell;
+            int transfer_len;
+            u32 intr;
+            u8 opcode;
+            u8 group_id;
+            u8 str_t;
+            u8 gear_rx: 4;
+            u8 gear_tx: 4;
+            u8 lane_rx: 4;
+            u8 lane_tx: 4;
+            u8 pwr_rx: 4;
+            u8 pwr_tx: 4;
+            u8 hwq_id;
+            u8 pad[1];
+        } ufshcd_command;       /* (const char *dev_name, enum ufs_trace_str_t str_t, u32 tag,
+                     u32 doorbell, int transfer_len, u32 intr, u64 lba, u8 opcode, u8 group_id)*/
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+#endif
+    } tp;
+#pragma pack(pop)
+};
+
+struct ring_buffer_tail {
+    u64 ktime_ns;
+    time64_t utc_seconds; /* seconds since 1970 */
+    int writers;
+};
+struct ring_buffer_t {
+    u32 size;
+    u32 mask;
+    atomic_t cursor;
+    atomic_t writers;
+    char pad[48];
+    char data[BUFFER_SIZE + sizeof(struct ring_buffer_tail)];
+} ring_buffer __attribute__((aligned(CACHELINE_SIZE)));
+
+void ring_buffer_int(void)
+{
+    ring_buffer.size = BUFFER_SIZE;
+    ring_buffer.mask = (ring_buffer.size >> 6) - 1;
+    atomic_set(&ring_buffer.cursor, -1);
+    atomic_set(&ring_buffer.writers, 0);
+}
+
+static __always_inline struct entry_t *get_pentry_from_ring_buffer(enum TP_t tp)
+{
+    struct entry_t* pentry = (struct entry_t*)ring_buffer.data;
+    u32 tmp = 0;
+    u32 slot = 0;
+
+    tmp = (u32)atomic_inc_return(&ring_buffer.cursor);
+    slot = ring_buffer.mask & tmp;
+    pentry = pentry + slot;
+    pentry->head.timestamp = ktime_get_ns();
+    pentry->head.magic = 0x63697274656d6f69;
+    pentry->head.counter = tmp;
+    pentry->head.tp_t = tp;
+    pentry->head.pid = current->pid;
+    pentry->head.in_nmi = in_nmi() ? 1 : 0;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+    pentry->head.in_hardirq = in_hardirq() ? 1 : 0;
+#else
+    pentry->head.in_hardirq = 0;
+#endif
+    pentry->head.in_softirq = in_softirq() ? 1 : 0;
+    pentry->head.in_atomic = in_atomic() ? 1 : 0;
+
+    return pentry;
+}
+
+static  __always_inline void ring_buffer_writer_inc(void)
+{
+    atomic_inc(&ring_buffer.writers);
+}
+
+static  __always_inline void ring_buffer_writer_dec(void)
+{
+    atomic_dec(&ring_buffer.writers);
+}
+
+static int dump_data_to_file(const char *logpath)
+{
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0))
+	return 0;
+#else
+    struct file *fp = NULL;
+    int ret = 0;
+    loff_t offset = 0;
+    struct ring_buffer_tail *ptail = NULL;
+
+    ptail = (struct ring_buffer_tail *)(ring_buffer.data + BUFFER_SIZE);
+    ptail->ktime_ns = ktime_get_ns();
+    ptail->utc_seconds = ktime_get_real_seconds();
+    ptail->writers = atomic_read(&ring_buffer.writers);
+    io_metrics_print("writers: %d\n", ptail->writers);
+    fp = filp_open(logpath, O_CREAT | O_WRONLY | O_TRUNC, 0666);
+    if (IS_ERR(fp)) {
+        ret = PTR_ERR(fp);
+    } else {
+        ret = kernel_write(fp, ring_buffer.data, BUFFER_SIZE + sizeof(struct ring_buffer_tail), &offset);
+        if (!ret) {
+           io_metrics_print("kernel_write err: %d\n", ret);
+        }
+        filp_close(fp, NULL);
+    }
+    ring_buffer_int();
+	return ret;
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+}
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+void record_f2fs_submit_page_write(void *ignore, struct page *page, struct f2fs_io_info *fio)
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(F2FS_SUBMIT_PAGE_WRITE);
+    pentry->tp.f2fs_submit_page_write.major = MAJOR(page_file_mapping(page)->host->i_sb->s_dev);
+    pentry->tp.f2fs_submit_page_write.minor = MINOR(page_file_mapping(page)->host->i_sb->s_dev);
+    pentry->tp.f2fs_submit_page_write.ino = page_file_mapping(page)->host->i_ino;
+    pentry->tp.f2fs_submit_page_write.index = (u64)page->index;
+    pentry->tp.f2fs_submit_page_write.old_blkaddr = (u64)fio->old_blkaddr;
+    pentry->tp.f2fs_submit_page_write.new_blkaddr = (u64)fio->new_blkaddr;
+    ring_buffer_writer_dec();
+}
+void record_f2fs_submit_page_bio(void *ignore, struct page *page, struct f2fs_io_info *fio)
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(F2FS_SUBMIT_PAGE_BIO);
+    pentry->tp.f2fs_submit_page_bio.major = MAJOR(page_file_mapping(page)->host->i_sb->s_dev);
+    pentry->tp.f2fs_submit_page_bio.minor = MINOR(page_file_mapping(page)->host->i_sb->s_dev);
+    pentry->tp.f2fs_submit_page_bio.ino = page_file_mapping(page)->host->i_ino;
+    pentry->tp.f2fs_submit_page_bio.index = (u64)page->index;
+    pentry->tp.f2fs_submit_page_bio.op = fio->op;
+    pentry->tp.f2fs_submit_page_bio.op_flags = fio->op_flags;
+    pentry->tp.f2fs_submit_page_bio.type = (u8)fio->type;
+    pentry->tp.f2fs_submit_page_bio.temp = (u8)fio->temp;
+    ring_buffer_writer_dec();
+}
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+void record_f2fs_filemap_fault(void *ignore, struct inode *inode, pgoff_t index, unsigned long ret)
+#else
+void record_f2fs_filemap_fault(void *ignore, struct inode *inode, unsigned long index, vm_flags_t flag, vm_fault_t ret)
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0) */
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(F2FS_FILEMAP_FAULT);
+    pentry->tp.f2fs_filemap_fault.major = MAJOR(inode->i_sb->s_dev);
+    pentry->tp.f2fs_filemap_fault.minor = MINOR(inode->i_sb->s_dev);
+    pentry->tp.f2fs_filemap_fault.ino = inode->i_ino;
+    pentry->tp.f2fs_filemap_fault.index = index;
+    pentry->tp.f2fs_filemap_fault.ret = ret;
+    ring_buffer_writer_dec();
+}
+void record_f2fs_submit_read_bio(void *ignore, struct super_block *sb, int type, struct bio *bio)
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(F2FS_SUBMIT_READ_BIO);
+    pentry->tp.f2fs_submit_read_bio.major = MAJOR(sb->s_dev);
+    pentry->tp.f2fs_submit_read_bio.minor = MINOR(sb->s_dev);
+    pentry->tp.f2fs_submit_read_bio.sector = bio->bi_iter.bi_sector;
+    pentry->tp.f2fs_submit_read_bio.op = bio_op(bio);
+    pentry->tp.f2fs_submit_read_bio.op_flags = bio->bi_opf;
+    pentry->tp.f2fs_submit_read_bio.type = type;
+    pentry->tp.f2fs_submit_read_bio.size = bio->bi_iter.bi_size;
+    ring_buffer_writer_dec();
+}
+void record_f2fs_submit_write_bio(void *ignore, struct super_block *sb, int type, struct bio *bio)
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(F2FS_SUBMIT_WRITE_BIO);
+    pentry->tp.f2fs_submit_write_bio.major = MAJOR(sb->s_dev);
+    pentry->tp.f2fs_submit_write_bio.minor = MINOR(sb->s_dev);
+    pentry->tp.f2fs_submit_write_bio.sector = bio->bi_iter.bi_sector;
+    pentry->tp.f2fs_submit_write_bio.op = bio_op(bio);
+    pentry->tp.f2fs_submit_write_bio.op_flags = bio->bi_opf;
+    pentry->tp.f2fs_submit_write_bio.type = type;
+    pentry->tp.f2fs_submit_write_bio.size = bio->bi_iter.bi_size;
+    ring_buffer_writer_dec();
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+void record_f2fs_dataread_start(void *ignore, struct inode *inode, loff_t offset, int bytes,
+                                      pid_t pid, char *pathname, char *command)
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(F2FS_DATAREAD_START);
+    pentry->tp.f2fs_dataread_start.offset = offset;
+    pentry->tp.f2fs_dataread_start.ino = inode->i_ino;
+    pentry->tp.f2fs_dataread_start.bytes = bytes;
+    pentry->tp.f2fs_dataread_start.pid = pid;
+    strncpy(pentry->tp.f2fs_dataread_start.pathname, pathname, PATHNAME_LEN-1);
+    pentry->tp.f2fs_dataread_start.pathname[PATHNAME_LEN-1] = 0;
+    ring_buffer_writer_dec();
+}
+
+void record_f2fs_dataread_end(void *ignore, struct inode *inode, loff_t offset, int bytes)
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(F2FS_DATAREAD_END);
+    pentry->tp.f2fs_dataread_end.offset = offset;
+    pentry->tp.f2fs_dataread_end.ino = inode->i_ino;
+    pentry->tp.f2fs_dataread_end.bytes = bytes;
+    ring_buffer_writer_dec();
+}
+
+void record_f2fs_datawrite_start(void *ignore, struct inode *inode, loff_t offset, int bytes,
+                                      pid_t pid, char *pathname, char *command)
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(F2FS_DATAWRITE_START);
+    pentry->tp.f2fs_datawrite_start.offset = offset;
+    pentry->tp.f2fs_datawrite_start.ino = inode->i_ino;
+    pentry->tp.f2fs_datawrite_start.bytes = bytes;
+    pentry->tp.f2fs_datawrite_start.pid = pid;
+    strncpy(pentry->tp.f2fs_datawrite_start.pathname, pathname, PATHNAME_LEN-1);
+    pentry->tp.f2fs_datawrite_start.pathname[PATHNAME_LEN-1] = 0;
+    ring_buffer_writer_dec();
+}
+
+void record_f2fs_datawrite_end(void *ignore, struct inode *inode, loff_t offset, int bytes)
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(F2FS_DATAWRITE_END);
+    pentry->tp.f2fs_datawrite_end.offset = offset;
+    pentry->tp.f2fs_datawrite_end.ino = inode->i_ino;
+    pentry->tp.f2fs_datawrite_end.bytes = bytes;
+    ring_buffer_writer_dec();
+}
+#endif
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+void record_block_rq_issue(void *ignore, struct request *rq)
+#else
+void record_block_rq_issue(void *ignore, struct request_queue *q,
+                             struct request *rq)
+#endif
+{
+    struct entry_t *pentry = NULL;
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+    dev_t dev = rq->rq_disk ? disk_devt(rq->rq_disk) : 0;
+#else
+    dev_t dev = rq->part ? (rq->part->bd_disk ? disk_devt(rq->part->bd_disk) : 0) : 0;
+#endif
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(BLOCK_RQ_ISSUE);
+    pentry->tp.block_rq_issue.major = MAJOR(dev);
+    pentry->tp.block_rq_issue.minor = MINOR(dev);
+    pentry->tp.block_rq_issue.sector = blk_rq_trace_sector(rq);
+    pentry->tp.block_rq_issue.bytes = blk_rq_bytes(rq);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+    blk_fill_rwbs(pentry->tp.block_rq_issue.rwbs, rq->cmd_flags);
+#else
+    blk_fill_rwbs(pentry->tp.block_rq_issue.rwbs, rq->cmd_flags, RWBS_LEN);
+#endif
+    ring_buffer_writer_dec();
+}
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+static void record_block_rq_complete(void *ignore, struct request *rq,
+                      int error, unsigned int nr_bytes)
+#else
+static void record_block_rq_complete(void *ignore, struct request *rq,
+                      blk_status_t error, unsigned int nr_bytes)
+#endif
+
+{
+    struct entry_t *pentry = NULL;
+    u64 io_complete_time_ns = ktime_get_ns();
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+        dev_t dev = rq->rq_disk ? disk_devt(rq->rq_disk) : 0;
+#else
+        dev_t dev = rq->part ? (rq->part->bd_disk ? disk_devt(rq->part->bd_disk) : 0) : 0;
+#endif
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(BLOCK_RQ_COMPLETE);
+    pentry->tp.block_rq_complete.major = MAJOR(dev);
+    pentry->tp.block_rq_complete.minor = MINOR(dev);
+    pentry->tp.block_rq_complete.sector = blk_rq_trace_sector(rq);
+    pentry->tp.block_rq_complete.bytes = nr_bytes;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)
+    pentry->tp.block_rq_complete.error = error;
+#endif
+    pentry->tp.block_rq_complete.elapsed = (rq->start_time_ns && (io_complete_time_ns > rq->start_time_ns)) ?
+                                           (io_complete_time_ns - rq->start_time_ns) : 0;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+    blk_fill_rwbs(pentry->tp.block_rq_complete.rwbs, rq->cmd_flags);
+#else
+    blk_fill_rwbs(pentry->tp.block_rq_complete.rwbs, rq->cmd_flags, RWBS_LEN);
+#endif
+    ring_buffer_writer_dec();
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+void record_block_getrq(void *ignore, struct bio *bio)
+#else
+void record_block_getrq(void *ignore, struct request_queue *q, struct bio *bio, int rw)
+#endif
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(BLOCK_GETRQ);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+    pentry->tp.block_getrq.sector = bio->bi_iter.bi_sector;
+    pentry->tp.block_getrq.bytes = bio_sectors(bio) << 9;
+    blk_fill_rwbs(pentry->tp.block_getrq.rwbs, bio->bi_opf);
+#else
+    pentry->tp.block_getrq.sector = bio ? bio->bi_iter.bi_sector : 0;
+    pentry->tp.block_getrq.bytes = bio ? (bio_sectors(bio) << 9) : 0;
+    blk_fill_rwbs(pentry->tp.block_getrq.rwbs, bio ? bio->bi_opf : 0, bio ? bio_sectors(bio) : 0);
+#endif
+    memcpy(pentry->tp.block_getrq.comm, current->comm, TASK_COMM_LEN);
+    ring_buffer_writer_dec();
+}
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+void record_block_bio_frontmerge(void *ignore, struct bio *bio)
+#else
+void record_block_bio_frontmerge(void *ignore, struct request_queue *q, struct request *rq, struct bio *bio)
+#endif
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(BLOCK_BIO_FRONTMERGE);
+    pentry->tp.block_bio_frontmerge.sector = bio->bi_iter.bi_sector;
+    pentry->tp.block_bio_frontmerge.bytes = bio_sectors(bio) << 9;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+    blk_fill_rwbs(pentry->tp.block_bio_frontmerge.rwbs, bio->bi_opf);
+#else
+    blk_fill_rwbs(pentry->tp.block_bio_frontmerge.rwbs, bio->bi_opf, RWBS_LEN);
+#endif
+    ring_buffer_writer_dec();
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+void record_block_bio_backmerge(void *ignore, struct bio *bio)
+#else
+void record_block_bio_backmerge(void *ignore, struct request_queue *q, struct request *rq, struct bio *bio)
+#endif
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(BLOCK_BIO_BACKMERGE);
+    pentry->tp.block_bio_backmerge.sector = bio->bi_iter.bi_sector;
+    pentry->tp.block_bio_backmerge.bytes = bio_sectors(bio) << 9;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+    blk_fill_rwbs(pentry->tp.block_bio_backmerge.rwbs, bio->bi_opf);
+#else
+    blk_fill_rwbs(pentry->tp.block_bio_backmerge.rwbs, bio->bi_opf, RWBS_LEN);
+#endif
+    ring_buffer_writer_dec();
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+void record_block_bio_remap(void *ignore, struct bio *bio, dev_t dev, sector_t from)
+#else
+void record_block_bio_remap(void *ignore, struct request_queue *q, struct bio *bio, dev_t dev, sector_t from)
+#endif
+{
+    struct entry_t *pentry = NULL;
+
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(BLOCK_BIO_REMAP);
+    pentry->tp.block_bio_remap.old_major = MAJOR(dev);
+    pentry->tp.block_bio_remap.old_minor = MINOR(dev);
+    pentry->tp.block_bio_remap.new_major = MAJOR(bio_dev(bio));
+    pentry->tp.block_bio_remap.new_minor = MINOR(bio_dev(bio));
+    pentry->tp.block_bio_remap.old_sector = from;
+    pentry->tp.block_bio_remap.new_sector = bio->bi_iter.bi_sector;
+    pentry->tp.block_bio_remap.bytes = bio_sectors(bio) << 9;
+    ring_buffer_writer_dec();
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(6, 1, 0)
+void record_ufshcd_command(void *ignore, const char *dev_name, enum ufs_trace_str_t str_t,
+                    unsigned int tag, u32 doorbell, u32 hwq_id, int transfer_len, u32 intr,
+                    u64 lba, u8 opcode, u8 group_id)
+#elif LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+void record_ufshcd_command(void *ignore, const char *dev_name, enum ufs_trace_str_t str_t,
+                    unsigned int tag, u32 doorbell, int transfer_len, u32 intr,
+                    u64 lba, u8 opcode, u8 group_id)
+#elif LINUX_VERSION_CODE > KERNEL_VERSION(5, 10, 0)
+void record_ufshcd_command(void *ignore, const char *dev_name, const char *str,
+                    unsigned int tag, u32 doorbell, int transfer_len, u32 intr,
+                    u64 lba, u8 opcode, u8 group_id)
+#elif LINUX_VERSION_CODE > KERNEL_VERSION(5, 4, 0)
+void record_ufshcd_command(void *ignore, const char *dev_name, const char *str,
+                    unsigned int tag, u32 doorbell, int transfer_len, u32 intr,
+                    u64 lba, u8 opcode)
+#endif
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 4, 0)
+{
+    struct entry_t *pentry = NULL;
+#if 0
+#ifdef CONFIG_ARCH_QCOM || CONFIG_ARCH_MEDIATEK
+    struct ufs_hba * hba = NULL;
+    struct ufshcd_lrb *lrbp = NULL;
+#endif
+#endif
+    if (unlikely(atomic_read(&multiple_dump))) {
+        return;
+    }
+    ring_buffer_writer_inc();
+    pentry = get_pentry_from_ring_buffer(UFSHCD_COMMAND);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+    pentry->tp.ufshcd_command.str_t = (u8)str_t;
+#else
+    /* To improve performance, only record completion commands */
+    if (!strcmp(str, "complete")) {
+        pentry->tp.ufshcd_command.str_t = 1; /* UFS_CMD_COMP */
+    } else {
+        pentry->tp.ufshcd_command.str_t = 0XFF; /* N/A */
+    }
+#endif
+    pentry->tp.ufshcd_command.tag = tag;
+    pentry->tp.ufshcd_command.doorbell = doorbell;
+    pentry->tp.ufshcd_command.transfer_len = transfer_len;
+    pentry->tp.ufshcd_command.intr = intr;
+    pentry->tp.ufshcd_command.lba = lba;
+    pentry->tp.ufshcd_command.opcode = opcode;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(6, 1, 0)
+    pentry->tp.ufshcd_command.hwq_id = (u8)hwq_id;
+#else
+    pentry->tp.ufshcd_command.hwq_id = 0;
+#endif
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 10, 0)
+    pentry->tp.ufshcd_command.group_id = group_id;
+#else
+    pentry->tp.ufshcd_command.group_id = 0;
+#endif
+#if 0
+#ifdef CONFIG_ARCH_QCOM
+    hba = ufs_qcom_hosts[0]->hba;
+    lrbp = &hba->lrb[tag];
+    pentry->tp.ufshcd_command.elapsed = (str_t == UFS_CMD_COMP) ?
+              lrbp->compl_time_stamp - lrbp->issue_time_stamp : 0;
+    pentry->tp.ufshcd_command.gear_rx = hba->pwr_info.gear_rx;
+    pentry->tp.ufshcd_command.gear_tx = hba->pwr_info.gear_tx;
+    pentry->tp.ufshcd_command.lane_rx = hba->pwr_info.lane_rx;
+    pentry->tp.ufshcd_command.lane_tx = hba->pwr_info.lane_tx;
+    pentry->tp.ufshcd_command.pwr_rx = hba->pwr_info.pwr_rx;
+    pentry->tp.ufshcd_command.pwr_tx = hba->pwr_info.pwr_tx;
+#endif
+#endif
+    ring_buffer_writer_dec();
+}
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(5, 4, 0) */
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+
+void abnormal_io_register_tracepoint_probes(void)
+{
+    int ret;
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    ret = register_trace_f2fs_submit_page_write(record_f2fs_submit_page_write, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_submit_page_bio(record_f2fs_submit_page_bio, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_filemap_fault(record_f2fs_filemap_fault, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_submit_read_bio(record_f2fs_submit_read_bio, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_submit_write_bio(record_f2fs_submit_write_bio, NULL);
+    WARN_ON(ret);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+    ret = register_trace_f2fs_dataread_start(record_f2fs_dataread_start, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_dataread_end(record_f2fs_dataread_end, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_datawrite_start(record_f2fs_datawrite_start, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_datawrite_end(record_f2fs_datawrite_end, NULL);
+    WARN_ON(ret);
+#endif
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+    ret = register_trace_block_rq_issue(record_block_rq_issue, NULL);
+    WARN_ON(ret);
+    ret = register_trace_block_rq_complete(record_block_rq_complete, NULL);
+    WARN_ON(ret);
+    ret = register_trace_block_getrq(record_block_getrq, NULL);
+    WARN_ON(ret);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    ret = register_trace_block_bio_frontmerge(record_block_bio_frontmerge, NULL);
+    WARN_ON(ret);
+    ret = register_trace_block_bio_backmerge(record_block_bio_backmerge, NULL);
+    WARN_ON(ret);
+    ret = register_trace_block_bio_remap(record_block_bio_remap, NULL);
+    WARN_ON(ret);
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 4, 0)
+    ret = register_trace_ufshcd_command(record_ufshcd_command, NULL);
+    WARN_ON(ret);
+#endif
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+}
+
+void abnormal_io_unregister_tracepoint_probes(void)
+{
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    unregister_trace_f2fs_submit_page_write(record_f2fs_submit_page_write, NULL);
+    unregister_trace_f2fs_submit_page_bio(record_f2fs_submit_page_bio, NULL);
+    unregister_trace_f2fs_filemap_fault(record_f2fs_filemap_fault, NULL);
+    unregister_trace_f2fs_submit_read_bio(record_f2fs_submit_read_bio, NULL);
+    unregister_trace_f2fs_submit_write_bio(record_f2fs_submit_write_bio, NULL);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 15, 0)
+    unregister_trace_f2fs_dataread_start(record_f2fs_dataread_start, NULL);
+    unregister_trace_f2fs_dataread_end(record_f2fs_dataread_end, NULL);
+    unregister_trace_f2fs_datawrite_start(record_f2fs_datawrite_start, NULL);
+    unregister_trace_f2fs_datawrite_end(record_f2fs_datawrite_end, NULL);
+#endif
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+    unregister_trace_block_rq_issue(record_block_rq_issue, NULL);
+    unregister_trace_block_rq_complete(record_block_rq_complete, NULL);
+    unregister_trace_block_getrq(record_block_getrq, NULL);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    unregister_trace_block_bio_frontmerge(record_block_bio_frontmerge, NULL);
+    unregister_trace_block_bio_backmerge(record_block_bio_backmerge, NULL);
+    unregister_trace_block_bio_remap(record_block_bio_remap, NULL);
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 4, 0) && (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    unregister_trace_ufshcd_command(record_ufshcd_command, NULL);
+#endif
+    tracepoint_synchronize_unregister();
+}
+
+static bool over_limit_dump(time64_t current_time_seconds)
+{
+    static int dump_limit_1_day = 1;
+
+    /* 当导出日志周期超过1天时重新计数 */
+    if ((current_time_seconds - base_dump_seconds) >= DAY_TO_SECONDS(1)) {
+        base_dump_seconds = current_time_seconds;
+        dump_limit_1_day = 1;
+    } else {
+        /* 1天的导出次数超过上限时不再继续导出 */
+        if (dump_limit_1_day <= abnormal_io_dump_limit_1_day) {
+            dump_limit_1_day++;
+        } else {
+            return true;
+        }
+    }
+
+    return false;
+}
+
+int abnormal_io_dump_to_file(const char *logpath)
+{
+    int ret = 0;
+    time64_t current_time_seconds = ktime_get_boottime_seconds();
+
+    if (!atomic_read(&abnormal_io_enabled)) {
+        io_metrics_print("abnormal_io_enabled is disabled,please enable first");
+        return ret;
+    }
+    /* 控制多人同时导出操作 */
+    if (1 == atomic_inc_return(&multiple_dump)) {
+        if (((current_time_seconds - last_dump_seconds) > abnormal_io_dump_min_interval_s) &&
+                                      (!over_limit_dump(current_time_seconds))) {
+            last_dump_seconds = current_time_seconds;
+            ret = dump_data_to_file(logpath);
+        }
+    }
+    atomic_dec(&multiple_dump);
+
+    return ret;
+}
+void abnormal_io_init(void)
+{
+    if (1 != atomic_inc_return(&abnormal_io_enabled)) {
+        atomic_dec(&abnormal_io_enabled);
+        io_metrics_print("abnormal_io has registerd:%d\n", atomic_read(&abnormal_io_enabled));
+        return;
+    }
+    abnormal_io_trigger = false;
+    atomic_set(&multiple_dump, 0);
+    last_dump_seconds = 0;
+    base_dump_seconds = 0;
+    /* 默认1天导出日志的次数不可超过10次 */
+    abnormal_io_dump_limit_1_day = DUMP_LIMIT_1_DAY;
+    /* 默认两次导出日志时间间隔不得小于5分钟 */
+    abnormal_io_dump_min_interval_s = DUMP_MINIMUM_INTERVAL;
+    ring_buffer_int();
+    abnormal_io_register_tracepoint_probes();
+    if (sizeof(struct entry_t) != CACHELINE_SIZE) {
+        io_metrics_print("sizeof(struct entry_t): %d", (int)sizeof(struct entry_t));
+    } else {
+        io_metrics_print("ok");
+    }
+    io_metrics_print("init\n");
+}
+
+void abnormal_io_exit(void)
+{
+    if (0 != atomic_dec_return(&abnormal_io_enabled)) {
+        atomic_inc(&abnormal_io_enabled);
+        io_metrics_print("abnormal_io has unregisterd:%d\n", atomic_read(&abnormal_io_enabled));
+        return;
+    }
+    abnormal_io_unregister_tracepoint_probes();
+    io_metrics_print("exit\n");
+}
diff --git a/drivers/soc/oplus/storage/common/io_metrics/abnormal_io.h b/drivers/soc/oplus/storage/common/io_metrics/abnormal_io.h
new file mode 100644
index 000000000..2be346aa3
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/abnormal_io.h
@@ -0,0 +1,15 @@
+#ifndef __ABNORMAL_IO_H__
+#define __ABNORMAL_IO_H__
+#include <linux/fs.h>
+#include <linux/f2fs_fs.h>
+
+extern atomic_t abnormal_io_enabled;
+extern bool abnormal_io_trigger;
+extern int abnormal_io_dump_min_interval_s;
+extern int abnormal_io_dump_limit_1_day;
+
+int abnormal_io_dump_to_file(const char *logpath);
+void abnormal_io_init(void);
+void abnormal_io_exit(void);
+
+#endif /* __ABNORMAL_IO_H__ */
diff --git a/drivers/soc/oplus/storage/common/io_metrics/block_metrics.c b/drivers/soc/oplus/storage/common/io_metrics/block_metrics.c
new file mode 100644
index 000000000..84e88add9
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/block_metrics.c
@@ -0,0 +1,666 @@
+#include "io_metrics_entry.h"
+#include "procfs.h"
+#include "block_metrics.h"
+#include <trace/events/block.h>
+
+#define BLK_METRICS_LAT(op, size, layer)   \
+    atomic64_t blk_metrics_lat_##op##_##size##_##layer[LAT_500M_TO_MAX + 1] = {0};
+
+BLK_METRICS_LAT(read,    4k, in_blk);
+BLK_METRICS_LAT(read,    4k, in_drv);
+BLK_METRICS_LAT(write,   4k, in_blk);
+BLK_METRICS_LAT(write,   4k, in_drv);
+BLK_METRICS_LAT(read,  512k, in_blk);
+BLK_METRICS_LAT(read,  512k, in_drv);
+BLK_METRICS_LAT(write, 512k, in_blk);
+BLK_METRICS_LAT(write, 512k, in_drv);
+
+bool block_rq_issue_enabled = false;
+bool block_rq_complete_enabled = false;
+module_param(block_rq_issue_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(block_rq_issue_enabled, " Debug block_rq_issue");
+module_param(block_rq_complete_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(block_rq_complete_enabled, " Debug block_rq_complete");
+
+struct blk_metrics_struct blk_metrics[OP_MAX][CYCLE_MAX][IO_SIZE_MAX] = {0};
+spinlock_t blk_metrics_lock[OP_MAX][CYCLE_MAX][IO_SIZE_MAX];
+
+static void block_stat_update(struct request *rq, enum io_op_type op_type,
+                                                  u64 io_complete_time_ns)
+{
+    unsigned long flags;
+    u64 elapse = 0;
+    int i = 0;
+    u64 in_driver = (io_complete_time_ns > rq->io_start_time_ns) && rq->io_start_time_ns ?
+                    (io_complete_time_ns - rq->io_start_time_ns) : 0;
+    u64 in_block = (rq->io_start_time_ns > rq->start_time_ns) && rq->start_time_ns ?
+                    (rq->io_start_time_ns - rq->start_time_ns) : 0;
+    u64 in_d_and_b = in_driver + in_block;
+    u64 in_driver_lat_range = LAT_500M_TO_MAX;
+    u64 in_block_lat_range = LAT_500M_TO_MAX;
+    enum io_range io_range = IO_SIZE_MAX;
+    u32 nr_bytes = blk_rq_bytes(rq);
+
+    if (nr_bytes >= IO_SIZE_512K_TO_MAX_MASK) {/* [512K, +∞) */
+        io_range = IO_SIZE_512K_TO_MAX;
+    } else if (nr_bytes > IO_SIZE_128K_TO_512K_MASK) {/* (128K, 512K) */
+        io_range = IO_SIZE_128K_TO_512K;
+    } else if (nr_bytes > IO_SIZE_32K_TO_128K_MASK) {/* (32K, 128K] */
+        io_range = IO_SIZE_32K_TO_128K;
+    } else if (nr_bytes > IO_SIZE_4K_TO_32K_MASK) {/* (4K, 32K] */
+        io_range = IO_SIZE_4K_TO_32K;
+    } else {/* (0, 4K] */
+        io_range = IO_SIZE_0_TO_4K;
+    }
+
+    /* 根据不同时间窗口计算一个采样周期内的平均耗时、最大耗时*/
+    for (i = 0; i < CYCLE_MAX; i++) {
+        elapse = io_complete_time_ns - blk_metrics[op_type][i][io_range].timestamp;
+        /* 统计复位(timestamp为0)、统计异常（timestamp比io_complete_time_ns大） */
+        if (unlikely(elapse >= io_complete_time_ns)) {
+            flags = 0;
+            spin_lock_irqsave(&blk_metrics_lock[op_type][i][io_range], flags);
+            blk_metrics[op_type][i][io_range].timestamp = io_complete_time_ns;
+            blk_metrics[op_type][i][io_range].total_cnt = 1;
+            blk_metrics[op_type][i][io_range].total_size = nr_bytes;
+            blk_metrics[op_type][i][io_range].layer[IN_BLOCK].elapse_time = in_block;
+            blk_metrics[op_type][i][io_range].layer[IN_DRIVER].elapse_time = in_driver;
+            blk_metrics[op_type][i][io_range].layer[IN_BLOCK].max_time = in_block;
+            blk_metrics[op_type][i][io_range].layer[IN_DRIVER].max_time = in_driver;
+            blk_metrics[op_type][i][io_range].max_time = in_d_and_b;
+            spin_unlock_irqrestore(&blk_metrics_lock[op_type][i][io_range], flags);
+            elapse = 0;
+            if (op_type == OP_READ) {
+                if (likely(io_range == IO_SIZE_0_TO_4K)) {
+                    lat_range_check(in_block, in_block_lat_range);
+                    lat_range_check(in_driver, in_driver_lat_range);
+                    memset(&blk_metrics_lat_read_4k_in_blk, 0, sizeof(blk_metrics_lat_read_4k_in_blk));
+                    memset(&blk_metrics_lat_read_4k_in_drv, 0, sizeof(blk_metrics_lat_read_4k_in_drv));
+                    atomic64_set(&blk_metrics_lat_read_4k_in_blk[in_block_lat_range], 1);
+                    atomic64_set(&blk_metrics_lat_read_4k_in_drv[in_driver_lat_range], 1);
+                } else if (io_range == IO_SIZE_512K_TO_MAX) {
+                    lat_range_check(in_block, in_block_lat_range);
+                    lat_range_check(in_driver, in_driver_lat_range);
+                    memset(&blk_metrics_lat_read_512k_in_blk, 0, sizeof(blk_metrics_lat_read_512k_in_blk));
+                    memset(&blk_metrics_lat_read_512k_in_drv, 0, sizeof(blk_metrics_lat_read_512k_in_drv));
+                    atomic64_set(&blk_metrics_lat_read_512k_in_blk[in_block_lat_range], 1);
+                    atomic64_set(&blk_metrics_lat_read_512k_in_drv[in_driver_lat_range], 1);
+                }
+            } else if (op_type == OP_WRITE) {
+                if (likely(io_range == IO_SIZE_0_TO_4K)) {
+                    lat_range_check(in_block, in_block_lat_range);
+                    lat_range_check(in_driver, in_driver_lat_range);
+                    memset(&blk_metrics_lat_write_4k_in_blk, 0, sizeof(blk_metrics_lat_write_4k_in_blk));
+                    memset(&blk_metrics_lat_write_4k_in_drv, 0, sizeof(blk_metrics_lat_write_4k_in_drv));
+                    atomic64_set(&blk_metrics_lat_write_4k_in_blk[in_block_lat_range], 1);
+                    atomic64_set(&blk_metrics_lat_write_4k_in_drv[in_driver_lat_range], 1);
+                } else if (io_range == IO_SIZE_512K_TO_MAX) {
+                    lat_range_check(in_block, in_block_lat_range);
+                    lat_range_check(in_driver, in_driver_lat_range);
+                    memset(&blk_metrics_lat_write_512k_in_blk, 0, sizeof(blk_metrics_lat_write_512k_in_blk));
+                    memset(&blk_metrics_lat_write_512k_in_drv, 0, sizeof(blk_metrics_lat_write_512k_in_drv));
+                    atomic64_set(&blk_metrics_lat_write_512k_in_blk[in_block_lat_range], 1);
+                    atomic64_set(&blk_metrics_lat_write_512k_in_drv[in_driver_lat_range], 1);
+                }
+            }
+        } else { /* 没有满足一个采样周期时更新数据 */
+            flags = 0;
+            spin_lock_irqsave(&blk_metrics_lock[op_type][i][io_range], flags);
+            blk_metrics[op_type][i][io_range].total_cnt += 1;
+            blk_metrics[op_type][i][io_range].total_size += nr_bytes;
+            blk_metrics[op_type][i][io_range].layer[IN_BLOCK].elapse_time += in_block;
+            blk_metrics[op_type][i][io_range].layer[IN_DRIVER].elapse_time += in_driver;
+
+            /* 最大值 */
+            blk_metrics[op_type][i][io_range].layer[IN_BLOCK].max_time =
+               (blk_metrics[op_type][i][io_range].layer[IN_BLOCK].max_time > in_block) ?
+               blk_metrics[op_type][i][io_range].layer[IN_BLOCK].max_time : in_block;
+            blk_metrics[op_type][i][io_range].layer[IN_DRIVER].max_time =
+               (blk_metrics[op_type][i][io_range].layer[IN_DRIVER].max_time > in_driver) ?
+               blk_metrics[op_type][i][io_range].layer[IN_DRIVER].max_time : in_driver;
+            blk_metrics[op_type][i][io_range].max_time =
+               (blk_metrics[op_type][i][io_range].max_time > in_d_and_b) ?
+               blk_metrics[op_type][i][io_range].max_time : in_d_and_b;
+            spin_unlock_irqrestore(&blk_metrics_lock[op_type][i][io_range], flags);
+            if (op_type == OP_READ) {
+                if (likely(io_range == IO_SIZE_0_TO_4K)) {
+                    lat_range_check(in_block, in_block_lat_range);
+                    lat_range_check(in_driver, in_driver_lat_range);
+                    atomic64_inc(&blk_metrics_lat_read_4k_in_blk[in_block_lat_range]);
+                    atomic64_inc(&blk_metrics_lat_read_4k_in_drv[in_driver_lat_range]);
+                } else if (io_range == IO_SIZE_512K_TO_MAX) {
+                    lat_range_check(in_block, in_block_lat_range);
+                    lat_range_check(in_driver, in_driver_lat_range);
+                    atomic64_inc(&blk_metrics_lat_read_512k_in_blk[in_block_lat_range]);
+                    atomic64_inc(&blk_metrics_lat_read_512k_in_drv[in_driver_lat_range]);
+                }
+            } else if (op_type == OP_WRITE) {
+                if (likely(io_range == IO_SIZE_0_TO_4K)) {
+                    lat_range_check(in_block, in_block_lat_range);
+                    lat_range_check(in_driver, in_driver_lat_range);
+                    atomic64_inc(&blk_metrics_lat_write_4k_in_blk[in_block_lat_range]);
+                    atomic64_inc(&blk_metrics_lat_write_4k_in_drv[in_driver_lat_range]);
+                } else if (io_range == IO_SIZE_512K_TO_MAX) {
+                    lat_range_check(in_block, in_block_lat_range);
+                    lat_range_check(in_driver, in_driver_lat_range);
+                    atomic64_inc(&blk_metrics_lat_write_512k_in_blk[in_block_lat_range]);
+                    atomic64_inc(&blk_metrics_lat_write_512k_in_drv[in_driver_lat_range]);
+                }
+            }
+
+        }
+        if (unlikely(elapse >= sample_cycle_config[i].cycle_value)) {
+            /* 过期复位 */
+            flags = 0;
+            spin_lock_irqsave(&blk_metrics_lock[op_type][i][io_range], flags);
+            blk_metrics[op_type][i][io_range].timestamp = 0;
+            spin_unlock_irqrestore(&blk_metrics_lock[op_type][i][io_range], flags);
+        }
+    }
+}
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(5, 11, 0)
+static char *blk_get_disk_name(struct gendisk *hd, int partno, char *buf)
+{
+    if (!partno)
+        snprintf(buf, BDEVNAME_SIZE, "%s", hd->disk_name);
+    else if (isdigit(hd->disk_name[strlen(hd->disk_name)-1]))
+        snprintf(buf, BDEVNAME_SIZE, "%sp%d", hd->disk_name, partno);
+    else
+        snprintf(buf, BDEVNAME_SIZE, "%s%d", hd->disk_name, partno);
+
+    return buf;
+}
+#endif
+
+static void blk_fill_rwbs_private(char *rwbs, unsigned int op, int bytes)
+{
+    int i = 0;
+    if (op & REQ_PREFLUSH)
+        rwbs[i++] = 'F';
+    switch (op & REQ_OP_MASK) {
+    case REQ_OP_WRITE:
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+    case REQ_OP_WRITE_SAME:
+#endif
+        rwbs[i++] = 'W';
+        break;
+    case REQ_OP_DISCARD:
+        rwbs[i++] = 'D';
+        break;
+    case REQ_OP_SECURE_ERASE:
+        rwbs[i++] = 'D';
+        rwbs[i++] = 'E';
+        break;
+    case REQ_OP_FLUSH:
+        rwbs[i++] = 'F';
+        break;
+    case REQ_OP_READ:
+        rwbs[i++] = 'R';
+        break;
+    default:
+        rwbs[i++] = 'N';
+    }
+    if (op & REQ_FUA)
+        rwbs[i++] = 'F';
+    if (op & REQ_RAHEAD)
+        rwbs[i++] = 'A';
+    if (op & REQ_SYNC)
+        rwbs[i++] = 'S';
+    if (op & REQ_META)
+        rwbs[i++] = 'M';
+    rwbs[i] = '\0';
+}
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(5, 15, 0)
+/**
+There is no consistency between Google and the Linux community here.
+The Linux community only one parameter(request) after 5.10.136, But
+Google Revert the Linux community update to the old version
+on Android 12-5.10-LTS, which means the item on 5.10
+No matter how the small version is upgraded, as long as it is an Android 12-5.10
+project, it will have two parameters(request_queue and request)
+https://android-review.googlesource.com/c/kernel/common/+/2201398
+*/
+static void cb_block_rq_issue(void *ignore, struct request_queue *q,
+                             struct request *rq)
+#else
+static void cb_block_rq_issue(void *ignore, struct request *rq)
+#endif /* LINUX_VERSION_CODE <= KERNEL_VERSION(5, 10, 136) */
+{
+    if (unlikely(!io_metrics_enabled)) {
+        return ;
+    }
+    rq->io_start_time_ns = ktime_get_ns();
+    if (unlikely(io_metrics_debug_enabled || block_rq_issue_enabled)) {
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+        char *devname = rq->rq_disk ? rq->rq_disk->disk_name : "";
+#else
+        char *devname = rq->part ? (rq->part->bd_disk ? rq->part->bd_disk->disk_name : "") : "";
+#endif
+        char rwbs[RWBS_LEN]={};
+        unsigned int nr_bytes = blk_rq_bytes(rq);
+
+        blk_fill_rwbs_private(rwbs, rq->cmd_flags, nr_bytes);
+
+        io_metrics_print("dev:%-6s rwbs:%-4s nr_bytes:%-10d " \
+          "start_time_ns:%-16llu io_start_time_ns:%-16llu \n",
+          devname, rwbs, nr_bytes, rq->start_time_ns, rq->io_start_time_ns);
+    }
+}
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+static void cb_block_rq_complete(void *ignore, struct request *rq,
+                      int error, unsigned int nr_bytes)
+#else
+static void cb_block_rq_complete(void *ignore, struct request *rq,
+                      blk_status_t error, unsigned int nr_bytes)
+#endif
+{
+    u64 io_complete_time_ns = ktime_get_ns();
+    u64 in_driver = (io_complete_time_ns > rq->io_start_time_ns) && rq->io_start_time_ns ?
+                    (io_complete_time_ns - rq->io_start_time_ns) : 0;
+    u64 in_block = (rq->io_start_time_ns > rq->start_time_ns) && rq->start_time_ns ?
+                    (rq->io_start_time_ns - rq->start_time_ns) : 0;
+
+    if (unlikely(!io_metrics_enabled)) {
+        return ;
+    }
+
+    switch (rq->cmd_flags & REQ_OP_MASK) {
+        case REQ_OP_WRITE:
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+        case REQ_OP_WRITE_SAME:
+#endif
+            if (!error && nr_bytes) {
+#if 1
+                block_stat_update(rq, OP_WRITE, io_complete_time_ns);
+#endif
+#if 0
+                if (rq->cmd_flags & REQ_SYNC) {
+                    block_stat_update(rq, OP_WRITE_SYNC, io_complete_time_ns);
+                } else if (rq->cmd_flags & REQ_META) {
+                    block_stat_update(rq, OP_WRITE_META, io_complete_time_ns);
+                } else {
+                    block_stat_update(rq, OP_WRITE, io_complete_time_ns);
+                }
+#endif
+            }
+            break;
+#if 0
+        case REQ_OP_DISCARD:
+            if (!error && nr_bytes) {
+                block_stat_update(rq, OP_DISCARD, io_complete_time_ns);
+            }
+            break;
+        case REQ_OP_SECURE_ERASE:
+            if (!error && nr_bytes) {
+                block_stat_update(rq, OP_SECURE_ERASE, io_complete_time_ns);
+            }
+            break;
+        case REQ_OP_FLUSH:
+            if (!error && nr_bytes) {
+                block_stat_update(rq, OP_FLUSH, io_complete_time_ns);
+            }
+            break;
+#endif
+        case REQ_OP_READ:
+            if (!error && nr_bytes) {
+#if 1
+                block_stat_update(rq, OP_READ, io_complete_time_ns);
+#endif
+#if 0
+                if (rq->cmd_flags & REQ_RAHEAD) {
+                    block_stat_update(rq, OP_RAHEAD, io_complete_time_ns);
+                } else if (rq->cmd_flags & REQ_META) {
+                    block_stat_update(rq, OP_READ_META, io_complete_time_ns);
+                }else {
+                    block_stat_update(rq, OP_READ, io_complete_time_ns);
+                }
+#endif
+            }
+            break;
+        default:
+            break;
+    }
+
+    if (unlikely(io_metrics_debug_enabled || block_rq_complete_enabled)) {
+        char devname[BDEVNAME_SIZE] = {0};
+        char rwbs[RWBS_LEN]={0};
+
+        blk_fill_rwbs_private(rwbs, rq->cmd_flags, nr_bytes);
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(5, 11, 0)
+        if (rq->bio && rq->bio->bi_disk) {
+            blk_get_disk_name(rq->bio->bi_disk, rq->bio->bi_partno, devname);
+#else
+        if (rq->bio && rq->bio->bi_bdev) {
+        //todo 通过block_device获取设备名称
+        //disk_name(rq->bio->bi_disk, rq->bio->bi_partno, devname);
+        devname[0] = '\0';
+#endif
+        } else {
+            devname[0] = '\0';
+        }
+
+        io_metrics_print("dev:%-6s rwbs:%-4s nr_bytes:%-10d error:%-3d " \
+            "start_time_ns:%-16llu io_start_time_ns:%-16llu io_complete_time_ns:%-16llu " \
+            "in_driver:%-10llu in_block:%-10llu\n", devname, rwbs, nr_bytes,
+            error, rq->start_time_ns, rq->io_start_time_ns, io_complete_time_ns,
+            in_driver, in_block);
+    }
+    return;
+}
+
+struct {
+    const char *name;
+    void *callback;
+    struct tracepoint *tp;
+    void *data;
+} tracepoint_probes[] = {
+    {"block_rq_issue", cb_block_rq_issue, NULL, NULL},
+    {"block_rq_complete", cb_block_rq_complete, NULL, NULL},
+    {NULL, NULL, NULL, NULL}
+};
+
+void block_register_tracepoint_probes(void)
+{
+    int ret;
+
+    ret = register_trace_block_rq_issue(cb_block_rq_issue, NULL);
+    ret = register_trace_block_rq_complete(cb_block_rq_complete, NULL);
+
+    return;
+}
+
+void block_unregister_tracepoint_probes(void)
+{
+    unregister_trace_block_rq_issue(cb_block_rq_issue, NULL);
+    unregister_trace_block_rq_complete(cb_block_rq_complete, NULL);
+
+    return;
+}
+
+struct {
+    enum io_op_type value;
+    const char * tag;
+} io_op_config[] = {
+    {OP_READ,         "read"      },
+    {OP_WRITE,        "write"     },
+#if 0
+    {OP_RAHEAD,       "rahead"    },
+    {OP_WRITE_SYNC,   "write_sync"},
+    {OP_READ_META,    "read_meta" },
+    {OP_WRITE_META,   "write_meta"},
+    {OP_DISCARD,      "discard"   },
+    {OP_SECURE_ERASE, "erase"     },
+    {OP_FLUSH,        "flush"     },
+#endif
+    {OP_MAX,          NULL        },
+};
+
+/*当前函数理论每个node一天只需要访问一次，因此可以不用太考虑性能，只关注代码紧凑性*/
+static int block_metrics_proc_show(struct seq_file *seq_filp, void *data)
+{
+    int i = 0;
+    enum io_op_type io_op;
+    u64 value = 0;
+    enum sample_cycle_type cycle;
+    struct file *file = (struct file *)seq_filp->private;
+
+    if (unlikely(!io_metrics_enabled)) {
+        seq_printf(seq_filp, "io_metrics_enabled not set to 1:%d\n", io_metrics_enabled);
+        return 0;
+    }
+    if (proc_show_enabled ||unlikely(io_metrics_debug_enabled)) {
+        io_metrics_print("%s(%d) read %s/%s\n",
+            current->comm, current->pid, file->f_path.dentry->d_parent->d_iname,
+            file->f_path.dentry->d_iname);
+    }
+    /* 确定采样周期的值（父目录） */
+    cycle = CYCLE_MAX;
+    for (i = 0; i < CYCLE_MAX; i++) {
+        if(!strcmp(file->f_path.dentry->d_parent->d_iname, sample_cycle_config[i].tag)) {
+            cycle = sample_cycle_config[i].value;
+        }
+    }
+    if (unlikely(cycle == CYCLE_MAX)) {
+        goto err;
+    }
+
+    /* 确定读、写操作命令 */
+    io_op = OP_MAX;
+    for (i = 0; i < OP_MAX; i++) {
+        if (strstr(file->f_path.dentry->d_iname, io_op_config[i].tag)) {
+            io_op = io_op_config[i].value;
+            break;
+        }
+    }
+    if (unlikely(io_op == OP_MAX)) {
+        goto err;
+    }
+    if (OP_MAX == OP_READ) {
+        goto bio_read;
+    } else if (OP_MAX == OP_WRITE) {
+         goto bio_write;
+    }
+
+    /* 确定读的具体是那个节点 */
+bio_read:
+    if (!strcmp(file->f_path.dentry->d_iname, "bio_read_cnt")) {
+        value = 0;
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            value += blk_metrics[OP_READ][cycle][i].total_cnt;
+        }
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_avg_size")) {
+        unsigned long flags = 0;
+        u64 total_size = 0;
+        u64 total_cnt = 0;
+        value = 0;
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            flags = 0;
+            spin_lock_irqsave(&blk_metrics_lock[OP_READ][cycle][i], flags);
+            total_size += blk_metrics[OP_READ][cycle][i].total_size;
+            total_cnt += blk_metrics[OP_READ][cycle][i].total_cnt;
+            spin_unlock_irqrestore(&blk_metrics_lock[OP_READ][cycle][i], flags);
+        }
+        value = total_size / total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_size_dist")) {
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", blk_metrics[OP_READ][cycle][i].total_cnt);
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_avg_time")) {
+        u64 total_time = 0;
+        u64 total_cnt = 0;
+        value = 0;
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            total_time += blk_metrics[OP_READ][cycle][i].layer[IN_BLOCK].elapse_time;
+            total_time += blk_metrics[OP_READ][cycle][i].layer[IN_DRIVER].elapse_time;
+            total_cnt += blk_metrics[OP_READ][cycle][i].total_cnt;
+        }
+        value = total_time / total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_max_time")) {
+        value = 0;
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            value = (value > blk_metrics[OP_READ][cycle][i].max_time) ?
+                      value : blk_metrics[OP_READ][cycle][i].max_time;
+        }
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_4k_blk_avg_time")) {
+        value = blk_metrics[OP_READ][cycle][IO_SIZE_0_TO_4K].layer[IN_BLOCK].elapse_time /
+                blk_metrics[OP_READ][cycle][IO_SIZE_0_TO_4K].total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_4k_blk_max_time")) {
+        value = blk_metrics[OP_READ][cycle][IO_SIZE_0_TO_4K].layer[IN_BLOCK].max_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_4k_blk_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&blk_metrics_lat_read_4k_in_blk[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_4k_drv_avg_time")) {
+        value = blk_metrics[OP_READ][cycle][IO_SIZE_0_TO_4K].layer[IN_DRIVER].elapse_time /
+                blk_metrics[OP_READ][cycle][IO_SIZE_0_TO_4K].total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_4k_drv_max_time")) {
+        value = blk_metrics[OP_READ][cycle][IO_SIZE_0_TO_4K].layer[IN_DRIVER].max_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_4k_drv_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&blk_metrics_lat_read_4k_in_drv[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_512k_blk_avg_time")) {
+        value = blk_metrics[OP_READ][cycle][IO_SIZE_512K_TO_MAX].layer[IN_BLOCK].elapse_time /
+                blk_metrics[OP_READ][cycle][IO_SIZE_512K_TO_MAX].total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_512k_blk_max_time")) {
+        value = blk_metrics[OP_READ][cycle][IO_SIZE_512K_TO_MAX].layer[IN_BLOCK].max_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_512k_blk_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&blk_metrics_lat_read_512k_in_blk[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_512k_drv_avg_time")) {
+        value = blk_metrics[OP_READ][cycle][IO_SIZE_512K_TO_MAX].layer[IN_DRIVER].elapse_time /
+                blk_metrics[OP_READ][cycle][IO_SIZE_512K_TO_MAX].total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_512k_drv_max_time")) {
+        value = blk_metrics[OP_READ][cycle][IO_SIZE_512K_TO_MAX].layer[IN_DRIVER].max_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_read_512k_drv_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&blk_metrics_lat_read_512k_in_drv[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    }
+
+bio_write:
+    if (!strcmp(file->f_path.dentry->d_iname, "bio_write_cnt")) {
+        value = 0;
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            value += blk_metrics[OP_WRITE][cycle][i].total_cnt;
+        }
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_avg_size")) {
+        unsigned long flags = 0;
+        u64 total_size = 0;
+        u64 total_cnt = 0;
+        value = 0;
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            flags = 0;
+            spin_lock_irqsave(&blk_metrics_lock[OP_WRITE][cycle][i], flags);
+            total_size += blk_metrics[OP_WRITE][cycle][i].total_size;
+            total_cnt += blk_metrics[OP_WRITE][cycle][i].total_cnt;
+            spin_unlock_irqrestore(&blk_metrics_lock[OP_WRITE][cycle][i], flags);
+        }
+        value = total_size / total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_size_dist")) {
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", blk_metrics[OP_WRITE][cycle][i].total_cnt);
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_avg_time")) {
+        u64 total_time = 0;
+        u64 total_cnt = 0;
+        value = 0;
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            total_time += blk_metrics[OP_WRITE][cycle][i].layer[IN_BLOCK].elapse_time;
+            total_time += blk_metrics[OP_WRITE][cycle][i].layer[IN_DRIVER].elapse_time;
+            total_cnt += blk_metrics[OP_WRITE][cycle][i].total_cnt;
+        }
+        value = total_time / total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_max_time")) {
+        value = 0;
+        for (i = 0; i < IO_SIZE_MAX; i++) {
+            value = (value > blk_metrics[OP_WRITE][cycle][i].max_time) ?
+                      value : blk_metrics[OP_WRITE][cycle][i].max_time;
+        }
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_4k_blk_avg_time")) {
+        value = blk_metrics[OP_WRITE][cycle][IO_SIZE_0_TO_4K].layer[IN_BLOCK].elapse_time /
+                blk_metrics[OP_WRITE][cycle][IO_SIZE_0_TO_4K].total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_4k_blk_max_time")) {
+        value = blk_metrics[OP_WRITE][cycle][IO_SIZE_0_TO_4K].layer[IN_BLOCK].max_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_4k_blk_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&blk_metrics_lat_write_4k_in_blk[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_4k_drv_avg_time")) {
+        value = blk_metrics[OP_WRITE][cycle][IO_SIZE_0_TO_4K].layer[IN_DRIVER].elapse_time /
+                blk_metrics[OP_WRITE][cycle][IO_SIZE_0_TO_4K].total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_4k_drv_max_time")) {
+        value = blk_metrics[OP_WRITE][cycle][IO_SIZE_0_TO_4K].layer[IN_DRIVER].max_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_4k_drv_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&blk_metrics_lat_write_4k_in_drv[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_512k_blk_avg_time")) {
+        value = blk_metrics[OP_WRITE][cycle][IO_SIZE_512K_TO_MAX].layer[IN_BLOCK].elapse_time /
+                blk_metrics[OP_WRITE][cycle][IO_SIZE_512K_TO_MAX].total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_512k_blk_max_time")) {
+        value = blk_metrics[OP_WRITE][cycle][IO_SIZE_512K_TO_MAX].layer[IN_BLOCK].max_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_512k_blk_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&blk_metrics_lat_write_512k_in_blk[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_512k_drv_avg_time")) {
+        value = blk_metrics[OP_WRITE][cycle][IO_SIZE_512K_TO_MAX].layer[IN_DRIVER].elapse_time /
+                blk_metrics[OP_WRITE][cycle][IO_SIZE_512K_TO_MAX].total_cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_512k_drv_max_time")) {
+        value = blk_metrics[OP_WRITE][cycle][IO_SIZE_512K_TO_MAX].layer[IN_DRIVER].max_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "bio_write_512k_drv_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&blk_metrics_lat_write_512k_in_drv[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    }
+
+    seq_printf(seq_filp, "%llu\n", value);
+
+    return 0;
+
+err:
+    io_metrics_print("%s(%d) I don't understand what the operation: %s/%s\n",
+    current->comm,current->pid,
+    file->f_path.dentry->d_parent->d_iname, file->f_path.dentry->d_iname);
+    return -1;
+}
+
+int block_metrics_proc_open(struct inode *inode, struct file *file)
+{
+    return single_open(file, block_metrics_proc_show, file);
+}
+
+void block_metrics_reset(void)
+{
+    /* 此处可以优化，如果blk_metrics_struct中有锁成员时 */
+    memset(blk_metrics, 0, OP_MAX * CYCLE_MAX * IO_SIZE_MAX
+                         * sizeof(struct blk_metrics_struct));
+    io_metrics_print("size:%lu\n", OP_MAX * CYCLE_MAX * IO_SIZE_MAX
+                              * sizeof(struct blk_metrics_struct));
+    memset(&blk_metrics_lat_read_4k_in_blk, 0, sizeof(blk_metrics_lat_read_4k_in_blk));
+    memset(&blk_metrics_lat_read_4k_in_drv, 0, sizeof(blk_metrics_lat_read_4k_in_drv));
+    memset(&blk_metrics_lat_write_4k_in_blk, 0, sizeof(blk_metrics_lat_write_4k_in_blk));
+    memset(&blk_metrics_lat_write_4k_in_drv, 0, sizeof(blk_metrics_lat_write_4k_in_drv));
+    memset(&blk_metrics_lat_read_512k_in_blk, 0, sizeof(blk_metrics_lat_read_512k_in_blk));
+    memset(&blk_metrics_lat_read_512k_in_drv, 0, sizeof(blk_metrics_lat_read_512k_in_drv));
+    memset(&blk_metrics_lat_write_512k_in_blk, 0, sizeof(blk_metrics_lat_write_512k_in_blk));
+    memset(&blk_metrics_lat_write_512k_in_drv, 0, sizeof(blk_metrics_lat_write_512k_in_drv));
+}
+
+void block_metrics_init(void)
+{
+    int i, j, k;
+
+    block_metrics_reset();
+    for (i = 0; i < OP_MAX; i++) {
+        for (j = 0; j < CYCLE_MAX; j++) {
+            for (k = 0; k < IO_SIZE_MAX; k++) {
+                spin_lock_init(&blk_metrics_lock[i][j][k]);
+            }
+        }
+    }
+}
diff --git a/drivers/soc/oplus/storage/common/io_metrics/block_metrics.h b/drivers/soc/oplus/storage/common/io_metrics/block_metrics.h
new file mode 100644
index 000000000..7569cf2e6
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/block_metrics.h
@@ -0,0 +1,73 @@
+#ifndef __BLOCK_METRICS_H__
+#define __BLOCK_METRICS_H__
+
+#include <linux/fs.h>
+
+#define IO_SIZE_4K_TO_32K_MASK       4096
+#define IO_SIZE_32K_TO_128K_MASK     32768
+#define IO_SIZE_128K_TO_512K_MASK    131072
+#define IO_SIZE_512K_TO_MAX_MASK     524288
+
+/* IO大小分布 */
+enum io_range {
+    IO_SIZE_0_TO_4K = 0,  /* (0, 4K]      */
+    IO_SIZE_4K_TO_32K,    /* (4K, 32K]    */
+    IO_SIZE_32K_TO_128K,  /* (32K, 128K]  */
+    IO_SIZE_128K_TO_512K, /* (128K, 512K) */
+    IO_SIZE_512K_TO_MAX, /*  [512K, +∞)   */
+    IO_SIZE_MAX /* 记录总的流量,不分大小 */
+};
+
+/* IO操作类型 */
+enum io_op_type {
+    OP_READ = 0,
+    OP_WRITE,
+#if 0
+    OP_RAHEAD,
+    OP_WRITE_SYNC,
+    OP_READ_META,
+    OP_WRITE_META,
+    OP_DISCARD,
+    OP_SECURE_ERASE,
+    OP_FLUSH,
+#endif
+    OP_MAX
+};
+
+/* 不同层IO的耗时 */
+enum layer_type {
+    IN_DRIVER = 0,/* IO在driver层的耗时 */
+    IN_BLOCK,     /* IO在block层的耗时  */
+    LAYER_MAX
+};
+
+//Ensure cache line alignment
+struct blk_metrics_struct {
+    /* 开始统计的时间戳 */
+    u64 timestamp;
+    /* IO计数 */
+    u64 total_cnt;
+    /* IO总的大小 */
+    u64 total_size;
+    /* block+driver的最大耗时 */
+    u64 max_time;
+    struct {
+        /* 累计耗时 */
+        u64 elapse_time;
+        /* 最大耗时 */
+        u64 max_time;
+    } layer[LAYER_MAX];//对block、driver层分别统计
+};
+
+extern bool block_rq_issue_enabled;
+extern bool block_rq_complete_enabled;
+extern struct blk_metrics_struct blk_metrics[OP_MAX][CYCLE_MAX][IO_SIZE_MAX];
+extern spinlock_t blk_metrics_lock[OP_MAX][CYCLE_MAX][IO_SIZE_MAX];
+
+void block_register_tracepoint_probes(void);
+void block_unregister_tracepoint_probes(void);
+int block_metrics_proc_open(struct inode *inode, struct file *file);
+void block_metrics_reset(void);
+void block_metrics_init(void);
+
+#endif /* __BLOCK_METRICS_H__ */
\ No newline at end of file
diff --git a/drivers/soc/oplus/storage/common/io_metrics/f2fs_metrics.c b/drivers/soc/oplus/storage/common/io_metrics/f2fs_metrics.c
new file mode 100644
index 000000000..027eb8a7c
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/f2fs_metrics.c
@@ -0,0 +1,431 @@
+#include "io_metrics_entry.h"
+#include "f2fs_metrics.h"
+#include "procfs.h"
+#include "fs/f2fs/f2fs.h"
+#include "fs/f2fs/segment.h"
+#include "fs/f2fs/node.h"
+#include <trace/events/f2fs.h>
+
+bool f2fs_issue_discard_enabled = false;
+bool f2fs_gc_begin_enabled = false;
+bool f2fs_gc_end_enabled = false;
+bool f2fs_write_checkpoint_enabled = false;
+bool f2fs_sync_file_enter_enabled = false;
+bool f2fs_sync_file_exit_enabled = false;
+bool f2fs_dataread_start_enabled = false;
+bool f2fs_dataread_end_enabled = false;
+bool f2fs_datawrite_start_enabled = false;
+bool f2fs_datawrite_end_enabled = false;
+
+module_param(f2fs_issue_discard_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_issue_discard_enabled, " Debug f2fs_issue_discard");
+module_param(f2fs_gc_begin_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_gc_begin_enabled, " Debug f2fs_gc_begin");
+module_param(f2fs_gc_end_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_gc_end_enabled, " Debug f2fs_gc_end");
+module_param(f2fs_write_checkpoint_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_write_checkpoint_enabled, " Debug f2fs_write_checkpoint");
+module_param(f2fs_sync_file_enter_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_sync_file_enter_enabled, " Debug f2fs_sync_file_enter");
+module_param(f2fs_sync_file_exit_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_sync_file_exit_enabled, " Debug f2fs_sync_file_exit");
+module_param(f2fs_dataread_start_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_dataread_start_enabled, " Debug f2fs_dataread_start");
+module_param(f2fs_dataread_end_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_dataread_end_enabled, " Debug f2fs_dataread_end");
+module_param(f2fs_datawrite_start_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_datawrite_start_enabled, " Debug f2fs_datawrite_start");
+module_param(f2fs_datawrite_end_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(f2fs_datawrite_end_enabled, " Debug f2fs_datawrite_end");
+
+enum {
+    GC_BG = 0,  //后台GC
+    GC_FG,      //前台GC
+    GC_MAX
+};
+atomic64_t f2fs_metrics_timestamp[CYCLE_MAX];
+/* gc自己有锁保护，没有竞争，因此无需自定义锁 */
+struct {
+    /* 累计耗时 */
+    u64 elapse_time;
+    /* 最近一次gc开始时间 */
+    u64 begin_time;
+    /* gc总次数 */
+    u64 cnt;
+    /* 平均一次gc的耗时 */
+    u64 avg_time;
+    /* 回收的总的segment数 */
+    u64 segs;
+    /* 平均一次gc回收的segment数 */
+    u64 avg_segs;
+    /* 每次GC中有效block占比的平均值 */
+    u64 efficiency;
+    char padding[8];
+} f2fs_gc_metrics[CYCLE_MAX][GC_MAX];//对不同层统计
+
+/* cp自己有gc锁保护，没有竞争，因此无需自定义锁 */
+struct {
+    /* cp总次数 */
+    u64 cnt;
+    /* 累计耗时 */
+    u64 elapse_time;
+    /* 最近一次gc开始时间 */
+    u64 begin_time;
+    /* 平均耗时 */
+    u64 avg_time;
+    /* 最大耗时 */
+    u64 max_time;
+    /* 本地更新次数 */
+    u32 inplace_count;
+    char padding[20];
+} f2fs_cp_metrics[CYCLE_MAX] = {0};
+
+struct {
+    /* discard次数 */
+    u64 discard_cnt;
+    u64 discard_len;
+    u64 ipu_cnt;
+    u64 fsync_cnt;
+    char padding[32];
+} f2fs_metrics[CYCLE_MAX] = {0};
+
+static void cb_f2fs_issue_discard(void *ignore, struct block_device *dev,
+                                          block_t blkstart, block_t blklen)
+{
+    int i;
+    u64 current_time_ns, elapse;
+
+    if (unlikely(!io_metrics_enabled)) {
+        return;
+    }
+    current_time_ns = ktime_get_ns();
+
+    for (i = 0; i < CYCLE_MAX; i++) {
+        elapse = current_time_ns - atomic64_read(&f2fs_metrics_timestamp[i]);
+        /* 统计复位(timestamp为0)、统计异常（timestamp比current_time_ns大） */
+        if (unlikely(elapse >= current_time_ns)) {
+            atomic64_set(&f2fs_metrics_timestamp[i], current_time_ns);
+            f2fs_metrics[i].discard_cnt = 1;
+            f2fs_metrics[i].discard_len = blklen;
+            elapse = 0;
+        } else {
+            f2fs_metrics[i].discard_cnt += 1;
+            f2fs_metrics[i].discard_len += blklen;
+        }
+        if (unlikely(elapse >= sample_cycle_config[i].cycle_value)) {
+            atomic64_set(&f2fs_metrics_timestamp[i], 0);
+        }
+    }
+    if (unlikely(io_metrics_debug_enabled || f2fs_issue_discard_enabled)) {
+        io_metrics_print("current_time_ns:%llu\n", current_time_ns);
+    }
+};
+int gc_t;
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+static void cb_f2fs_gc_begin(void *ignore, struct super_block *sb, bool sync,
+            bool background, long long dirty_nodes, long long dirty_dents,
+            long long dirty_imeta, unsigned int free_sec,
+            unsigned int free_seg, int reserved_seg,
+            unsigned int prefree_seg)
+#else
+static void cb_f2fs_gc_begin(void *ignore, struct super_block *sb, int gc_type, bool no_bg_gc,
+            unsigned int nr_free_secs,
+            long long dirty_nodes, long long dirty_dents,
+            long long dirty_imeta, unsigned int free_sec,
+            unsigned int free_seg, int reserved_seg,
+            unsigned int prefree_seg)
+#endif
+{
+    int i;
+    u64 current_time_ns, elapse;
+
+    if (unlikely(!io_metrics_enabled)) {
+        return;
+    }
+    current_time_ns = ktime_get_ns();
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+    gc_t = background ? GC_BG : GC_FG;
+#else
+    gc_t = no_bg_gc ? GC_FG : GC_BG;
+#endif
+    for (i = 0; i < CYCLE_MAX; i++) {
+        elapse = current_time_ns - atomic64_read(&f2fs_metrics_timestamp[i]);
+        if (unlikely(elapse >= current_time_ns)) {
+            atomic64_set(&f2fs_metrics_timestamp[i], current_time_ns);
+            f2fs_gc_metrics[i][gc_t].elapse_time = 0;
+            f2fs_gc_metrics[i][gc_t].begin_time = current_time_ns;
+            f2fs_gc_metrics[i][gc_t].cnt = 0;
+            f2fs_gc_metrics[i][gc_t].avg_time = 0;
+            f2fs_gc_metrics[i][gc_t].segs = 0;
+            f2fs_gc_metrics[i][gc_t].avg_segs = 0;
+            f2fs_gc_metrics[i][gc_t].efficiency = 0;
+            elapse = 0;
+        } else {
+            f2fs_gc_metrics[i][gc_t].begin_time = current_time_ns;
+        }
+        if (unlikely(elapse >= sample_cycle_config[i].cycle_value)) {
+            atomic64_set(&f2fs_metrics_timestamp[i], 0);
+        }
+    }
+    if (unlikely(io_metrics_debug_enabled || f2fs_gc_begin_enabled)) {
+        io_metrics_print("current_time_ns:%llu\n", current_time_ns);
+    }
+};
+
+static void cb_f2fs_gc_end(void *ignore, struct super_block *sb, int ret,
+            int seg_freed, int sec_freed, long long dirty_nodes,
+            long long dirty_dents, long long dirty_imeta,
+            unsigned int free_sec, unsigned int free_seg,
+            int reserved_seg, unsigned int prefree_seg)
+{
+    int i;
+    u64 current_time_ns, gc_elapse = 0;
+
+    if (unlikely(!io_metrics_enabled)) {
+        return;
+    }
+    if (unlikely(!(sb->s_flags & SB_ACTIVE))) {
+        return;
+    }
+    if (gc_t > GC_FG) {
+        if (unlikely(io_metrics_debug_enabled)) {
+            io_metrics_print("gc_t(%d) is not a expected value\n", gc_t);
+        }
+        return;
+    }
+    current_time_ns = ktime_get_ns();
+    for (i = 0; i < CYCLE_MAX; i++) {
+        /* 考虑到gc开始会后有外界复位操作，导致数据错乱 */
+        if (likely(f2fs_gc_metrics[i][gc_t].begin_time)) {
+            gc_elapse = current_time_ns - f2fs_gc_metrics[i][gc_t].begin_time;
+            f2fs_gc_metrics[i][gc_t].elapse_time += gc_elapse;
+            f2fs_gc_metrics[i][gc_t].cnt += 1;
+            f2fs_gc_metrics[i][gc_t].segs += free_seg;/* todo */
+            f2fs_gc_metrics[i][gc_t].avg_time = f2fs_gc_metrics[i][gc_t].elapse_time /
+                                               f2fs_gc_metrics[i][gc_t].cnt;
+            /* todo */
+            f2fs_gc_metrics[i][gc_t].avg_segs = f2fs_gc_metrics[i][gc_t].segs /
+                                               f2fs_gc_metrics[i][gc_t].cnt;
+            f2fs_gc_metrics[i][gc_t].begin_time = 0;
+        }
+    }
+    if (unlikely(io_metrics_debug_enabled || f2fs_gc_end_enabled)) {
+        const char *gc_type[] = {"Background", "Foreground"};
+        io_metrics_print("%s gc elapse:%llu  count:%llu\n", gc_type[gc_t], gc_elapse,
+                                                f2fs_gc_metrics[CYCLE_MAX-1][gc_t].cnt);
+    }
+};
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(5, 15, 0)
+static void cb_f2fs_write_checkpoint(void *ignore, struct super_block *sb,
+                                                    int reason, char *msg)
+#else
+static void cb_f2fs_write_checkpoint(void *ignore, struct super_block *sb,
+                                               int reason, const char *msg)
+#endif /* LINUX_VERSION_CODE <= KERNEL_VERSION(5, 15, 0) */
+{
+    int i;
+    u64 current_time_ns, elapse, cp_elapse = 0;
+#ifdef CONFIG_F2FS_STAT_FS
+    struct f2fs_sb_info *sbi = F2FS_SB(sb);
+#endif
+
+    if (unlikely(!io_metrics_enabled)) {
+        return;
+    }
+    current_time_ns = ktime_get_ns();
+    if (!strcmp(msg, "start block_ops")) {
+        for (i = 0; i < CYCLE_MAX; i++) {
+            elapse = current_time_ns - atomic64_read(&f2fs_metrics_timestamp[i]);
+            if (unlikely(elapse >= current_time_ns)) {
+                atomic64_set(&f2fs_metrics_timestamp[i], current_time_ns);
+                f2fs_cp_metrics[i].begin_time = current_time_ns;
+                f2fs_cp_metrics[i].cnt = 0;
+                f2fs_cp_metrics[i].elapse_time = 0;
+                f2fs_cp_metrics[i].avg_time = 0;
+                f2fs_cp_metrics[i].max_time = 0;
+                elapse = 0;
+            } else {
+                f2fs_cp_metrics[i].begin_time = current_time_ns;
+            }
+#ifdef CONFIG_F2FS_STAT_FS
+            f2fs_cp_metrics[i].inplace_count = atomic_read(&sbi->inplace_count);
+#endif
+            if (unlikely(elapse >= sample_cycle_config[i].cycle_value)) {
+                atomic64_set(&f2fs_metrics_timestamp[i], 0);
+            }
+        }
+        if (unlikely(io_metrics_debug_enabled || f2fs_write_checkpoint_enabled)) {
+            io_metrics_print("current_time_ns:%llu\n", current_time_ns);
+        }
+    } else if (!strcmp(msg, "finish checkpoint")) {
+        for (i = 0; i < CYCLE_MAX; i++) {
+            /* 考虑到cp开始会后有外界复位操作，导致数据错乱 */
+            if (likely(f2fs_cp_metrics[i].begin_time)) {
+                cp_elapse = current_time_ns - f2fs_cp_metrics[i].begin_time;
+                f2fs_cp_metrics[i].elapse_time += cp_elapse;
+                f2fs_cp_metrics[i].cnt += 1;
+                f2fs_cp_metrics[i].avg_time = f2fs_cp_metrics[i].elapse_time /
+                                              f2fs_cp_metrics[i].cnt;
+                f2fs_cp_metrics[i].max_time = f2fs_cp_metrics[i].max_time >= cp_elapse ?
+                                              f2fs_cp_metrics[i].max_time : cp_elapse;
+                f2fs_cp_metrics[i].begin_time = 0;
+            }
+        }
+        if (unlikely(io_metrics_debug_enabled || f2fs_write_checkpoint_enabled)) {
+            io_metrics_print("checkpoint elapse:%llu  count:%llu\n", cp_elapse,
+                                            f2fs_cp_metrics[CYCLE_MAX-1].cnt);
+        }
+    }
+};
+
+static void cb_f2fs_sync_file_enter(void *ignore, struct inode *inode)
+{
+    int i;
+    u64 current_time_ns, elapse;
+
+    if (unlikely(!io_metrics_enabled)) {
+        return;
+    }
+    current_time_ns = ktime_get_ns();
+    for (i = 0; i < CYCLE_MAX; i++) {
+        elapse = current_time_ns - atomic64_read(&f2fs_metrics_timestamp[i]);
+        if (unlikely(elapse >= current_time_ns)) {
+            atomic64_set(&f2fs_metrics_timestamp[i], current_time_ns);
+            f2fs_metrics[i].fsync_cnt = 1;
+            elapse = 0;
+        } else {
+            f2fs_metrics[i].fsync_cnt += 1;
+        }
+        if (unlikely(elapse >= sample_cycle_config[i].cycle_value)) {
+            atomic64_set(&f2fs_metrics_timestamp[i], 0);
+        }
+    }
+    if (unlikely(io_metrics_debug_enabled || f2fs_sync_file_enter_enabled)) {
+        io_metrics_print("current_time_ns:%llu count:%llu\n", current_time_ns,
+                                       f2fs_metrics[CYCLE_MAX-1].fsync_cnt);
+    }
+};
+
+static void cb_f2fs_sync_file_exit(void *ignore, struct inode *inode,
+                                   int cp_reason, int datasync, int ret)
+{
+    if (unlikely(!io_metrics_enabled)) {
+        return;
+    }
+    if (unlikely(io_metrics_debug_enabled || f2fs_sync_file_exit_enabled)) {
+        io_metrics_print("here\n");
+    }
+};
+
+void f2fs_register_tracepoint_probes(void)
+{
+    int ret;
+    ret = register_trace_f2fs_issue_discard(cb_f2fs_issue_discard, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_gc_begin(cb_f2fs_gc_begin, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_gc_end(cb_f2fs_gc_end, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_write_checkpoint(cb_f2fs_write_checkpoint, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_sync_file_enter(cb_f2fs_sync_file_enter, NULL);
+    WARN_ON(ret);
+    ret = register_trace_f2fs_sync_file_exit(cb_f2fs_sync_file_exit, NULL);
+    WARN_ON(ret);
+}
+
+void f2fs_unregister_tracepoint_probes(void)
+{
+    unregister_trace_f2fs_issue_discard(cb_f2fs_issue_discard, NULL);
+    unregister_trace_f2fs_gc_begin(cb_f2fs_gc_begin, NULL);
+    unregister_trace_f2fs_gc_end(cb_f2fs_gc_end, NULL);
+    unregister_trace_f2fs_write_checkpoint(cb_f2fs_write_checkpoint, NULL);
+    unregister_trace_f2fs_sync_file_enter(cb_f2fs_sync_file_enter, NULL);
+    unregister_trace_f2fs_sync_file_exit(cb_f2fs_sync_file_exit, NULL);
+}
+
+static int f2fs_metrics_proc_show(struct seq_file *seq_filp, void *data)
+{
+    int i = 0;
+    u64 value = 123;
+    struct file *file = (struct file *)seq_filp->private;
+    enum sample_cycle_type cycle;
+
+    if (unlikely(!io_metrics_enabled)) {
+        seq_printf(seq_filp, "io_metrics_enabled not set to 1:%d\n", io_metrics_enabled);
+        return 0;
+    }
+
+    if (proc_show_enabled || unlikely(io_metrics_debug_enabled)) {
+        io_metrics_print("%s(%d) read %s/%s\n",
+            current->comm, current->pid, file->f_path.dentry->d_parent->d_iname,
+            file->f_path.dentry->d_iname);
+    }
+    /* 确定采样周期的值（父目录） */
+    cycle = CYCLE_MAX;
+    for (i = 0; i < CYCLE_MAX; i++) {
+        if(!strcmp(file->f_path.dentry->d_parent->d_iname, sample_cycle_config[i].tag)) {
+            cycle = sample_cycle_config[i].value;
+        }
+    }
+    if (unlikely(cycle == CYCLE_MAX)) {
+        goto err;
+    }
+    if(!strcmp(file->f_path.dentry->d_iname, "f2fs_discard_cnt")) {
+        value = f2fs_metrics[cycle].discard_cnt;
+    } else if(!strcmp(file->f_path.dentry->d_iname, "f2fs_discard_len")) {
+        value = f2fs_metrics[cycle].discard_len;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_fg_gc_cnt")) {
+        value = f2fs_gc_metrics[cycle][GC_FG].cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_fg_gc_avg_time")) {
+        value = f2fs_gc_metrics[cycle][GC_FG].avg_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_fg_gc_seg_cnt")) {
+        value = f2fs_gc_metrics[cycle][GC_FG].segs;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_bg_gc_cnt")) {
+        value = f2fs_gc_metrics[cycle][GC_BG].cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_bg_gc_avg_time")) {
+        value = f2fs_gc_metrics[cycle][GC_BG].avg_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_bg_gc_seg_cnt")) {
+        value = f2fs_gc_metrics[cycle][GC_BG].segs;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_cp_cnt")) {
+        value = f2fs_cp_metrics[cycle].cnt;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_cp_avg_time")) {
+        value = f2fs_cp_metrics[cycle].avg_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_cp_max_time")) {
+        value = f2fs_cp_metrics[cycle].max_time;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_ipu_cnt")) {
+        value = f2fs_cp_metrics[cycle].inplace_count;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "f2fs_fsync_cnt")) {
+        value = f2fs_metrics[cycle].fsync_cnt;
+    }
+    seq_printf(seq_filp, "%llu\n", value);
+
+    return 0;
+
+err:
+    io_metrics_print("%s(%d) I don't understand what the operation: %s/%s\n",
+                                                      current->comm,current->pid,
+             file->f_path.dentry->d_parent->d_iname, file->f_path.dentry->d_iname);
+    return -1;
+}
+
+int f2fs_metrics_proc_open(struct inode *inode, struct file *file)
+{
+    return single_open(file, f2fs_metrics_proc_show, file);
+}
+
+void f2fs_metrics_reset(void)
+{
+    int i = 0;
+    for (i = 0; i < CYCLE_MAX; i++) {
+        atomic64_set(&f2fs_metrics_timestamp[i], 0);
+    }
+    memset(&f2fs_gc_metrics, 0, sizeof(f2fs_gc_metrics));
+    memset(&f2fs_cp_metrics, 0, sizeof(f2fs_cp_metrics));
+    memset(&f2fs_metrics, 0, sizeof(f2fs_metrics));
+}
+void f2fs_metrics_init(void)
+{
+    f2fs_metrics_reset();
+    gc_t = 0;
+}
\ No newline at end of file
diff --git a/drivers/soc/oplus/storage/common/io_metrics/f2fs_metrics.h b/drivers/soc/oplus/storage/common/io_metrics/f2fs_metrics.h
new file mode 100644
index 000000000..050f05e95
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/f2fs_metrics.h
@@ -0,0 +1,12 @@
+#ifndef __F2FS_METRICS_H__
+#define __F2FS_METRICS_H__
+#include <linux/fs.h>
+#include <linux/f2fs_fs.h>
+
+void f2fs_register_tracepoint_probes(void);
+void f2fs_unregister_tracepoint_probes(void);
+int f2fs_metrics_proc_open(struct inode *inode, struct file *file);
+void f2fs_metrics_reset(void);
+void f2fs_metrics_init(void);
+
+#endif /* __F2FS_METRICS_H__ */
\ No newline at end of file
diff --git a/drivers/soc/oplus/storage/common/io_metrics/io_metrics_entry.c b/drivers/soc/oplus/storage/common/io_metrics/io_metrics_entry.c
new file mode 100644
index 000000000..f6e4b7d4a
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/io_metrics_entry.c
@@ -0,0 +1,73 @@
+#include "io_metrics_entry.h"
+#include <trace/events/block.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+#include "f2fs_metrics.h"
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+#include "block_metrics.h"
+#include "ufs_metrics.h"
+#include "procfs.h"
+#include "abnormal_io.h"
+
+bool io_metrics_enabled = false;
+bool io_metrics_debug_enabled = false;
+
+static void io_metrics_register_tracepoints(void)
+{
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    /* filesystem layer */
+    f2fs_register_tracepoint_probes();
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+    /* block layer */
+    block_register_tracepoint_probes();
+    /* ufs layer */
+    ufs_register_tracepoint_probes();
+}
+
+static void io_metrics_unregister_tracepoints(void)
+{
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    /* filesystem layer */
+    f2fs_unregister_tracepoint_probes();
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+    /* block layer */
+    block_unregister_tracepoint_probes();
+    /* ufs layer */
+    ufs_unregister_tracepoint_probes();
+    tracepoint_synchronize_unregister();
+
+    return;
+}
+
+static int __init io_metrics_init(void)
+{
+    io_metrics_print("Startting...\n");
+    io_metrics_enabled = false;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    f2fs_metrics_init();
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+    block_metrics_init();
+    ufs_metrics_reset();
+    io_metrics_register_tracepoints();
+    if (io_metrics_procfs_init())
+    {
+        io_metrics_print("io_metrics_procfs_init failed\n");
+    }
+    io_metrics_enabled = true;
+    io_metrics_print("Start OK\n");
+    return 0;
+}
+
+static void __exit io_metrics_exit(void)
+{
+    io_metrics_enabled = false;
+    io_metrics_print("io_metrics_exit\n");
+    io_metrics_unregister_tracepoints();
+    io_metrics_procfs_exit();
+}
+
+module_init(io_metrics_init);
+module_exit(io_metrics_exit);
+
+MODULE_DESCRIPTION("oplus_bsp_storage_io_metrics");
+MODULE_VERSION("1.0");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/oplus/storage/common/io_metrics/io_metrics_entry.h b/drivers/soc/oplus/storage/common/io_metrics/io_metrics_entry.h
new file mode 100644
index 000000000..73c68aecf
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/io_metrics_entry.h
@@ -0,0 +1,94 @@
+#ifndef __IO_METRICS_ENTRY_H__
+#define __IO_METRICS_ENTRY_H__
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/ctype.h>
+#include <linux/blkdev.h>
+#include <linux/percpu.h>
+#include <linux/init.h>
+#include <linux/mutex.h>
+#include <linux/export.h>
+#include <linux/time.h>
+#include <linux/uaccess.h>
+#include <linux/platform_device.h>
+#include <linux/task_io_accounting_ops.h>
+#include <linux/trace_clock.h>
+#include <linux/sched.h>
+#include <linux/err.h>
+#include <linux/namei.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/tracepoint.h>
+#include <linux/fs.h>
+#include <linux/bio.h>
+#include <linux/printk.h>
+
+#define io_metrics_print(fmt, arg...) \
+    printk("[IO_METRICS] [%-16s] %20s:%-4d "fmt, current->comm, __func__, __LINE__, ##arg)
+
+/* 统计周期 */
+enum sample_cycle_type {
+//    CYCLE_SECOND_10 = 0,
+//    CYCLE_MINUTES_1,
+//    CYCLE_MINUTES_10,
+//    CYCLE_HOUR_1,
+//    CYCLE_DAY_1,
+//    CYCLE_WEEK_1,
+    /* 记录总的流量 */
+    CYCLE_FOREVER = 0,
+    CYCLE_MAX
+};
+
+#define LAT_0_TO_100U_MASK      100000
+#define LAT_100U_TO_200U_MASK   200000
+#define LAT_200U_TO_500U_MASK   500000
+#define LAT_500U_TO_2M_MASK     2000000
+#define LAT_2M_TO_20M_MASK      20000000
+#define LAT_20M_TO_100M_MASK    100000000
+#define LAT_100M_TO_500M_MASK   500000000
+
+/* 延迟分布 */
+enum lat_range {
+    LAT_0_TO_100U = 0,   /* (0, 100us]      */
+    LAT_100U_TO_200U,    /* (100us, 200us]  */
+    LAT_200U_TO_500U,    /* (200us, 500us]  */
+    LAT_500U_TO_2M,      /* (500us, 2ms)    */
+    LAT_2M_TO_20M,       /* [2ms, 20ms)     */
+    LAT_20M_TO_100M,     /* [20ms, 200ms)   */
+    LAT_100M_TO_500M,    /* [100ms, 500ms)  */
+    LAT_500M_TO_MAX,     /* [500ms, +∞)     */
+};
+
+#define lat_range_check(elapsed, lat_range) \
+do { \
+    if (likely(elapsed <= LAT_0_TO_100U_MASK)) {   \
+        lat_range = LAT_0_TO_100U;         \
+    } else if (elapsed <= LAT_100U_TO_200U_MASK) { \
+        lat_range = LAT_100U_TO_200U;      \
+    } else if (elapsed <= LAT_200U_TO_500U_MASK) { \
+        lat_range = LAT_200U_TO_500U;      \
+    } else if (elapsed <= LAT_500U_TO_2M_MASK) {   \
+        lat_range = LAT_500U_TO_2M;        \
+    } else if (elapsed <= LAT_2M_TO_20M_MASK) {    \
+        lat_range = LAT_2M_TO_20M;         \
+    } else if (elapsed <= LAT_20M_TO_100M_MASK) {  \
+        lat_range = LAT_20M_TO_100M;       \
+    } else if (elapsed <= LAT_100M_TO_500M_MASK) { \
+        lat_range = LAT_100M_TO_500M;      \
+    }  else {                                        \
+        lat_range = LAT_500M_TO_MAX;       \
+    }                                                \
+} while (0)
+
+
+struct sample_cycle {
+    enum sample_cycle_type value;
+    const char * tag;
+    const u64 cycle_value;
+};
+
+extern bool io_metrics_enabled;
+extern bool io_metrics_debug_enabled;
+
+#endif /* __IO_METRICS_ENTRY_H__ */
\ No newline at end of file
diff --git a/drivers/soc/oplus/storage/common/io_metrics/procfs.c b/drivers/soc/oplus/storage/common/io_metrics/procfs.c
new file mode 100644
index 000000000..c5caefbb9
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/procfs.c
@@ -0,0 +1,492 @@
+#include <linux/proc_fs.h>
+#include <linux/uaccess.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/seq_file.h>
+#include <linux/dcache.h>
+
+#include "procfs.h"
+#include "block_metrics.h"
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+#include "f2fs_metrics.h"
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+#include "ufs_metrics.h"
+#include "abnormal_io.h"
+
+#define STORAGE_DIR_NODE "oplus_storage"
+#define IO_METRICS_DIR_NODE "io_metrics"
+#define IO_METRICS_CONTROL_DIR_NODE "control"
+#define DUMP_PATH_LEN 1024
+static char abnormal_io_dump_path[DUMP_PATH_LEN];
+bool proc_show_enabled = true;
+module_param(proc_show_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(proc_show_enabled, " Debug proc");
+
+#define LABEL_BUF_LEN 50
+static char label_buf[LABEL_BUF_LEN] = {"common"};
+static struct proc_dir_entry *storage_procfs;
+static struct proc_dir_entry *io_metrics_procfs;
+static struct proc_dir_entry *io_metrics_control_procfs;
+static struct proc_dir_entry *sample_dir[CYCLE_MAX] = {0};
+
+struct sample_cycle sample_cycle_config[] = {
+//    { CYCLE_SECOND_10,  "second_10", 10000000000        }, /* 10秒均值 */
+//    { CYCLE_MINUTES_1,  "minutes_1", 60000000000        }, /* 1分钟均值 */
+//    {CYCLE_MINUTES_10, "minutes_10", 600000000000       }, /* 10分钟均值 */
+//    {    CYCLE_HOUR_1,       "hour", 3600000000000      }, /* 1小时均值 */
+//    {     CYCLE_DAY_1,        "day", 86400000000000     }, /* 1天均值 */
+//    {    CYCLE_WEEK_1,       "week", 604800000000000    }, /* 1周均值 */
+    {   CYCLE_FOREVER,    "forever", 3153600000000000000}, /* 总流量 */
+};
+
+#define CREATE_IO_METRICS_CONTROL_NODE(__name, __parent)            \
+    proc_create(#__name, S_IRUGO | S_IWUGO, __parent, &__name ## _proc_ops)
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+#define DEFINE_IO_METRICS_CONTROL(__name)                           \
+static int __name ## _open(struct inode *inode, struct file *file)  \
+{                                                                   \
+    return single_open(file, __name ## _show, file);     \
+}                                                                   \
+                                                                    \
+static const struct proc_ops __name ## _proc_ops = {                \
+    .proc_open  = __name ## _open,                                  \
+    .proc_read  = seq_read,                                         \
+    .proc_write = __name ## _write,                                 \
+    .proc_lseek = seq_lseek,                                        \
+    .proc_release   = single_release,                               \
+}
+
+static const struct proc_ops block_metrics_proc_fops = {
+    .proc_open      = block_metrics_proc_open,
+    .proc_read      = seq_read,
+    .proc_lseek     = seq_lseek,
+    .proc_release   = single_release,
+};
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+static const struct proc_ops f2fs_metrics_proc_fops = {
+    .proc_open      = f2fs_metrics_proc_open,
+    .proc_read      = seq_read,
+    .proc_lseek     = seq_lseek,
+    .proc_release   = single_release,
+};
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+static const struct proc_ops ufs_metrics_proc_fops = {
+    .proc_open      = ufs_metrics_proc_open,
+    .proc_read      = seq_read,
+    .proc_lseek     = seq_lseek,
+    .proc_release   = single_release,
+};
+#else
+#define DEFINE_IO_METRICS_CONTROL(__name)                           \
+static int __name ## _open(struct inode *inode, struct file *file)  \
+{                                                                   \
+    return single_open(file, __name ## _show, file);     \
+}                                                                   \
+                                                                    \
+static const struct file_operations __name ## _proc_ops = {         \
+    .open   = __name ## _open,                                      \
+    .read   = seq_read,                                             \
+    .write  = __name ## _write,                                     \
+    .llseek = seq_lseek,                                            \
+    .release    = single_release,                                   \
+}
+
+static const struct file_operations block_metrics_proc_fops = {
+    .open      = block_metrics_proc_open,
+    .read      = seq_read,
+    .llseek     = seq_lseek,
+    .release   = single_release,
+};
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+static const struct file_operations f2fs_metrics_proc_fops = {
+    .open      = f2fs_metrics_proc_open,
+    .read      = seq_read,
+    .llseek     = seq_lseek,
+    .release   = single_release,
+};
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+static const struct file_operations ufs_metrics_proc_fops = {
+    .open      = ufs_metrics_proc_open,
+    .read      = seq_read,
+    .llseek     = seq_lseek,
+    .release   = single_release,
+};
+#endif
+
+static int io_metrics_control_show(struct seq_file *seq_filp, void *data)
+{
+    struct file *file = (struct file *)seq_filp->private;
+
+    if (proc_show_enabled || unlikely(io_metrics_debug_enabled)) {
+        io_metrics_print("%s(%d) read %s/%s\n",
+            current->comm, current->pid, file->f_path.dentry->d_parent->d_iname,
+            file->f_path.dentry->d_iname);
+    }
+
+    if (!strcmp(file->f_path.dentry->d_iname, "enable")) {
+       seq_printf(seq_filp, "%d\n", io_metrics_enabled);
+    } else if (!strcmp(file->f_path.dentry->d_iname, "debug_enable")) {
+        seq_printf(seq_filp, "%d\n", io_metrics_debug_enabled);
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_enabled")) {
+        seq_printf(seq_filp, "%d\n", atomic_read(&abnormal_io_enabled));
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_trigger")) {
+        seq_printf(seq_filp, "%d\n", abnormal_io_trigger);
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_dump_min_interval_s")) {
+        seq_printf(seq_filp, "%d\n", abnormal_io_dump_min_interval_s);
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_dump_limit_1_day")) {
+        seq_printf(seq_filp, "%d\n", abnormal_io_dump_limit_1_day);
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_dump_path")) {
+        abnormal_io_dump_path[DUMP_PATH_LEN - 1] = '\0';
+        seq_printf(seq_filp, "%s\n", abnormal_io_dump_path);
+    }
+
+    return 0;
+}
+
+static ssize_t io_metrics_control_write(struct file *file,
+                       const char __user *buf, size_t len, loff_t *ppos)
+{
+    enum  {
+        INT_VALUE = 0,
+        BOOL_VALUE,
+        STR_VALUE
+    } val_t = INT_VALUE;
+#define DATA_LEN    DUMP_PATH_LEN
+    char buffer[DATA_LEN+1] = {0};
+    int ret = 0;
+    int value = 0;
+
+    len = (len > DATA_LEN) ? DATA_LEN : len;
+    //kstrtoint_from_user
+    if (copy_from_user(buffer, buf, len)) {
+        return -EFAULT;
+    }
+    buffer[len] = '\0';
+
+    if (!strcmp(file->f_path.dentry->d_iname, "enable")) {
+        ret = kstrtoint(strstrip(buffer), len, &value);
+        if (ret) {
+            return ret;
+        }
+        val_t = INT_VALUE;
+        WRITE_ONCE(io_metrics_enabled, value);
+    } else if (!strcmp(file->f_path.dentry->d_iname, "debug_enable")) {
+        ret = kstrtoint(strstrip(buffer), len, &value);
+        if (ret) {
+            return ret;
+        }
+        val_t = INT_VALUE;
+        WRITE_ONCE(io_metrics_debug_enabled, value);
+    } else if (!strcmp(file->f_path.dentry->d_iname, "reset_stat")) {
+        ret = kstrtoint(strstrip(buffer), len, &value);
+        if (ret) {
+            return ret;
+        }
+        val_t = INT_VALUE;
+        if (value == 1) {
+            io_metrics_print("reset_stat start\n");
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+            f2fs_metrics_reset();
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+            block_metrics_reset();
+            ufs_metrics_reset();
+            strcpy(label_buf, "common");
+        }
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_enabled")) {
+        ret = kstrtoint(strstrip(buffer), len, &value);
+        if (ret) {
+            return ret;
+        }
+        val_t = INT_VALUE;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+        if (0 == value) {
+            abnormal_io_exit();
+        } else if (1 == value) {
+            abnormal_io_init();
+        }
+#else
+        io_metrics_print("kernel-6.6 do not support abnormal io\n");
+#endif
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_trigger")) {
+        ret = kstrtoint(strstrip(buffer), len, &value);
+        if (ret) {
+            return ret;
+        }
+        val_t = INT_VALUE;
+        if (value == 1) {
+            ret = abnormal_io_dump_to_file("/data/persist_log/DCS/de/storage/storage_io.hex");
+            if (!ret) {
+                io_metrics_print("abnormal_io_dump_to_file err(%d)\n", ret);
+            }
+        }
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_dump_min_interval_s")) {
+        ret = kstrtoint(strstrip(buffer), len, &value);
+        if (ret) {
+            return ret;
+        }
+        val_t = INT_VALUE;
+        WRITE_ONCE(abnormal_io_dump_min_interval_s, value);
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_dump_limit_1_day")) {
+        ret = kstrtoint(strstrip(buffer), len, &value);
+        if (ret) {
+            return ret;
+        }
+        val_t = INT_VALUE;
+        WRITE_ONCE(abnormal_io_dump_limit_1_day, value);
+    } else if (!strcmp(file->f_path.dentry->d_iname, "abnormal_io_dump_path")) {
+        val_t = STR_VALUE;
+        if (strncpy(abnormal_io_dump_path, strstrip(buffer), len)) {
+            if (strcat(abnormal_io_dump_path, "/storage_io.hex")) {
+                ret = abnormal_io_dump_to_file((const char *)abnormal_io_dump_path);
+                if (!ret) {
+                    io_metrics_print("abnormal_io_dump_to_file err(%d)\n", ret);
+                }
+            }
+        }
+    }
+    /* update log */
+    if (val_t == INT_VALUE) {
+        io_metrics_print("%s(%d) write %d to %s/%s\n", current->comm, current->pid, value,
+                     file->f_path.dentry->d_parent->d_iname, file->f_path.dentry->d_iname);
+    } else if (val_t == STR_VALUE) {
+        io_metrics_print("%s(%d) write %s to %s/%s\n", current->comm, current->pid, buffer,
+                     file->f_path.dentry->d_parent->d_iname, file->f_path.dentry->d_iname);
+    }
+    *ppos += len;
+
+    return len;
+}
+
+int io_metrics_control_open(struct inode *inode, struct file *file)
+{
+    return single_open(file, io_metrics_control_show, file);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+static const struct proc_ops io_metrics_control_proc_fops = {
+    .proc_open      = io_metrics_control_open,
+    .proc_read      = seq_read,
+    .proc_write     = io_metrics_control_write,
+    .proc_lseek     = seq_lseek,
+    .proc_release   = single_release,
+};
+#else
+static const struct file_operations io_metrics_control_proc_fops = {
+    .open      = io_metrics_control_open,
+    .read      = seq_read,
+    .write     = io_metrics_control_write,
+    .llseek     = seq_lseek,
+    .release   = single_release,
+};
+#endif
+
+enum node_type {
+    F2FS = 0,
+    BLOCK,
+    UFS,
+    CONTROL,
+};
+
+struct {
+    const char *name;
+    enum node_type node_type;
+    umode_t mode;
+} io_metrics_procfs_node[] = {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    /* filesystem layer */
+    {"f2fs_discard_cnt",             F2FS, S_IRUGO},
+    {"f2fs_discard_len",             F2FS, S_IRUGO},
+    {"f2fs_fg_gc_cnt",               F2FS, S_IRUGO},
+    {"f2fs_fg_gc_avg_time",          F2FS, S_IRUGO},
+    {"f2fs_fg_gc_seg_cnt",           F2FS, S_IRUGO},
+    {"f2fs_bg_gc_cnt",               F2FS, S_IRUGO},
+    {"f2fs_bg_gc_avg_time",          F2FS, S_IRUGO},
+    {"f2fs_bg_gc_seg_cnt",           F2FS, S_IRUGO},
+    {"f2fs_cp_cnt",                  F2FS, S_IRUGO},
+    {"f2fs_cp_avg_time",             F2FS, S_IRUGO},
+    {"f2fs_cp_max_time",             F2FS, S_IRUGO},
+    {"f2fs_ipu_cnt",                 F2FS, S_IRUGO},
+    {"f2fs_fsync_cnt",                F2FS, S_IRUGO},
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+    /* block layer */
+    {"bio_read_cnt",                BLOCK, S_IRUGO},
+    {"bio_read_avg_size",           BLOCK, S_IRUGO},
+    {"bio_read_size_dist",          BLOCK, S_IRUGO},
+    {"bio_read_avg_time",           BLOCK, S_IRUGO},
+    {"bio_read_max_time",           BLOCK, S_IRUGO},
+    {"bio_read_4k_blk_avg_time",    BLOCK, S_IRUGO},
+    {"bio_read_4k_blk_max_time",    BLOCK, S_IRUGO},
+    {"bio_read_4k_blk_lat_dist",    BLOCK, S_IRUGO},
+    {"bio_read_4k_drv_avg_time",    BLOCK, S_IRUGO},
+    {"bio_read_4k_drv_max_time",    BLOCK, S_IRUGO},
+    {"bio_read_4k_drv_lat_dist",    BLOCK, S_IRUGO},
+    {"bio_read_512k_blk_avg_time",  BLOCK, S_IRUGO},
+    {"bio_read_512k_blk_max_time",  BLOCK, S_IRUGO},
+    {"bio_read_512k_blk_lat_dist",  BLOCK, S_IRUGO},
+    {"bio_read_512k_drv_avg_time",  BLOCK, S_IRUGO},
+    {"bio_read_512k_drv_max_time",  BLOCK, S_IRUGO},
+    {"bio_read_512k_drv_lat_dist",  BLOCK, S_IRUGO},
+    {"bio_write_cnt",               BLOCK, S_IRUGO},
+    {"bio_write_avg_size",          BLOCK, S_IRUGO},
+    {"bio_write_size_dist",         BLOCK, S_IRUGO},
+    {"bio_write_avg_time",          BLOCK, S_IRUGO},
+    {"bio_write_max_time",          BLOCK, S_IRUGO},
+    {"bio_write_4k_blk_avg_time",   BLOCK, S_IRUGO},
+    {"bio_write_4k_blk_max_time",   BLOCK, S_IRUGO},
+    {"bio_write_4k_blk_lat_dist",   BLOCK, S_IRUGO},
+    {"bio_write_4k_drv_avg_time",   BLOCK, S_IRUGO},
+    {"bio_write_4k_drv_max_time",   BLOCK, S_IRUGO},
+    {"bio_write_4k_drv_lat_dist",   BLOCK, S_IRUGO},
+    {"bio_write_512k_blk_avg_time", BLOCK, S_IRUGO},
+    {"bio_write_512k_blk_max_time", BLOCK, S_IRUGO},
+    {"bio_write_512k_blk_lat_dist", BLOCK, S_IRUGO},
+    {"bio_write_512k_drv_avg_time", BLOCK, S_IRUGO},
+    {"bio_write_512k_drv_max_time", BLOCK, S_IRUGO},
+    {"bio_write_512k_drv_lat_dist", BLOCK, S_IRUGO},
+    /* ufs layer */
+    {"ufs_total_read_size_mb",        UFS, S_IRUGO},
+    {"ufs_total_read_time_ms",        UFS, S_IRUGO},
+    {"ufs_total_write_size_mb",       UFS, S_IRUGO},
+    {"ufs_total_write_time_ms",       UFS, S_IRUGO},
+    {"ufs_read_lat_dist",             UFS, S_IRUGO},
+    {"ufs_write_lat_dist",            UFS, S_IRUGO},
+    /* control */
+    {"enable",                    CONTROL, S_IRUGO | S_IWUGO},
+    {"debug_enable",              CONTROL, S_IRUGO | S_IWUGO},
+    {"reset_stat",                CONTROL, S_IRUGO | S_IWUGO},
+    {"abnormal_io_enabled",       CONTROL, S_IRUGO | S_IWUGO},
+    {"abnormal_io_trigger",       CONTROL, S_IRUGO | S_IWUGO},
+    {"abnormal_io_dump_min_interval_s", CONTROL, S_IRUGO | S_IWUGO},
+    {"abnormal_io_dump_limit_1_day",        CONTROL, S_IRUGO | S_IWUGO},
+    {"abnormal_io_dump_path",               CONTROL, S_IRUGO | S_IWUGO},
+    {NULL,                               0,                 0}
+};
+
+static int label_show(struct seq_file *seq_filp, void *data)
+{
+    struct file *file = (struct file *)seq_filp->private;
+    seq_printf(seq_filp, "%s\n", label_buf);
+    if (proc_show_enabled) {
+        io_metrics_print("%s(%d) read %s/%s: %s\n",
+            current->comm, current->pid, file->f_path.dentry->d_parent->d_iname,
+            file->f_path.dentry->d_iname, label_buf);
+    }
+
+    return 0;
+}
+
+static ssize_t label_write(struct file *file,
+                       const char __user *buf, size_t len, loff_t *ppos)
+{
+    len = len < (LABEL_BUF_LEN - 1) ? len : (LABEL_BUF_LEN - 1);
+    if (copy_from_user(label_buf, buf, len)) {
+        return -EFAULT;
+    }
+    label_buf[LABEL_BUF_LEN - 1] = '\0';
+    *ppos += len;
+    io_metrics_print("%s(%d) write %s to %s/%s\n", current->comm, current->pid, label_buf,
+                     file->f_path.dentry->d_parent->d_iname, file->f_path.dentry->d_iname);
+
+    return len;
+}
+DEFINE_IO_METRICS_CONTROL(label);
+
+int io_metrics_procfs_init(void)
+{
+    int i = 0;
+    int j = 0;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+    struct proc_ops *proc_ops = NULL;
+#else
+    struct file_operations *proc_ops = NULL;
+#endif
+
+    struct proc_dir_entry *pnode;
+    /* /proc/oplus_storage */
+    storage_procfs = proc_mkdir(STORAGE_DIR_NODE, NULL);
+    if (!storage_procfs) {
+        io_metrics_print("Can't create procfs node\n");
+        goto error_out;
+    }
+    /* /proc/oplus_storage/io_metrics */
+    io_metrics_procfs = proc_mkdir(IO_METRICS_DIR_NODE, storage_procfs);
+    if (!io_metrics_procfs) {
+        io_metrics_print("Can't create procfs node\n");
+        goto error_out;
+    }
+    /* /proc/oplus_storage/io_metrics/control */
+    io_metrics_control_procfs = proc_mkdir(IO_METRICS_CONTROL_DIR_NODE, io_metrics_procfs);
+    if (!io_metrics_control_procfs) {
+        io_metrics_print("Can't create procfs node\n");
+        goto error_out;
+    }
+    /* 在/proc/oplus_storage/io_metrics下面创建按照周期统计的目录 */
+    for (i = 0; i < CYCLE_MAX; i++) {
+        sample_dir[i] = proc_mkdir(sample_cycle_config[i].tag, io_metrics_procfs);
+        if (!sample_dir[i]) {
+            io_metrics_print("Can't create procfs node\n");
+            goto error_out;
+        }
+    }
+    for (i = 0; io_metrics_procfs_node[i].name; i++) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+        if (io_metrics_procfs_node[i].node_type == BLOCK) {
+            proc_ops = (struct proc_ops *)&block_metrics_proc_fops;
+        }
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+		else if (io_metrics_procfs_node[i].node_type == F2FS) {
+            proc_ops = (struct proc_ops *)&f2fs_metrics_proc_fops;
+        }
+#endif /* (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)) */
+		else if (io_metrics_procfs_node[i].node_type == UFS) {
+            proc_ops = (struct proc_ops *)&ufs_metrics_proc_fops;
+        }
+#else
+        if (io_metrics_procfs_node[i].node_type == F2FS) {
+            proc_ops = (struct file_operations *)&f2fs_metrics_proc_fops;
+        } else if (io_metrics_procfs_node[i].node_type == BLOCK) {
+            proc_ops = (struct file_operations *)&block_metrics_proc_fops;
+        } else if (io_metrics_procfs_node[i].node_type == UFS) {
+            proc_ops = (struct file_operations *)&ufs_metrics_proc_fops;
+        }
+#endif
+        if (io_metrics_procfs_node[i].node_type == CONTROL) {
+            pnode = proc_create(io_metrics_procfs_node[i].name,
+                                io_metrics_procfs_node[i].mode,
+                                io_metrics_control_procfs,
+                                &io_metrics_control_proc_fops);
+            if (!pnode) {
+                io_metrics_print("Can't create %s\n", io_metrics_procfs_node[i].name);
+                goto error_out;
+            }
+        } else {
+            for (j = 0; j < CYCLE_MAX; j++) {
+                pnode = proc_create(io_metrics_procfs_node[i].name,
+                                    io_metrics_procfs_node[i].mode,
+                                    sample_dir[j],
+                                    proc_ops);
+                if (!pnode) {
+                    io_metrics_print("Can't create %s\n", io_metrics_procfs_node[i].name);
+                    goto error_out;
+                }
+            }
+        }
+    }
+    CREATE_IO_METRICS_CONTROL_NODE(label, io_metrics_control_procfs);
+
+    return 0;
+
+error_out:
+    return -1;
+}
+void io_metrics_procfs_exit(void)
+{
+    if (storage_procfs) {
+        if (io_metrics_procfs) {
+            remove_proc_entry(IO_METRICS_DIR_NODE, storage_procfs);
+        }
+        remove_proc_entry(STORAGE_DIR_NODE, NULL);
+    }
+}
diff --git a/drivers/soc/oplus/storage/common/io_metrics/procfs.h b/drivers/soc/oplus/storage/common/io_metrics/procfs.h
new file mode 100644
index 000000000..76035d1e1
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/procfs.h
@@ -0,0 +1,10 @@
+#ifndef __PROFS_H__
+#define __PROFS_H__
+#include "io_metrics_entry.h"
+
+extern bool proc_show_enabled;
+extern struct sample_cycle sample_cycle_config[CYCLE_MAX];
+int io_metrics_procfs_init(void);
+void io_metrics_procfs_exit(void);
+
+#endif /* __PROFS_H__ */
\ No newline at end of file
diff --git a/drivers/soc/oplus/storage/common/io_metrics/ufs_metrics.c b/drivers/soc/oplus/storage/common/io_metrics/ufs_metrics.c
new file mode 100644
index 000000000..c911c0878
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/ufs_metrics.c
@@ -0,0 +1,259 @@
+#include "io_metrics_entry.h"
+#include "procfs.h"
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(6, 1, 0)
+#include "drivers/scsi/ufs/ufshcd.h"
+#else
+#include <scsi/scsi_cmnd.h>
+#include <ufs/ufshcd.h>
+#endif
+#include "ufs_metrics.h"
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+#include <trace/hooks/ufshcd.h>
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 4, 0)
+#include <trace/hooks/oplus_ufs.h>
+#endif
+
+#define UFS_METRICS_LAT(op)   \
+    atomic64_t ufs_metrics_lat_##op[LAT_500M_TO_MAX + 1] = {0};
+
+UFS_METRICS_LAT(write);
+UFS_METRICS_LAT(read);
+
+bool ufs_compl_command_enabled = false;
+module_param(ufs_compl_command_enabled, bool, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(ufs_compl_command_enabled, " Debug android_vh_ufs_compl_command");
+
+atomic64_t ufs_metrics_timestamp[CYCLE_MAX];
+
+struct {
+    atomic64_t read_size;
+    atomic64_t read_cnt;
+    atomic64_t read_elapse;
+    atomic64_t write_size;
+    atomic64_t write_cnt;
+    atomic64_t write_elapse;
+    char padding[16];
+} ufs_metrics[CYCLE_MAX] = {0};
+
+void cb_android_vh_ufs_compl_command(void *ignore, struct ufs_hba *hba,
+                                     struct ufshcd_lrb *lrbp)
+{
+    ktime_t elapsed_in_ufs;
+    int transfer_len = 0;
+    u64 ufs_lat_range = 0;
+
+    if (unlikely(!io_metrics_enabled)) {
+        return ;
+    }
+    if (!lrbp->cmd) {
+        goto exit;
+    }
+    /*complete*/
+    elapsed_in_ufs = lrbp->compl_time_stamp - lrbp->issue_time_stamp;
+    switch(lrbp->cmd->cmnd[0]) {
+        case READ_6:
+        case READ_10:
+        case READ_16:
+        {
+            int i;
+            u64 current_time_ns, elapse;
+            transfer_len = be32_to_cpu(lrbp->ucd_req_ptr->sc.exp_data_transfer_len);
+            current_time_ns = lrbp->compl_time_stamp;
+            for (i = 0; i < CYCLE_MAX; i++) {
+                elapse = current_time_ns - atomic64_read(&ufs_metrics_timestamp[i]);
+                if (unlikely(elapse >= current_time_ns)) {
+                    atomic64_set(&ufs_metrics_timestamp[i], current_time_ns);
+                    atomic64_set(&ufs_metrics[i].read_cnt, 1);
+                    atomic64_set(&ufs_metrics[i].read_size, transfer_len);
+                    atomic64_set(&ufs_metrics[i].read_elapse, elapsed_in_ufs);
+                    elapse = 0;
+                    lat_range_check(elapsed_in_ufs, ufs_lat_range);
+                    memset(&ufs_metrics_lat_read, 0, sizeof(ufs_metrics_lat_read));
+                    atomic64_set(&ufs_metrics_lat_read[ufs_lat_range], 1);
+                } else {
+                    atomic64_inc(&ufs_metrics[i].read_cnt);
+                    atomic64_add(transfer_len, &ufs_metrics[i].read_size);
+                    atomic64_add(elapsed_in_ufs, &ufs_metrics[i].read_elapse);
+                    lat_range_check(elapsed_in_ufs, ufs_lat_range);
+                    atomic64_inc(&ufs_metrics_lat_read[ufs_lat_range]);
+                }
+                if (unlikely(elapse >= sample_cycle_config[i].cycle_value)) {
+                    /* 过期复位 */
+                    atomic64_set(&ufs_metrics_timestamp[i], 0);
+                }
+            }
+            if (unlikely(ufs_compl_command_enabled || io_metrics_debug_enabled)) {
+                io_metrics_print("read %d bytes cost %llu ns\n",
+                                 transfer_len, elapsed_in_ufs);
+            }
+            break;
+        }
+        case WRITE_6:
+        case WRITE_10:
+        case WRITE_16:
+        {
+            int i;
+            u64 current_time_ns, elapse;
+            transfer_len = be32_to_cpu(lrbp->ucd_req_ptr->sc.exp_data_transfer_len);
+            current_time_ns = lrbp->compl_time_stamp;
+            for (i = 0; i < CYCLE_MAX; i++) {
+                elapse = current_time_ns - atomic64_read(&ufs_metrics_timestamp[i]);
+                if (unlikely(elapse >= current_time_ns)) {
+                    atomic64_set(&ufs_metrics_timestamp[i], current_time_ns);
+                    atomic64_set(&ufs_metrics[i].write_cnt, 1);
+                    atomic64_set(&ufs_metrics[i].write_size, transfer_len);
+                    atomic64_set(&ufs_metrics[i].write_elapse, elapsed_in_ufs);
+                    elapse = 0;
+                    lat_range_check(elapsed_in_ufs, ufs_lat_range);
+                    memset(&ufs_metrics_lat_write, 0, sizeof(ufs_metrics_lat_write));
+                    atomic64_set(&ufs_metrics_lat_write[ufs_lat_range], 1);
+                } else {
+                    atomic64_inc(&ufs_metrics[i].write_cnt);
+                    atomic64_add(transfer_len, &ufs_metrics[i].write_size);
+                    atomic64_add(elapsed_in_ufs, &ufs_metrics[i].write_elapse);
+                    lat_range_check(elapsed_in_ufs, ufs_lat_range);
+                    atomic64_inc(&ufs_metrics_lat_write[ufs_lat_range]);
+                }
+                if (unlikely(elapse >= sample_cycle_config[i].cycle_value)) {
+                    /* 过期复位 */
+                    atomic64_set(&ufs_metrics_timestamp[i], 0);
+                }
+            }
+            if (unlikely(ufs_compl_command_enabled || io_metrics_debug_enabled)) {
+                io_metrics_print("write %d bytes cost %llu ns\n",
+                                 transfer_len, elapsed_in_ufs);
+            }
+            break;
+        }
+        case UNMAP:
+        {
+            if (unlikely(ufs_compl_command_enabled || io_metrics_debug_enabled)) {
+                transfer_len = be32_to_cpu(lrbp->ucd_req_ptr->sc.exp_data_transfer_len);
+                io_metrics_print("unmap %d bytes cost %llu ns\n",
+                                 transfer_len, elapsed_in_ufs);
+            }
+            break;
+        }
+        default:
+            goto exit;
+    }
+
+exit:
+    return;
+}
+
+void ufs_register_tracepoint_probes(void)
+{
+    int ret = 0;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 4, 0)
+    ret = register_trace_android_vh_ufs_compl_command(cb_android_vh_ufs_compl_command, NULL);
+    WARN_ON(ret);
+#endif
+    io_metrics_print("run:%d\n", ret);
+}
+
+void ufs_unregister_tracepoint_probes(void)
+{
+#if LINUX_VERSION_CODE > KERNEL_VERSION(5, 4, 0)
+    unregister_trace_android_vh_ufs_compl_command(cb_android_vh_ufs_compl_command, NULL);
+#endif
+    return;
+}
+
+static int ufs_metrics_proc_show(struct seq_file *seq_filp, void *data)
+{
+    u64 value = 123;
+    struct file *file = (struct file *)seq_filp->private;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 4, 0)
+    int i = 0;
+    enum sample_cycle_type cycle;
+#endif
+
+    if (unlikely(!io_metrics_enabled)) {
+        seq_printf(seq_filp, "io_metrics_enabled not set to 1:%d\n", io_metrics_enabled);
+        return 0;
+    }
+
+    if (proc_show_enabled || unlikely(io_metrics_debug_enabled)) {
+        io_metrics_print("%s(%d) read %s/%s\n",
+            current->comm, current->pid, file->f_path.dentry->d_parent->d_iname,
+            file->f_path.dentry->d_iname);
+    }
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 4, 0)
+    /* 确定采样周期的值（父目录） */
+    cycle = CYCLE_MAX;
+    for (i = 0; i < CYCLE_MAX; i++) {
+        if(!strcmp(file->f_path.dentry->d_parent->d_iname, sample_cycle_config[i].tag)) {
+            cycle = sample_cycle_config[i].value;
+        }
+    }
+    if (unlikely(cycle == CYCLE_MAX)) {
+        goto err;
+    }
+    if(!strcmp(file->f_path.dentry->d_iname, "ufs_total_read_size_mb")) {
+        value = atomic64_read(&ufs_metrics[cycle].read_size) >> 20;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "ufs_total_read_time_ms")) {
+        /*1ns=1/(1000*1000)ms≈1/(1024*1024)ms=1>>20ms,Precision=95.1%*/
+        value = atomic64_read(&ufs_metrics[cycle].read_elapse) >> 20;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "ufs_read_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&ufs_metrics_lat_read[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "ufs_total_write_size_mb")) {
+        value = atomic64_read(&ufs_metrics[cycle].write_size) >> 20;;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "ufs_total_write_time_ms")) {
+        value = atomic64_read(&ufs_metrics[cycle].write_elapse) >> 20;
+    } else if (!strcmp(file->f_path.dentry->d_iname, "ufs_write_lat_dist")) {
+        for (i = 0; i <= LAT_500M_TO_MAX; i++) {
+            seq_printf(seq_filp, "%llu,", atomic64_read(&ufs_metrics_lat_write[i]));
+        }
+        seq_printf(seq_filp, "\n");
+        return 0;
+    }
+#else
+    value = 0;
+#endif
+    seq_printf(seq_filp, "%llu\n", value);
+
+    return 0;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 4, 0)
+err:
+    io_metrics_print("%s(%d) I don't understand what the operation: %s/%s\n",
+             current->comm,current->pid, file->f_path.dentry->d_parent->d_iname,
+             file->f_path.dentry->d_iname);
+    return -1;
+#endif
+}
+
+int ufs_metrics_proc_open(struct inode *inode, struct file *file)
+{
+    return single_open(file, ufs_metrics_proc_show, file);
+}
+
+void ufs_metrics_reset(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 4, 0)
+    int i = 0;
+
+    for (i = 0; i < CYCLE_MAX; i++) {
+        atomic64_set(&ufs_metrics_timestamp[i], 0);
+        atomic64_set(&ufs_metrics[i].read_size, 0);
+        atomic64_set(&ufs_metrics[i].read_cnt, 0);
+        atomic64_set(&ufs_metrics[i].read_elapse, 0);
+        atomic64_set(&ufs_metrics[i].write_size, 0);
+        atomic64_set(&ufs_metrics[i].write_cnt, 0);
+        atomic64_set(&ufs_metrics[i].write_elapse, 0);
+    }
+    memset(&ufs_metrics_lat_write, 0, sizeof(ufs_metrics_lat_write));
+    memset(&ufs_metrics_lat_read, 0, sizeof(ufs_metrics_lat_read));
+#else
+    return;
+#endif
+}
+void ufs_metrics_init(void)
+{
+    ufs_metrics_reset();
+}
\ No newline at end of file
diff --git a/drivers/soc/oplus/storage/common/io_metrics/ufs_metrics.h b/drivers/soc/oplus/storage/common/io_metrics/ufs_metrics.h
new file mode 100644
index 000000000..16a24ace5
--- /dev/null
+++ b/drivers/soc/oplus/storage/common/io_metrics/ufs_metrics.h
@@ -0,0 +1,12 @@
+#ifndef __UFS_METRICS_H__
+#define __UFS_METRICS_H__
+
+#include <linux/fs.h>
+
+void ufs_register_tracepoint_probes(void);
+void ufs_unregister_tracepoint_probes(void);
+int ufs_metrics_proc_open(struct inode *inode, struct file *file);
+void ufs_metrics_reset(void);
+void ufs_metrics_init(void);
+
+#endif /* __UFS_METRICS_H__ */
\ No newline at end of file
diff --git a/drivers/soc/oplus/storage/include/storage.h b/drivers/soc/oplus/storage/include/storage.h
new file mode 100644
index 000000000..79d3f5236
--- /dev/null
+++ b/drivers/soc/oplus/storage/include/storage.h
@@ -0,0 +1,11 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+* Copyright (C) 2020 Oplus. All rights reserved.
+*/
+
+#ifndef _STORAGE_H
+#define _STORAGE_H
+
+extern int pr_storage(const char *fmt, ...);
+
+#endif /*_STORAGE_H*/
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/Kconfig b/drivers/soc/oplus/storage/storage_feature_in_module/Kconfig
new file mode 100644
index 000000000..4d9876374
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/Kconfig
@@ -0,0 +1,5 @@
+source "drivers/soc/oplus/storage/common/storage_log/Kconfig"
+source "drivers/soc/oplus/storage/common/oplus_uprobe/Kconfig"
+source "drivers/soc/oplus/storage/common/ufs_oplus_dbg/Kconfig"
+source "drivers/soc/oplus/storage/common/oplus_f2fslog_storage/Kconfig"
+source "drivers/soc/oplus/storage/common/wq_dynamic_priority/Kconfig"
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/Makefile b/drivers/soc/oplus/storage/storage_feature_in_module/Makefile
new file mode 100644
index 000000000..e366ce37f
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/Makefile
@@ -0,0 +1,6 @@
+GCOV_PROFILE := y
+
+obj-y	+= common/storage_log/
+obj-y	+= common/oplus_uprobe/
+obj-y	+= common/oplus_f2fslog_storage/
+obj-y	+= common/wq_dynamic_priority/
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/Kconfig b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/Kconfig
new file mode 100644
index 000000000..21992dde6
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/Kconfig
@@ -0,0 +1,7 @@
+
+config OPLUS_F2FSLOG_STORAGE
+    tristate "f2fs log storage"
+    default n
+    help
+      define this config to enable f2fs log storage.
+
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/Makefile b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/Makefile
new file mode 100644
index 000000000..1bab20e7b
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/Makefile
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: GPL-2.0-only
+# Copyright (C) 2022-2030 Oplus. All rights reserved.
+
+LINUXINCLUDE += -I$(srctree)/
+obj-$(CONFIG_OPLUS_F2FSLOG_STORAGE) += oplus_f2fslog_storage.o
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/oplus_f2fslog_storage.c b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/oplus_f2fslog_storage.c
new file mode 100644
index 000000000..59327e34f
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_f2fslog_storage/oplus_f2fslog_storage.c
@@ -0,0 +1,229 @@
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/kprobes.h>
+#include <linux/printk.h>
+#include <linux/errno.h>
+#include <linux/f2fs_fs.h>
+#include <linux/ptrace.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include "fs/f2fs/f2fs.h"
+#include <linux/proc_fs.h>
+#include <linux/mutex.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/suspend.h>
+#include <linux/platform_device.h>
+#include <linux/version.h>
+#include <linux/string.h>
+#include <linux/rwsem.h>
+#include <linux/timer.h>
+#include <linux/reboot.h>
+#include <linux/signal.h>
+#include "kernel/trace/trace_probe.h"
+#include <linux/kstrtox.h>
+#include <linux/ctype.h>
+#include <linux/debugfs.h>
+#include <linux/of.h>
+#include <linux/rtc.h>
+#include <linux/kthread.h>
+#include <linux/moduleparam.h>
+#include <linux/namei.h>
+#include <linux/uprobes.h>
+#include <linux/time64.h>
+#include <linux/sched/clock.h>
+#include <linux/fs.h>
+#include <linux/proc_fs.h>
+#include <linux/err.h>
+
+#define WRITE_BUFSIZE        4
+#define OPLUS_F2FS_PRINTK "[F2FS]"
+#define SIGVOLD_GET_LOG (SIGRTMIN + 0x13)
+#define BOOT_MAX_ERRLOG_NUM  2
+#define LOGLEVEL_BASE        48
+#define KERN_EME_LEVEL       0
+#define KERN_ERR_LEVEL       3
+#define KERN_WAEN_LEVEL      4
+#define KERN_DBG_LEVEL       7
+#define DElTA_T              60
+static int boot_geterrlog_num = 0;
+static struct proc_dir_entry *oplus_f2fslog_storage_procfs;
+static struct proc_dir_entry *oplus_f2fslog_level;
+extern int pr_storage(const char *fmt, ...);
+static int f2fslog_level = KERN_WAEN_LEVEL;
+static u64 timestamp[2] = {0};
+
+static struct task_struct *f2fs_get_task_struct_by_comm(const char *comm) {
+    struct task_struct *task;
+
+    for_each_process(task) {
+        if (strcmp(task->comm, comm) == 0) {
+            return task;
+        }
+    }
+
+    return NULL;
+}
+
+static int oplus_f2fslog_storage_show(struct seq_file *m, void *v)
+{
+    seq_printf(m, "f2fslog level: %d\n", f2fslog_level);
+    pr_storage("f2fslog level: %d\n", f2fslog_level);
+    return 0;
+}
+
+static int oplus_f2fslog_storage_open(struct inode *inode, struct file *file)
+{
+    return single_open(file, oplus_f2fslog_storage_show, inode->i_private);
+}
+
+static ssize_t oplus_f2fslog_storage_write(struct file *file, const char __user *buffer,
+                size_t count, loff_t *ppos)
+{
+    char kbuf[5] = {0};
+    int ret = 0, tmp = 0;
+
+    if (count > WRITE_BUFSIZE) {
+        printk("input str is too long %s %d\n", __FUNCTION__, __LINE__);
+        return -EINVAL;
+    }
+
+    tmp = f2fslog_level;
+
+    if (copy_from_user(kbuf, buffer, count)) {
+        printk("copy data from user buffer failed %s %d\n", __FUNCTION__, __LINE__);
+    }
+
+    ret = kstrtoint(kbuf, 10, &f2fslog_level);
+    if(ret == 0){
+        printk("kstrtoint success %s %d %s %d\n",kbuf, f2fslog_level, __FUNCTION__, __LINE__);
+    } else {
+        printk("kstrtoint fail\n");
+    }
+
+    if(f2fslog_level > KERN_DBG_LEVEL || f2fslog_level < KERN_EME_LEVEL) {
+        f2fslog_level = tmp;
+        printk("f2fs log level set error %d %s %d\n", f2fslog_level, __FUNCTION__, __LINE__);
+    }
+
+    return count;
+}
+
+static struct proc_ops oplus_f2fsloglevel_proc_ops = {
+    .proc_open          = oplus_f2fslog_storage_open,
+    .proc_read          = seq_read,
+    .proc_write         = oplus_f2fslog_storage_write,
+    .proc_release       = single_release,
+    .proc_lseek         = default_llseek,
+};
+
+void f2fs_printk_wrapper(struct f2fs_sb_info *sbi, const char *fmt, ...)
+{
+    va_list args;
+    char buf[1024];
+    int level, log_level;
+    struct va_format vaf;
+    struct task_struct *task;
+    unsigned long ts = 0;
+
+    level = printk_get_level(fmt);
+    log_level = level - LOGLEVEL_BASE;
+    if(log_level > f2fslog_level || log_level < KERN_EME_LEVEL || log_level > KERN_DBG_LEVEL) {
+        return;
+    }
+
+    ts = local_clock();
+    do_div(ts, 1000000000);
+    timestamp[1] = ts;
+
+    va_start(args, fmt);
+    vaf.fmt = printk_skip_level(fmt);
+    vaf.va = &args;
+
+    //skip printk_skip_level for print
+    fmt = printk_skip_level(fmt);
+
+    vsnprintf(buf, sizeof(buf), fmt, args);
+    va_end(args);
+    pr_storage(OPLUS_F2FS_PRINTK "[%s][%c]: %s\n", sbi ? sbi->sb->s_id : "unknown", level , buf);
+
+    if(boot_geterrlog_num < BOOT_MAX_ERRLOG_NUM && log_level <= KERN_ERR_LEVEL) {
+        if(timestamp[1] - timestamp[0] > DElTA_T) {
+            task = f2fs_get_task_struct_by_comm("Binder:vold");
+            printk("%s, %d\n", __FUNCTION__, __LINE__);
+            if (!task) {
+                pr_err(OPLUS_F2FS_PRINTK "No task_struct found for process\n");
+                pr_storage(OPLUS_F2FS_PRINTK "No task_struct found for process\n");
+            } else{
+                send_sig_info(SIGVOLD_GET_LOG, SEND_SIG_PRIV, task);
+            }
+
+            boot_geterrlog_num ++;
+            timestamp[0] = timestamp[1];
+        }
+    }
+}
+
+static int handler_f2fslog_storage_pre(struct kprobe *p, struct pt_regs *regs)
+{
+    struct f2fs_sb_info *sbi = (struct f2fs_sb_info *)regs->regs[0];
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0))
+    const char *fmt = (const char *)regs->regs[1];
+    unsigned long arg2 = regs->regs[2];
+    unsigned long arg3 = regs->regs[3];
+
+    f2fs_printk_wrapper(sbi, fmt, arg2, arg3);
+
+#else
+    const char *fmt = (const char *)regs->regs[2];
+    unsigned long arg3 = regs->regs[3];
+    unsigned long arg4 = regs->regs[4];
+
+    f2fs_printk_wrapper(sbi, fmt, arg3, arg4);
+#endif
+    return 0;
+}
+
+static struct kprobe oplus_f2fs_printk_kp = {
+    .symbol_name = "f2fs_printk",
+    .pre_handler = handler_f2fslog_storage_pre,
+};
+
+static int __init oplus_f2fshook_init(void) {
+    int ret;
+    printk("oplus_f2fshook_init\n");
+    ret = register_kprobe(&oplus_f2fs_printk_kp);
+    if (ret < 0) {
+        printk(" register_kprobe f2fs kprobe_register_kp failed, return %d\n", ret);
+        return ret;
+    }
+
+    oplus_f2fslog_storage_procfs = proc_mkdir("f2fslog_storage", NULL);
+    if (!oplus_f2fslog_storage_procfs) {
+        printk(" Failed to create oplus_f2fs_debug procfs\n");
+        return -EFAULT;
+    }
+
+    oplus_f2fslog_level = proc_create("f2fslog_level", 0644, oplus_f2fslog_storage_procfs, &oplus_f2fsloglevel_proc_ops);
+    if (oplus_f2fslog_level == NULL) {
+        printk(" Failed to create storage_reliable procfs\n");
+        return -EFAULT;
+    }
+
+    return 0;
+}
+
+static void __exit oplus_f2fshook_exit(void)
+{
+    unregister_kprobe(&oplus_f2fs_printk_kp);
+    if(NULL == oplus_f2fslog_storage_procfs || NULL == oplus_f2fslog_level) {
+        printk(" oplus_f2fshook_register or oplus_f2fshook_unregister is NULL\n");
+        return;
+    }
+
+    remove_proc_entry("f2fslog_storage", oplus_f2fslog_storage_procfs);
+    remove_proc_entry("f2fslog_level", oplus_f2fslog_level);
+}
+
+module_init(oplus_f2fshook_init);
+module_exit(oplus_f2fshook_exit);
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/Kconfig b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/Kconfig
new file mode 100644
index 000000000..ea1ad010e
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/Kconfig
@@ -0,0 +1,7 @@
+
+config OPLUS_FEATURE_OPLUS_UPROBE
+    tristate "oplus uprobe"
+    default n
+    help
+      define this config to enable oplus uprobe feature.
+
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/Makefile b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/Makefile
new file mode 100644
index 000000000..67a0780f2
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/Makefile
@@ -0,0 +1,7 @@
+# SPDX-License-Identifier: GPL-2.0-only
+# Copyright (C) 2022-2030 Oplus. All rights reserved.
+
+GCOV_PROFILE := y
+LINUXINCLUDE += -I$(srctree)/
+obj-$(CONFIG_OPLUS_FEATURE_OPLUS_UPROBE) += oplus_uprobe.o
+
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/oplus_uprobe.c b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/oplus_uprobe.c
new file mode 100644
index 000000000..de9b4f4dc
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/oplus_uprobe.c
@@ -0,0 +1,1296 @@
+#include <linux/module.h>
+#include <linux/ptrace.h>
+#include <linux/uprobes.h>
+#include <linux/namei.h>
+#include <linux/moduleparam.h>
+#include <linux/kthread.h>
+#include <linux/rtc.h>
+#include <linux/proc_fs.h>
+#include <linux/mutex.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/suspend.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/debugfs.h>
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/string.h>
+#include <linux/rwsem.h>
+#include <linux/timer.h>
+#include <linux/ctype.h>
+#include <linux/kstrtox.h>
+#include <linux/slab.h>
+#include <linux/reboot.h>
+#include <linux/signal.h>
+#include <linux/kprobes.h>
+#include "kernel/trace/trace_probe.h"
+#define OPLUS_UPROBE_LOG_TAG "[oplus_uprobe]"
+
+#define WRITE_BUFSIZE  4096
+#define MAX_COMM_LENGTH 256
+#define WHITELIST_SIZE 48
+#define MAX_UPROBE_COUNT 47
+#define REG_VAL_MAX 64
+#define SIGVOLD_GET_LOG (SIGRTMIN + 0x13)
+#define SIGOLC_GET_LOG (SIGRTMIN + 0x14)
+#define SIGCONTROL_RUN_TIME (SIGRTMIN + 0x15)
+#define oplus_procedure_link_pointer(regs)	((regs)->regs[30])
+
+extern int pr_storage(const char *fmt, ...);
+static LIST_HEAD(uprobe_event_list);
+static struct proc_dir_entry *reliable_procfs;
+static struct proc_dir_entry *storage_reliable_procfs;
+struct proc_dir_entry *proc_fs_uprobe;
+struct proc_dir_entry *proc_fs_uprobe_enable;
+static int oplus_uprobe_enable = 0;
+static atomic_t uprobe_count = ATOMIC_INIT(0);
+typedef int (*uprobe_register_t)(struct inode *, loff_t, struct uprobe_consumer *);
+typedef void (*uprobe_unregister_t)(struct inode *, loff_t , struct uprobe_consumer *);
+uprobe_register_t uprobe_register_funcptr = NULL;
+uprobe_unregister_t uprobe_unregister_funcptr = NULL;
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0))
+typedef int (*kern_path_t)(const char *name, unsigned int flags, struct path *path);
+typedef void (*path_put_t)(const struct path *path);
+kern_path_t kern_path_funcptr = NULL;
+path_put_t path_put_funcptr = NULL;
+#endif
+DECLARE_RWSEM(oplus_event_sem);
+
+struct ou_param {
+	struct timer_list       timer;
+	unsigned int            timeout;
+	unsigned int            nr_args;
+	struct probe_arg        args[];
+};
+
+enum OPLUS_ACTION {
+	A_PANIC,
+	A_LOG,
+	A_OLC,
+	A_RUNTIME,
+	A_RESET_MEDIA,
+	A_RESET_SYSTEM,
+	A_REBOOT,
+	A_KILL,
+	A_NONE,
+};
+
+struct oplus_uprobe {
+	struct list_head        head;
+	struct uprobe_consumer  consumer;
+	struct path             path;
+	struct inode            *inode;
+	char                    *filename;
+	unsigned int            userid;
+	unsigned int            runtime;
+	unsigned long           offset;
+	int                     action;
+	char*                   raw_cmd;
+	struct ou_param         param;
+};
+
+struct oplus_pt_regs_offset {
+	const char *name;
+	int offset;
+};
+
+#define REG_OFFSET_NAME(r) {.name = #r, .offset = offsetof(struct pt_regs, r)}
+#define REG_OFFSET_END {.name = NULL, .offset = 0}
+#define GPR_OFFSET_NAME(r) \
+	{.name = "x" #r, .offset = offsetof(struct pt_regs, regs[r])}
+
+static const struct oplus_pt_regs_offset oplus_regoffset_table[] = {
+	GPR_OFFSET_NAME(0),
+	GPR_OFFSET_NAME(1),
+	GPR_OFFSET_NAME(2),
+	GPR_OFFSET_NAME(3),
+	GPR_OFFSET_NAME(4),
+	GPR_OFFSET_NAME(5),
+	GPR_OFFSET_NAME(6),
+	GPR_OFFSET_NAME(7),
+	GPR_OFFSET_NAME(8),
+	GPR_OFFSET_NAME(9),
+	GPR_OFFSET_NAME(10),
+	GPR_OFFSET_NAME(11),
+	GPR_OFFSET_NAME(12),
+	GPR_OFFSET_NAME(13),
+	GPR_OFFSET_NAME(14),
+	GPR_OFFSET_NAME(15),
+	GPR_OFFSET_NAME(16),
+	GPR_OFFSET_NAME(17),
+	GPR_OFFSET_NAME(18),
+	GPR_OFFSET_NAME(19),
+	GPR_OFFSET_NAME(20),
+	GPR_OFFSET_NAME(21),
+	GPR_OFFSET_NAME(22),
+	GPR_OFFSET_NAME(23),
+	GPR_OFFSET_NAME(24),
+	GPR_OFFSET_NAME(25),
+	GPR_OFFSET_NAME(26),
+	GPR_OFFSET_NAME(27),
+	GPR_OFFSET_NAME(28),
+	GPR_OFFSET_NAME(29),
+	GPR_OFFSET_NAME(30),
+	{.name = "lr", .offset = offsetof(struct pt_regs, regs[30])},
+	REG_OFFSET_NAME(sp),
+	REG_OFFSET_NAME(pc),
+	REG_OFFSET_NAME(pstate),
+	REG_OFFSET_END,
+};
+
+static char ou_whitelist[WHITELIST_SIZE][MAX_COMM_LENGTH] = {
+	"/system/bin/vold",
+	"/system/bin/vold_prepare_subdirs",
+	"/system/bin/ueventd",
+	"/system/bin/init",
+	"/system/bin/rm",
+	"/system/bin/cp",
+	"/system/bin/mkdir",
+	"/system/bin/rmdir",
+	"/system/bin/chown",
+	"/system/bin/chgrp",
+};
+
+
+static int is_event_in_whitelist(const char *event)
+{
+	int i;
+
+	for (i = 0; i < WHITELIST_SIZE; i++) {
+		if (strcmp(event, ou_whitelist[i]) == 0) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "is_event_in_whitelist true\n");
+			return 0;
+		}
+	}
+
+	return -1;
+}
+
+struct task_struct *get_task_struct_by_comm(const char *comm) {
+	struct task_struct *task;
+
+	for_each_process(task) {
+		if (strcmp(task->comm, comm) == 0) {
+			return task;
+		}
+	}
+
+	return NULL;
+}
+
+struct task_struct *get_media_task_struct(const char *comm, unsigned int userid) {
+	struct task_struct *task;
+
+	for_each_process(task) {
+		if (!strcmp(task->comm, comm) && (userid == (task->real_cred->uid.val/100000))) {
+			return task;
+		}
+	}
+
+	return NULL;
+}
+
+static int process_action_insn(int action, unsigned int	userid, unsigned int runtime)
+{
+	struct task_struct *task;
+	kernel_siginfo_t si;
+
+	switch (action) {
+	case A_PANIC:
+		panic("oplus uprobe trriger panic!");
+		break;
+	case A_LOG:
+		task = get_task_struct_by_comm("Binder:vold");
+		if (!task) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			pr_storage(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			break;
+		}
+		send_sig_info(SIGVOLD_GET_LOG, SEND_SIG_PRIV, task);
+		break;
+	case A_OLC:
+		task = get_task_struct_by_comm("Binder:vold");
+		if (!task) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			pr_storage(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			break;
+		}
+		send_sig_info(SIGOLC_GET_LOG, SEND_SIG_PRIV, task);
+		break;
+	case A_RUNTIME:
+		clear_siginfo(&si);
+		si.si_signo = SIGCONTROL_RUN_TIME;
+		si.si_errno = 0;
+		si.si_code  = -1;
+		si.si_int = runtime;
+		task = get_task_struct(current);
+		if (!task) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			pr_storage(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			break;
+		}
+		send_sig_info(SIGCONTROL_RUN_TIME, &si, task);
+		put_task_struct(current);
+		break;
+	case A_RESET_MEDIA:
+		task = get_media_task_struct("rs.media.module", userid);
+		if (!task) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			pr_storage(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			break;
+		}
+		send_sig_info(SIGKILL, SEND_SIG_PRIV, task);
+		break;
+	case A_RESET_SYSTEM:
+		task = get_task_struct_by_comm("system_server");
+		if (!task) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			pr_storage(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			break;
+		}
+		send_sig_info(SIGKILL, SEND_SIG_PRIV, task);
+		break;
+	case A_REBOOT:
+		kernel_restart(NULL);
+		break;
+	case A_NONE:
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static void uprobe_timed_out_timer(struct timer_list *t)
+{
+	struct ou_param *op = container_of(t, struct ou_param, timer);
+	struct oplus_uprobe *ou = container_of(op, struct oplus_uprobe, param);
+
+	pr_storage(OPLUS_UPROBE_LOG_TAG "uprobe_timed_out_timer\n");
+	if (!ou) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "get uprobe object failed\n");
+		return;
+	}
+	if (ou->action != A_REBOOT) {
+		process_action_insn(ou->action, ou->userid, ou->runtime);
+	}
+}
+
+static int timer_handler_pre(struct uprobe_consumer *self, struct pt_regs *regs)
+{
+	struct oplus_uprobe *ou = container_of(self, struct oplus_uprobe, consumer);
+
+	timer_setup(&ou->param.timer, uprobe_timed_out_timer, 0);
+	pr_storage(OPLUS_UPROBE_LOG_TAG "timer_handler_pre\n");
+	mod_timer(&ou->param.timer, jiffies + msecs_to_jiffies(ou->param.timeout));
+    return 0;
+}
+
+static int timer_handler_ret(struct uprobe_consumer *self,
+                                unsigned long func,
+                                struct pt_regs *regs)
+{
+	struct oplus_uprobe *ou = container_of(self, struct oplus_uprobe, consumer);
+
+	pr_storage(OPLUS_UPROBE_LOG_TAG "oplus_uprobe handler_ret\n");
+	del_timer_sync(&ou->param.timer);
+	return 0;
+}
+
+static inline void oplus_regs_set_return_value(struct pt_regs *regs, unsigned long rc)
+{
+  	regs->regs[0] = rc;
+}
+
+static inline void oplus_instruction_pointer_set(struct pt_regs *regs,
+  		unsigned long val)
+{
+  	regs->pc = val;
+}
+
+static int oplus_regs_query_register_offset(const char *name)
+{
+	const struct oplus_pt_regs_offset *roff;
+
+	for (roff = oplus_regoffset_table; roff->name != NULL; roff++)
+		if (!strcmp(roff->name, name))
+			return roff->offset;
+	return -EINVAL;
+}
+
+static int
+process_fetch_insn(struct fetch_insn *code, void *rec, void *dest,
+		   void *base,int action, unsigned int userid, unsigned int runtime)
+{
+	struct pt_regs *regs = rec;
+	unsigned long long val;
+	unsigned long long comp_val;
+	unsigned long long set_val;
+	unsigned int offset;
+	char *reg;
+	char *reg_val;
+	int ret;
+	struct task_struct *task;
+	pid_t current_tgid, tgid;
+	char *reg_string;
+
+	switch (code->op) {
+	case FETCH_OP_REG:
+		reg_string = kmalloc(REG_VAL_MAX, GFP_KERNEL);
+		strncpy(reg_string, (char*)code->data, REG_VAL_MAX);
+		reg = strsep(&reg_string, "=");
+		if (!reg) {
+			kfree(reg_string);
+			pr_err(OPLUS_UPROBE_LOG_TAG "parse register reg failed\n");
+			return -EINVAL;
+		}
+		reg_val = strsep(&reg_string, "=");
+		if (!reg_val) {
+			kfree(reg_string);
+			pr_err(OPLUS_UPROBE_LOG_TAG "parse register reg_val failed\n");
+			return -EINVAL;
+		}
+		pr_storage(OPLUS_UPROBE_LOG_TAG"oplus_parse_arg reg(%s) regval(%s)\n", reg,  reg_val);
+		if (isdigit(reg_val[0])) {
+			ret = kstrtoull(reg_val, 0, &comp_val);
+			if (ret) {
+				kfree(reg_string);
+				pr_err(OPLUS_UPROBE_LOG_TAG "parse register kstrtoull failed\n");
+				return -EINVAL;
+			}
+		} else {
+			kfree(reg_string);
+			return -EINVAL;
+		}
+
+		ret = oplus_regs_query_register_offset(reg);
+		if (ret < 0) {
+			kfree(reg_string);
+			pr_err(OPLUS_UPROBE_LOG_TAG "parse register failed\n");
+			return -EINVAL;
+		}
+
+                offset = ret;
+		val = regs_get_register(regs, offset);
+		pr_storage(OPLUS_UPROBE_LOG_TAG "FETCH_OP_REG val(0x%llx) offset(%u) comp_val(0x%llx)\n", val, offset, comp_val);
+		if (comp_val == val) {
+			pr_storage(OPLUS_UPROBE_LOG_TAG "FETCH_OP_REG val(0x%llx) comp_val(0x%llx)\n", val, comp_val);
+			process_action_insn(action, userid, runtime);
+		}
+		kfree(reg_string);
+		break;
+	case FETCH_OP_MOD_BF:
+		reg_string = kmalloc(REG_VAL_MAX, GFP_KERNEL);
+		strncpy(reg_string, (char*)code->data, REG_VAL_MAX);
+		reg = strsep(&reg_string, "=");
+		if (!reg) {
+			kfree(reg_string);
+			pr_err(OPLUS_UPROBE_LOG_TAG "parse register reg failed\n");
+			return -EINVAL;
+		}
+		reg_val = strsep(&reg_string, "=");
+		if (!reg_val) {
+			kfree(reg_string);
+			pr_err(OPLUS_UPROBE_LOG_TAG "parse register reg_val failed\n");
+			return -EINVAL;
+		}
+		if (isdigit(reg_val[0])) {
+			ret = kstrtoull(reg_val, 0, &set_val);
+			if (ret) {
+				kfree(reg_string);
+				pr_err(OPLUS_UPROBE_LOG_TAG "parse register kstrtoull failed\n");
+				return -EINVAL;
+			}
+		} else {
+			kfree(reg_string);
+			return -EINVAL;
+		}
+
+		offset = (unsigned int)oplus_regs_query_register_offset(reg);
+		if (offset < 0) {
+			pr_storage(OPLUS_UPROBE_LOG_TAG"oplus_parse_arg 2 reg(%s) offset(%d) regval(0x%llx)\n", reg, offset, set_val);
+			pr_err(OPLUS_UPROBE_LOG_TAG "parse register failed\n");
+			kfree(reg_string);
+			return -EINVAL;
+		} else if (offset >= 0) {
+			offset >>= 3;
+			pt_regs_write_reg(regs, offset, (unsigned long)set_val);
+			pr_storage(OPLUS_UPROBE_LOG_TAG "FETCH_OP_MOD_BF offset(%d) set_val(0x%llx)\n", offset, set_val);
+		}
+
+		kfree(reg_string);
+		break;
+	case FETCH_OP_DATA:
+		oplus_regs_set_return_value(regs, code->offset);
+		oplus_instruction_pointer_set(regs, oplus_procedure_link_pointer(regs));
+		val = regs_return_value(regs);
+		pr_storage(OPLUS_UPROBE_LOG_TAG "FETCH_OP_DATA set ret val(%d) offset(%d)\n", val, code->offset);
+		process_action_insn(action, userid, runtime);
+		break;
+	case FETCH_OP_RETVAL:
+		val = regs_return_value(regs);
+		if(val == code->offset) {
+			pr_storage(OPLUS_UPROBE_LOG_TAG "FETCH_OP_RETVAL get ret val(%llu) offset(%d)\n", val, code->offset);
+			process_action_insn(action, userid, runtime);
+		}
+		break;
+	case FETCH_OP_COMM:
+		current_tgid = task_tgid_nr(current);
+		pr_storage(OPLUS_UPROBE_LOG_TAG "FETCH_OP_COMM current_tgid(%d) \n",current_tgid);
+		task = get_task_struct_by_comm(code->data);
+		if (!task) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "No task_struct found for process\n");
+			break;
+		}
+		tgid = task->tgid;
+		pr_storage(OPLUS_UPROBE_LOG_TAG "FETCH_OP_COMM tgid(%d) \n",tgid);
+		if (current_tgid == tgid) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "FETCH_OP_COMM\n");
+			pr_storage(OPLUS_UPROBE_LOG_TAG "FETCH_OP_COMM\n");
+			process_action_insn(action, userid, runtime);
+		}
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int none_handler_pre(struct uprobe_consumer *self, struct pt_regs *regs)
+{
+	struct oplus_uprobe *ou = container_of(self, struct oplus_uprobe, consumer);
+
+	process_action_insn(ou->action, ou->userid, ou->runtime);
+	return 0;
+
+}
+
+static int keyvalue_handler_pre(struct uprobe_consumer *self, struct pt_regs *regs)
+{
+	struct oplus_uprobe *ou = container_of(self, struct oplus_uprobe, consumer);
+	int argc = ou->param.nr_args;
+	int action = ou->action;
+	unsigned int userid = ou->userid;
+	unsigned int runtime = ou->runtime;
+	int i;
+	for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) {
+		struct probe_arg *parg = &ou->param.args[i];
+		if (parg->code->op != FETCH_OP_RETVAL)
+			process_fetch_insn(parg->code, regs, NULL, NULL, action, userid, runtime);
+	}
+	pr_storage(OPLUS_UPROBE_LOG_TAG "keyvalue_handler_pre end\n");
+
+	return 0;
+
+}
+
+static int keyvalue_handler_ret(struct uprobe_consumer *self, unsigned long func, struct pt_regs *regs)
+{
+	struct oplus_uprobe *ou = container_of(self, struct oplus_uprobe, consumer);
+	int argc = ou->param.nr_args;
+	int action = ou->action;
+	unsigned int userid = ou->userid;
+	unsigned int runtime = ou->runtime;
+	int i;
+	for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) {
+		struct probe_arg *parg = &ou->param.args[i];
+		if (parg->code->op == FETCH_OP_RETVAL)
+			process_fetch_insn(parg->code, regs, NULL, NULL, action, userid, runtime);
+	}
+	pr_storage(OPLUS_UPROBE_LOG_TAG "keyvalue_handler_ret end\n");
+
+	return 0;
+}
+
+static int parse_probe_vars(char *arg, const struct fetch_type *t,
+			struct fetch_insn *code, int offs, char *value, unsigned int flags)
+{
+	int retval;
+	unsigned long param;
+	int ret = 0;
+	int len;
+
+	if (!strcmp(arg, "retval") && (flags & TPARG_FL_RETURN)) {
+		code->op = FETCH_OP_RETVAL;
+		ret = kstrtoint(value, 0, &retval);
+		if (ret) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "parse get retvalue failed\n");
+			goto inval_var;
+		}
+		code->offset = retval;
+	} else if (!strcmp(arg, "setret")) {
+		pr_storage(OPLUS_UPROBE_LOG_TAG "setret\n");
+		if(oplus_uprobe_enable == 1) {
+			code->op = FETCH_OP_DATA;
+			ret = kstrtoint(value, 0, &retval);
+			if (ret) {
+				pr_err(OPLUS_UPROBE_LOG_TAG "parse set retvalue failed\n");
+				goto inval_var;
+			}
+			code->offset = retval;
+			return ret;
+		}
+		return -EINVAL;
+	} else if ((len = str_has_prefix(arg, "stack"))) {
+		if (arg[len] == '\0') {
+			code->op = FETCH_OP_STACKP;
+		} else if (isdigit(arg[len])) {
+			ret = kstrtoul(arg + len, 10, &param);
+			if (ret) {
+				goto inval_var;
+			} else {
+				code->op = FETCH_OP_STACK;
+				code->param = (unsigned int)param;
+			}
+		} else
+			goto inval_var;
+	} else if (strcmp(arg, "comm") == 0) {
+		code->op = FETCH_OP_COMM;
+		code->data = kstrdup(value, GFP_KERNEL);
+		if (!code->data)
+			return -ENOMEM;
+	} else
+		goto inval_var;
+
+	return ret;
+
+inval_var:
+	return -EINVAL;
+}
+
+static int parse_probe_reg(char *arg, struct fetch_insn *code)
+{
+	char *value;
+	int ret = 0;
+
+	if (strstr(arg, "setreg")) {
+		pr_storage(OPLUS_UPROBE_LOG_TAG "parse register setreg\n");
+		if(oplus_uprobe_enable == 1) {
+			value = strrchr(arg, ':');
+			if (!value) {
+				pr_err(OPLUS_UPROBE_LOG_TAG "parse register value failed\n");
+				return -EINVAL;
+			}
+			*value++ = '\0';
+			code->op = FETCH_OP_MOD_BF;
+			code->data = kstrdup(value, GFP_KERNEL);
+			if (!code->data)
+				return -ENOMEM;
+			return ret;
+		}
+		return -EINVAL;
+	} else if (strstr(arg, "getreg")) {
+		pr_storage(OPLUS_UPROBE_LOG_TAG "parse register getreg\n");
+		value = strrchr(arg, ':');
+		if (!value) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "parse register reg failed\n");
+			return -EINVAL;
+		}
+		*value++ = '\0';
+		code->op = FETCH_OP_REG;
+		code->data = kstrdup(value, GFP_KERNEL);
+		if (!code->data )
+			return -ENOMEM;
+	} else {
+			pr_err(OPLUS_UPROBE_LOG_TAG "parse register ending failed arg(%s)\n",arg);
+			return -EINVAL;
+	}
+
+	return ret;
+}
+static int oplus_parse_arg(char *arg, const struct fetch_type *type,
+								struct fetch_insn **pcode, struct fetch_insn *end,
+								unsigned int flags, int offs, bool is_return)
+{
+	struct fetch_insn *code = *pcode;
+	unsigned long param;
+	long offset = 0;
+	int ret = 0;
+	char *value;
+
+	switch (arg[0]) {
+	case '#':
+		value = strrchr(arg + 1, '=');
+		if (!value) {
+			return -EINVAL;
+		}
+		*value++ = '\0';
+		ret = parse_probe_vars(arg + 1, type, code, offs, value, is_return ? TPARG_FL_RETURN : 0);
+		break;
+
+	case '%':
+		ret = parse_probe_reg(arg + 1, code);
+		break;
+
+	case '@':
+		pr_storage(OPLUS_UPROBE_LOG_TAG"oplus_parse_arg 3 arg(%s)\n", arg + 1);
+		if (isdigit(arg[1])) {
+			ret = kstrtoul(arg + 1, 0, &param);
+			if (ret) {
+				pr_err(OPLUS_UPROBE_LOG_TAG "parse memory-offset failed\n");
+				break;
+			}
+			code->op = FETCH_OP_IMM;
+			code->immediate = param;
+		} else if (arg[1] == '+') {
+			if (flags & TPARG_FL_KERNEL) {
+				pr_err(OPLUS_UPROBE_LOG_TAG "parse memory-offset failed\n");
+				return -EINVAL;
+			}
+			ret = kstrtol(arg + 2, 0, &offset);
+			if (ret) {
+				pr_err(OPLUS_UPROBE_LOG_TAG "parse memory-offset failed\n");
+				break;
+			}
+
+			code->op = FETCH_OP_FOFFS;
+			code->immediate = (unsigned long)offset;  // imm64?
+		} else {
+			code->op = FETCH_NOP_SYMBOL;
+			code->data = kstrdup(arg + 1, GFP_KERNEL);
+			if (!code->data)
+				return -ENOMEM;
+			if (++code == end) {
+				pr_err(OPLUS_UPROBE_LOG_TAG "too many options\n");
+				return -EINVAL;
+			}
+			code->op = FETCH_OP_IMM;
+			code->immediate = 0;
+		}
+		if (++code == end) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "too many options\n");
+			return -EINVAL;
+		}
+
+		*pcode = code;
+		code->op = FETCH_OP_DEREF;
+		code->offset = offset;
+		break;
+	default:
+		return -EINVAL;
+
+	}
+	if (ret) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "BAD_FETCH_ARG\n");
+		ret = -EINVAL;
+	}
+	return ret;
+}
+
+int parse_keyvalue_body_arg(struct ou_param *op, int i, char *argv, bool is_return)
+{
+	struct probe_arg *parg = &op->args[i];
+	int ret = 0;
+	char *arg;
+	unsigned int flags = TPARG_FL_KERNEL;
+	arg = strchr(argv, ':');
+	if (!arg) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "key_value param is invalid, find no:\n");
+		goto err;
+	}
+	*arg++ = '\0';
+	pr_storage(OPLUS_UPROBE_LOG_TAG "parse arg(%s)\n", arg);
+	parg->code = kzalloc(sizeof(*parg->code), GFP_KERNEL);
+	if (!parg->code) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "kcalloc failed\n");
+		goto err;
+	}
+
+	ret = oplus_parse_arg(arg, parg->type, &parg->code, &parg->code[FETCH_INSN_MAX - 1], flags, 0, is_return);
+	if (ret) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "parse arg failed\n");
+		goto err;
+	}
+
+	return 0;
+
+err:
+	if(parg->code)
+		kfree(parg->code);
+	return -EINVAL;
+}
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0))
+static int __nocfi kern_path_fn(const char *name, unsigned int flags, struct path *path)
+{
+	int ret = 0;
+
+	ret = kern_path_funcptr(name, flags, path);
+
+	return ret;
+}
+
+static void __nocfi path_put_fn(const struct path *path)
+{
+	path_put_funcptr(path);
+}
+#endif
+static struct oplus_uprobe* parse_uprobe_cmd(int argc, char **argv)
+{
+	char *arg;
+	struct path path;
+	char *event_name = "uprobe";
+	char *filename;
+	char *action;
+	unsigned long offset;
+	struct oplus_uprobe *ou = NULL;
+	int ret;
+	int i;
+	unsigned long timeout;
+	unsigned long userid;
+	unsigned long runtime;
+	bool is_return = false;
+	int type;
+
+	if (argc < 4) {
+		pr_err(OPLUS_UPROBE_LOG_TAG"argc %d is not invalid\n", argc);
+        return NULL;
+	}
+
+	type = argv[0][0];
+	switch (type) {
+	case 'r':
+		is_return = true;
+		break;
+	case 'p':
+		break;
+	default:
+		pr_err(OPLUS_UPROBE_LOG_TAG "cmd type(%c) is not supported\n", type);
+		return NULL;
+	}
+	if (argv[0][1] == ':') {
+		event_name = &argv[0][2];
+	}
+
+	filename = kstrdup(argv[1], GFP_KERNEL);
+	if (!filename)
+		return NULL;
+
+	arg = strrchr(filename, ':');
+	if (!arg || !isdigit(arg[1])) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "filename %s is not invalid\n", filename);
+		goto err;
+	}
+	*arg++ = '\0';
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0))
+	ret = kern_path_fn(filename, LOOKUP_FOLLOW, &path);
+#else
+	ret = kern_path(filename, LOOKUP_FOLLOW, &path);
+#endif
+	if (ret < 0) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "kern_path failed, ret %d, filename %s\n", ret, filename);
+		goto err;
+	}
+	if (is_event_in_whitelist(filename)) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "is_event_in_whitelist false\n");
+		goto err;
+	}
+	ret = kstrtoul(arg, 0, &offset);
+	if (ret) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "offset param is invalid, %s\n", arg);
+		goto err;
+	}
+	argc -= 3;
+	pr_storage(OPLUS_UPROBE_LOG_TAG "filename(%s), offset(0x%lx) argc(%d)\n", filename, offset, argc);
+	ou = kzalloc(struct_size(ou, param.args, argc), GFP_KERNEL);
+	if (!ou) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "alloc ou failed\n");
+		goto err;
+	}
+	argv += 2;
+	pr_storage(OPLUS_UPROBE_LOG_TAG "argv[0](%s)\n", argv[0]);
+	if (0 == strncmp(argv[0], "timer", strlen("timer"))) {
+		arg = strrchr(argv[0], ':');
+		if (!arg || !isdigit(arg[1])) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "argv[0] param is invalid, %s\n", argv[0]);
+			goto err;
+		}
+
+		*arg++ = '\0';
+		ret = kstrtoul(arg, 0, &timeout);
+		if (ret) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "timer timeout param is invalid (%s)\n", arg);
+			goto err;
+		}
+		ou->param.timeout = timeout;
+		ou->consumer.handler = timer_handler_pre;
+		ou->consumer.ret_handler = timer_handler_ret;
+	} else if (0 == strncmp(argv[0], "key_value", strlen("key_value"))) {
+		//argv += 2;
+		pr_storage(OPLUS_UPROBE_LOG_TAG "key_value argc(%d)\n", argc);
+		for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) {
+			ret = parse_keyvalue_body_arg(&ou->param, i, argv[i], is_return);
+			if (ret)
+				goto err;
+		}
+		ou->param.nr_args = argc;
+		ou->consumer.handler = keyvalue_handler_pre;
+		if (is_return)
+			ou->consumer.ret_handler = keyvalue_handler_ret;
+	} else if (0 == strncmp(argv[0], "none", strlen("none"))) {
+		ou->consumer.handler = none_handler_pre;
+	} else {
+		pr_err(OPLUS_UPROBE_LOG_TAG " invalid argv[2] (%s)\n", argv[2]);
+		goto err;
+	}
+
+	action = argv[argc];
+	if (!action) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "action is NULL \n");
+		goto err;
+	}
+	pr_storage(OPLUS_UPROBE_LOG_TAG "action(%s)\n", action);
+	if (0 == strncmp(action, "panic", strlen("panic"))) {
+		ou->action = A_PANIC;
+	} else if(0 == strncmp(action, "vlog", strlen("vlog"))) {
+		ou->action = A_LOG;
+	} else if(0 == strncmp(action, "rtime", strlen("rtime"))) {
+		arg = strrchr(action, ':');
+		if (!arg || !isdigit(arg[1])) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "action is invalid, %s\n", action);
+			goto err;
+		}
+
+		*arg++ = '\0';
+		ret = kstrtoul(arg, 0, &runtime);
+		if (ret) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "timer userid is invalid (%s)\n", arg);
+			goto err;
+		}
+		ou->runtime = runtime;
+		ou->action = A_RUNTIME;
+	} else if(0 == strncmp(action, "olog", strlen("olog"))) {
+		ou->action = A_OLC;
+	} else if(0 == strncmp(action, "rmedia", strlen("rmedia"))) {
+		arg = strrchr(action, ':');
+		if (!arg || !isdigit(arg[1])) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "action is invalid, %s\n", action);
+			goto err;
+		}
+
+		*arg++ = '\0';
+		ret = kstrtoul(arg, 0, &userid);
+		if (ret) {
+			pr_err(OPLUS_UPROBE_LOG_TAG "timer userid is invalid (%s)\n", arg);
+			goto err;
+		}
+		ou->userid = userid;
+		ou->action = A_RESET_MEDIA;
+	} else if(0 == strncmp(action, "rsystem", strlen("rsystem"))) {
+		ou->action = A_RESET_SYSTEM;
+	} else if(0 == strncmp(action, "reboot", strlen("reboot"))) {
+		ou->action = A_REBOOT;
+	} else if(0 == strncmp(action, "none", strlen("none"))) {
+		ou->action = A_NONE;
+	} else {
+		pr_storage(OPLUS_UPROBE_LOG_TAG "invalid action\n");
+		pr_err(OPLUS_UPROBE_LOG_TAG "invalid action\n");
+		goto err;
+	}
+	ou->offset = offset;
+	ou->path = path;
+	ou->filename = filename;
+	return ou;
+
+err:
+	if (ou)
+		kfree(ou);
+
+	if (filename)
+		kfree(filename);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0))
+	if (path.mnt && path.dentry)
+		path_put_fn(&path);
+#else
+	if (path.mnt && path.dentry)
+		path_put(&path);
+#endif
+
+	return NULL;
+}
+
+void ou_param_cleanup(struct probe_arg *arg)
+{
+	struct fetch_insn *code = arg->code;
+
+	if (code->op == FETCH_OP_COMM) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "ou_param_cleanup code->data \n");
+		kfree(code->data);
+	}
+
+	kfree(arg->code);
+}
+
+int __nocfi uprobe_register_fn(struct inode *inode, unsigned long offset, struct uprobe_consumer *consumer)
+{
+	int ret = 0;
+
+	ret = uprobe_register_funcptr(inode, offset, consumer);
+
+	return ret;
+}
+
+void __nocfi uprobe_unregister_fn(struct inode *inode, unsigned long offset, struct uprobe_consumer *consumer)
+{
+	uprobe_unregister_funcptr(inode, offset, consumer);
+}
+
+static void delete_uprobe_cmd(int argc, char **argv)
+{
+	int ret;
+	char *arg;
+	char *filename;
+	unsigned long offset;
+	struct oplus_uprobe *comp;
+	struct path path;
+	struct inode *comp_inode;
+	int i;
+
+	if (argc < 2) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "argc %d is not invalid\n", argc);
+		return;
+	}
+
+	filename = argv[1];
+	if (!strchr(filename, '/')) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "filename %s is not invalid\n", filename);
+		return;
+	}
+
+	arg = strrchr(filename, ':');
+	if (!arg || !isdigit(arg[1])) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "filename %s is not invalid\n", filename);
+		return;
+	}
+	*arg++ = '\0';
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0))
+	ret = kern_path_fn(filename, LOOKUP_FOLLOW, &path);
+#else
+	ret = kern_path(filename, LOOKUP_FOLLOW, &path);
+#endif
+	if (ret < 0) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "kern_path failed, ret %d, filename %s\n", ret, filename);
+		return;
+	}
+
+	ret = kstrtoul(arg, 0, &offset);
+	if (ret) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "offset param is invalid, %s\n", arg);
+		return;
+	}
+	down_write(&oplus_event_sem);
+	list_for_each_entry(comp, &uprobe_event_list, head) {
+		comp_inode = d_real_inode(comp->path.dentry);
+		if (comp_inode == d_real_inode(path.dentry) && (comp->offset == offset)) {
+			list_del(&comp->head);
+			uprobe_unregister_fn(comp->inode, comp->offset, &comp->consumer);
+			comp->inode = NULL;
+			for (i = 0; i < comp->param.nr_args && i < MAX_TRACE_ARGS; i++) {
+				ou_param_cleanup(&comp->param.args[i]);
+			}
+			kfree(comp->filename);
+			kfree(comp->raw_cmd);
+			kfree(comp);
+			atomic_dec(&uprobe_count);
+			pr_storage(OPLUS_UPROBE_LOG_TAG "ou_param_cleanup done \n");
+			break;
+		}
+	}
+	up_write(&oplus_event_sem);
+}
+
+static int oplus_uprobe_register(struct oplus_uprobe *ou)
+{
+	int ret;
+
+	ou->inode = d_real_inode(ou->path.dentry);
+	pr_err(OPLUS_UPROBE_LOG_TAG "oplus_uprobe_register\n");
+	ret = uprobe_register_fn(ou->inode, ou->offset, &ou->consumer);
+	if (ret)
+		ou->inode = NULL;
+
+	return ret;
+}
+
+static ssize_t oplus_uprobe_proc_write(struct file *file, const char __user *buffer,
+			    size_t count, loff_t *ppos)
+{
+	char *kbuf = NULL;
+	int argc = 0;
+	char **argv = NULL;
+	struct oplus_uprobe *ou = NULL;
+	struct oplus_uprobe *comp;
+	struct inode *comp_inode;
+	int ret;
+
+    if (count > WRITE_BUFSIZE) {
+        return -EINVAL;
+	}
+	kbuf = kmalloc(WRITE_BUFSIZE, GFP_KERNEL);
+	if (!kbuf) {
+		return -ENOMEM;
+	}
+
+	if (copy_from_user(kbuf, buffer, count)) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "copy data from user buffer failed\n");
+		goto err;
+	}
+
+	kbuf[count] = '\0';
+
+	argv = argv_split(GFP_KERNEL, kbuf, &argc);
+	if (!argv) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "argv_split fail\n");
+		goto err;
+	}
+
+	if (argv[0][0] == '-') {
+		delete_uprobe_cmd(argc, argv);
+		goto out;
+	}
+
+    if (atomic_read(&uprobe_count) > MAX_UPROBE_COUNT) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "uprobe_count Maximum limit\n");
+		pr_storage(OPLUS_UPROBE_LOG_TAG "uprobe_count Maximum limit uprobe_count(%d)\n", atomic_read(&uprobe_count));
+		goto err;
+	}
+
+	ou = parse_uprobe_cmd(argc, argv);
+	if (!ou) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "parse_uprobe_cmd fail\n");
+		goto err;
+	}
+	ou->raw_cmd = kbuf;
+	INIT_LIST_HEAD(&ou->head);
+
+	down_write(&oplus_event_sem);
+	list_for_each_entry(comp, &uprobe_event_list, head) {
+		comp_inode = d_real_inode(comp->path.dentry);
+		if (comp_inode == d_real_inode(ou->path.dentry) && (comp->offset == ou->offset)) {
+			up_write(&oplus_event_sem);
+			pr_storage(OPLUS_UPROBE_LOG_TAG "the same event exist, filename %s, offset 0x%lx\n", comp->filename, comp->offset);
+			goto err;
+		}
+	}
+	list_add_tail(&ou->head, &uprobe_event_list);
+	up_write(&oplus_event_sem);
+
+	ret = oplus_uprobe_register(ou);
+	if (ret) {
+		kfree(ou->filename);
+		//uprobe_unregister(ou->inode, ou->offset, &ou->consumer);
+		uprobe_unregister_fn(ou->inode, ou->offset, &ou->consumer);
+		down_write(&oplus_event_sem);
+		list_del(&ou->head);
+		up_write(&oplus_event_sem);
+		pr_err(OPLUS_UPROBE_LOG_TAG "create_uprobe_cmd failed, ret %d\n", ret);
+		goto err;
+	}
+	atomic_inc(&uprobe_count);
+out:
+	argv_free(argv);
+	pr_storage(OPLUS_UPROBE_LOG_TAG "proc write succeed\n");
+	return count;
+
+err:
+	if (kbuf) {
+		kfree(kbuf);
+	}
+
+	if (argv) {
+		argv_free(argv);
+	}
+
+	if (ou) {
+		kfree(ou);
+	}
+
+	pr_storage(OPLUS_UPROBE_LOG_TAG "proc write failed\n");
+	return -1;
+}
+
+static int oplus_uprobe_proc_show(struct seq_file *m, void *v)
+{
+	struct oplus_uprobe* tmp;
+
+	down_write(&oplus_event_sem);
+	list_for_each_entry(tmp, &uprobe_event_list, head) {
+		seq_printf(m, "%s\n", tmp->raw_cmd);
+		pr_storage("%s\n", tmp->raw_cmd);
+	}
+	up_write(&oplus_event_sem);
+
+	return 0;
+}
+
+static int oplus_uprobe_proc_open(struct inode *inode, struct file *file)
+{
+    return single_open(file, oplus_uprobe_proc_show, inode->i_private);
+}
+
+static struct proc_ops oplus_uprobe_proc_ops = {
+	.proc_open			= oplus_uprobe_proc_open,
+	.proc_read			= seq_read,
+	.proc_write			= oplus_uprobe_proc_write,
+	.proc_release			= single_release,
+	.proc_lseek			= default_llseek,
+};
+
+static ssize_t uprobe_enable_proc_write(struct file *file, const char __user *buf,
+		size_t count, loff_t *off)
+{
+	char str[3] = {0};
+
+	if (count > 2 || count < 1) {
+		return -EINVAL;
+	}
+
+	if (copy_from_user(str, buf, count)) {
+		pr_err(OPLUS_UPROBE_LOG_TAG "copy_from_user failed\n");
+		return -EFAULT;
+	}
+
+	if (unlikely(!strncmp(str, "1", 1))) {
+		pr_info(OPLUS_UPROBE_LOG_TAG "oplus uprobe enable\n");
+		oplus_uprobe_enable = 1;
+	} else {
+		pr_info(OPLUS_UPROBE_LOG_TAG "oplus uprobe disabled\n");
+		oplus_uprobe_enable = 0;
+	}
+
+	return (ssize_t)count;
+}
+
+static int uprobe_enable_show(struct seq_file *s, void *data)
+{
+	if (oplus_uprobe_enable == 1)
+		seq_printf(s, "%d\n", 1);
+	else
+		seq_printf(s, "%d\n", 0);
+
+	return 0;
+}
+
+static int uprobe_enable_proc_open(struct inode *inodp, struct file *filp)
+{
+	return single_open(filp, uprobe_enable_show, inodp);
+}
+
+static struct proc_ops uprobe_enable_fops = {
+	.proc_open		= uprobe_enable_proc_open,
+	.proc_read		= seq_read,
+	.proc_write		= uprobe_enable_proc_write,
+	.proc_release		= single_release,
+	.proc_lseek		= default_llseek,
+};
+
+static struct kprobe uprobe_register_kp = {
+	.symbol_name = "uprobe_register"
+};
+static struct kprobe uprobe_unregister_kp = {
+	.symbol_name = "uprobe_unregister"
+};
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0))
+static struct kprobe kern_path_kp = {
+	.symbol_name = "kern_path"
+};
+static struct kprobe path_put_kp = {
+	.symbol_name = "path_put"
+};
+#endif
+static int __init oplus_uprobe_init(void) {
+
+	int ret;
+	ret = register_kprobe(&uprobe_register_kp);
+	if (ret < 0) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" register_kprobe uprobe_register_kp failed, return %d\n", ret);
+		return ret;
+	} else {
+		uprobe_register_funcptr = (uprobe_register_t)uprobe_register_kp.addr;
+		unregister_kprobe(&uprobe_register_kp);
+		pr_err(OPLUS_UPROBE_LOG_TAG" uprobe_register func addr:0x%lx\n", (unsigned long)uprobe_register_funcptr);
+	}
+
+	ret = register_kprobe(&uprobe_unregister_kp);
+	if (ret < 0) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" register_kprobe uprobe_unregister_kp failed, return %d\n", ret);
+		return ret;
+	} else {
+		uprobe_unregister_funcptr = (uprobe_unregister_t)uprobe_unregister_kp.addr;
+		unregister_kprobe(&uprobe_unregister_kp);
+		pr_err(OPLUS_UPROBE_LOG_TAG" uprobe_unregister func addr:0x%lx\n", (unsigned long)uprobe_unregister_funcptr);
+	}
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0))
+	ret = register_kprobe(&kern_path_kp);
+	if (ret < 0) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" register_kprobe kern_path_kp failed, return %d\n", ret);
+		return ret;
+	} else {
+		kern_path_funcptr = (kern_path_t)kern_path_kp.addr;
+		unregister_kprobe(&kern_path_kp);
+		//pr_err(OPLUS_UPROBE_LOG_TAG" uprobe_register func addr:0x%lx\n", (unsigned long)kern_path_kp);
+	}
+
+	ret = register_kprobe(&path_put_kp);
+	if (ret < 0) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" register_kprobe path_put_kp failed, return %d\n", ret);
+		return ret;
+	} else {
+		path_put_funcptr = (path_put_t)uprobe_unregister_kp.addr;
+		unregister_kprobe(&path_put_kp);
+		//pr_err(OPLUS_UPROBE_LOG_TAG" uprobe_unregister func addr:0x%lx\n",(unsigned long) path_put_kp);
+	}
+#endif
+	if(NULL == uprobe_register_funcptr || NULL == uprobe_unregister_funcptr) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" uprobe_register or uprobe_unregister is NULL\n");
+		return -EFAULT;
+	}
+
+	reliable_procfs = proc_mkdir("oplus_reliable", NULL);
+	if (!reliable_procfs) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" Failed to create oplus_reliable procfs\n");
+		return -EFAULT;
+	}
+
+	storage_reliable_procfs = proc_mkdir("storage_reliable", reliable_procfs);
+	if (storage_reliable_procfs == NULL) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" Failed to create storage_reliable procfs\n");
+		return -EFAULT;
+	}
+
+	proc_fs_uprobe = proc_create("oplus_uprobe", 0644, storage_reliable_procfs, &oplus_uprobe_proc_ops);
+	if (proc_fs_uprobe == NULL) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" Failed to create file oplus_storage/storage_reliable/oplus_uprobe\n");
+		return -EFAULT;
+	}
+
+	proc_fs_uprobe_enable = proc_create("uprobe_enable", 0600, storage_reliable_procfs, &uprobe_enable_fops);
+	if (proc_fs_uprobe_enable == NULL) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" Failed to create file oplus_storage/storage_reliable/uprobe_enable\n");
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static void __exit oplus_uprobe_exit(void)
+{
+
+	if(NULL == uprobe_register_funcptr || NULL == uprobe_unregister_funcptr) {
+		pr_err(OPLUS_UPROBE_LOG_TAG" uprobe_register or uprobe_unregister is NULL\n");
+		return;
+	}
+
+
+	remove_proc_entry("oplus_uprobe", proc_fs_uprobe);
+	remove_proc_entry("uprobe_enable", proc_fs_uprobe_enable);
+}
+
+module_init(oplus_uprobe_init);
+module_exit(oplus_uprobe_exit);
+
+MODULE_IMPORT_NS(VFS_internal_I_am_really_a_filesystem_and_am_NOT_a_driver);
+MODULE_AUTHOR("Cheng");
+MODULE_DESCRIPTION("oplus uprobe driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/oplus_uprobe.h b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/oplus_uprobe.h
new file mode 100644
index 000000000..0a17f1ab0
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/oplus_uprobe/oplus_uprobe.h
@@ -0,0 +1,4 @@
+#ifdef _OPLUS_UPROBE_H
+
+
+#endif
\ No newline at end of file
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/Kconfig b/drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/Kconfig
new file mode 100644
index 000000000..86620fbe4
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/Kconfig
@@ -0,0 +1,7 @@
+
+config OPLUS_FEATURE_STORAGE_LOG
+   tristate "config storage log"
+   default n
+   help
+     General Configuration of Storage Log
+
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/Makefile b/drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/Makefile
new file mode 100644
index 000000000..32d1b7242
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/Makefile
@@ -0,0 +1,3 @@
+GCOV_PROFILE := y
+obj-$(CONFIG_OPLUS_FEATURE_STORAGE_LOG) += oplus_storage_log.o
+oplus_storage_log-y := storage_log.o
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/storage_log.c b/drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/storage_log.c
new file mode 100644
index 000000000..4e2577dc6
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/storage_log/storage_log.c
@@ -0,0 +1,240 @@
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/fs.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/err.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/spinlock.h>
+#include <linux/time64.h>
+#include <linux/sched/clock.h>
+
+#define LOG_BUFFER_SIZE 1048576
+#define LOG_LINE_MAX    1024
+#define CLEAR_STORAGE_LOG "clear storage log"
+#define STORAGE_LOG_HEAD "storage log begin:\n"
+
+struct storage_log_data {
+    char* buf; /* ring buffer */
+    loff_t pos;
+    ssize_t len;
+    ssize_t head_len;
+    ssize_t show_len;
+    spinlock_t lock;
+    struct proc_dir_entry *storage;
+    struct proc_dir_entry *buf_log;
+};
+
+struct storage_log_data *log_data;
+
+#define STORAGE_LOG_HEAD_LEN log_data->head_len
+
+static size_t add_timestamp(char *buf)
+{
+    u64 ts = local_clock();
+    unsigned long rem_nsec = do_div(ts, 1000000000);
+
+    return sprintf(buf, "[%5lu.%06lu] ", (unsigned long)ts, rem_nsec / 1000);
+}
+
+#define STORAGE_LOG_WRITE(buffer, count, flags)                                  \
+    do {                                                                         \
+        char _timestamp[64];                                                     \
+        ssize_t _len = add_timestamp(_timestamp);                                \
+        spin_lock_irqsave(&log_data->lock, flags);                               \
+        if (log_data->pos + count + _len < LOG_BUFFER_SIZE) {                    \
+            memcpy(log_data->buf + log_data->pos, _timestamp, _len);              \
+            log_data->pos += _len;                                                \
+            memcpy(log_data->buf + log_data->pos, buffer, count);                  \
+            log_data->pos += count;                                                \
+        } else {                                                                  \
+            memcpy(log_data->buf, _timestamp, _len);                              \
+            memcpy(log_data->buf + _len , buffer, count);                         \
+            log_data->pos = _len + count;                                         \
+        }                                                                         \
+        log_data->len = min_t(size_t, (log_data->len + _len + count), LOG_BUFFER_SIZE); \
+        spin_unlock_irqrestore(&log_data->lock, flags);                                 \
+    } while (0)
+
+static void* log_start(struct seq_file *m, loff_t *pos)
+{
+    return (*pos >= log_data->len) ? NULL : log_data;
+}
+
+static void* log_next(struct seq_file *m, void *v, loff_t *pos)
+{
+    *pos += log_data->show_len;
+    return (*pos >= log_data->len) ? NULL : log_data;
+}
+
+static int log_show(struct seq_file *m, void *v)
+{
+    ssize_t len;
+
+    pr_debug("log show, size:%lu, count:%lu, read pos:%lu, log pos:%lu, log len:%lu\n",
+            m->size, m->count, (unsigned long)m->read_pos, (unsigned long)log_data->pos, log_data->len);
+
+    len = min_t(size_t, m->size, (log_data->len - m->read_pos));
+    memcpy(m->buf, log_data->buf + m->read_pos, len);
+    m->count = len;
+    log_data->show_len = len;
+    return 0;
+}
+
+static void log_stop(struct seq_file *m, void *v)
+{
+    log_data->show_len = 0;
+}
+
+static struct seq_operations log_ops = {
+    .start = log_start,
+    .stop = log_stop,
+    .next  = log_next,
+    .show = log_show
+};
+
+static int log_open(struct inode *inode, struct file *file)
+{
+    int err = -1;
+
+    err = seq_open(file, &log_ops);
+    if (!err)
+        ((struct seq_file *)file->private_data)->private = log_data;
+
+    return err;
+}
+
+static ssize_t log_write(struct file *file,
+               const char __user *buf,
+               size_t count,
+               loff_t *pos)
+{
+    unsigned long flags;
+    ssize_t len;
+    static char data_buf[LOG_LINE_MAX];
+
+    if (!count)
+        return 0;
+
+    if (count > LOG_LINE_MAX) {
+        pr_err("count(%lu) is larger than log line max(%d)\n", count, LOG_LINE_MAX);
+        return -1;
+    }
+
+    len = min_t(size_t, count, LOG_LINE_MAX);
+    if (copy_from_user(data_buf, buf, len)) {
+        pr_err("log write, copy data failed, count:%lu\n", count);
+        return -1;
+    }
+
+    pr_debug("log write, count:%lu, len:%lu, log pos:%u, log len:%lu\n",
+            count, len, (unsigned int)log_data->pos, log_data->len);
+
+    if (strncmp(CLEAR_STORAGE_LOG, data_buf, strlen(CLEAR_STORAGE_LOG)) == 0) {
+        pr_err("clear all storage log\n");
+        spin_lock_irqsave(&log_data->lock, flags);
+        log_data->len = STORAGE_LOG_HEAD_LEN;
+        log_data->pos = STORAGE_LOG_HEAD_LEN;
+        spin_unlock_irqrestore(&log_data->lock, flags);
+        return count;
+    }
+
+    STORAGE_LOG_WRITE(data_buf, len, flags);
+    return len;
+}
+
+static const struct proc_ops log_fops = {
+    .proc_open = log_open,
+    .proc_read = seq_read,
+    .proc_write = log_write,
+    .proc_release = seq_release,
+    .proc_lseek = default_llseek,
+};
+
+int pr_storage(const char *fmt, ...)
+{
+    static char text_buf[LOG_LINE_MAX];
+    va_list args;
+    int count;
+    unsigned long flags;
+
+    va_start(args, fmt);
+    count = vsnprintf(text_buf, LOG_LINE_MAX, fmt, args);
+    va_end(args);
+
+    STORAGE_LOG_WRITE(text_buf, count, flags);
+    return count;
+}
+
+EXPORT_SYMBOL_GPL(pr_storage);
+
+static void log_data_init(void)
+{
+    log_data->head_len = strlen(STORAGE_LOG_HEAD);
+    log_data->len = log_data->head_len;
+    log_data->pos = log_data->head_len;
+
+    spin_lock_init(&log_data->lock);
+}
+
+static int __init storage_log_init(void)
+{
+    log_data = kmalloc(sizeof(struct storage_log_data), GFP_KERNEL);
+    if (!log_data) {
+        pr_err("kmalloc error, storage_log init failed\n");
+        goto out;
+    }
+
+    log_data_init();
+
+    log_data->buf = vmalloc(LOG_BUFFER_SIZE);
+    if (!log_data->buf) {
+        pr_err("vmalloc error, storage_log init failed\n");
+        goto out_kfree;
+    }
+    memcpy(log_data->buf, STORAGE_LOG_HEAD, STORAGE_LOG_HEAD_LEN);
+
+    log_data->storage = proc_mkdir("storage", NULL);
+    if (!log_data->storage) {
+        pr_err("create storage error, storage init failed\n");
+        goto out_vfree;
+    }
+
+    log_data->buf_log = proc_create_data("buf_log", S_IRWXUGO, log_data->storage,
+            &log_fops, log_data);
+    if (!log_data->buf_log) {
+        pr_err("create buf log error, buf_log init failed\n");
+        goto out_remove;
+    }
+
+    pr_info("storage_log init succeed\n");
+    return 0;
+
+out_remove:
+    remove_proc_entry("storage", NULL);
+out_vfree:
+    vfree(log_data->buf);
+out_kfree:
+    kfree(log_data);
+out:
+    return -1;
+}
+
+static void __exit storage_log_exit(void)
+{
+    remove_proc_entry("buf_log", log_data->storage);
+    remove_proc_entry("storage", NULL);
+
+    vfree(log_data->buf);
+    kfree(log_data);
+
+    pr_info("storage_log exit succeed\n");
+}
+
+module_init(storage_log_init);
+module_exit(storage_log_exit);
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/Kconfig b/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/Kconfig
new file mode 100644
index 000000000..6797d2b18
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/Kconfig
@@ -0,0 +1,9 @@
+config OPLUS_UFS_DRIVER
+  tristate "config oplus ufs driver"
+  help
+    define this config to compile ufs-oplus-dbg for device register
+
+config OPLUS_QCOM_UFS_DRIVER
+  tristate "config oplus qcom ufs driver"
+  help
+    define this config to compile ufs-qcom-oplus-dbg for device register
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/Makefile b/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/Makefile
new file mode 100644
index 000000000..7301b88ef
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/Makefile
@@ -0,0 +1,2 @@
+ccflags-y += -I$(srctree)/drivers/ufs/core/
+obj-$(CONFIG_OPLUS_UFS_DRIVER)	+= ufs-oplus-dbg.o
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/ufs-oplus-dbg.c b/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/ufs-oplus-dbg.c
new file mode 100644
index 000000000..4d007f085
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/ufs-oplus-dbg.c
@@ -0,0 +1,1215 @@
+#include <scsi/scsi.h>
+#include <scsi/scsi_ioctl.h>
+#include <scsi/scsi_cmnd.h>
+#include <ufs/ufs_quirks.h>
+#include <ufs/ufshcd.h>
+#include <ufs/unipro.h>
+#include <trace/hooks/ufshcd.h>
+#include <linux/tracepoint.h>
+#include <linux/proc_fs.h>
+#include <linux/rtc.h>
+#include <linux/async.h>
+
+#include <soc/oplus/device_info.h>
+
+#include "ufs-oplus-dbg.h"
+#ifdef CONFIG_OPLUS_QCOM_UFS_DRIVER
+#include "../../../../../ufs/host/ufs-qcom.h"
+#endif /* CONFIG_OPLUS_QCOM_UFS_DRIVER */
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0))
+#ifdef CONFIG_OPLUS_QCOM_UFS_DRIVER
+#include "../../../../../ufs/core/ufshcd-priv.h"
+#endif /* CONFIG_OPLUS_QCOM_UFS_DRIVER */
+#include "ufshcd-priv.h"
+#else
+#ifdef CONFIG_OPLUS_QCOM_UFS_DRIVER
+#include <ufs/ufshcd-priv.h>
+#endif /* CONFIG_OPLUS_QCOM_UFS_DRIVER */
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0) */
+
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0))
+#define UFS_OPLUS_IOCTL_MAGIC 0xF5
+#define UFS_OPLUS_QUERY_IOCTL _IOWR(UFS_OPLUS_IOCTL_MAGIC, 0, int)
+#define UFS_OPLUS_MONITOR_IOCTL _IOWR(UFS_OPLUS_IOCTL_MAGIC, 1, int)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0) */
+
+struct unipro_signal_quality_ctrl signalCtrl;
+
+struct ufs_transmission_status_t ufs_transmission_status;
+struct device_attribute ufs_transmission_status_attr;
+
+#define ONE_DAY_SEC 86400
+static const char *ufs_null_device_strs = "nullnullnullnull";
+atomic_t ufs_init_done;
+
+int ufsplus_wb_status = 0;
+int ufsplus_hpb_status = 0;
+
+#ifdef CONFIG_OPLUS_QCOM_UFS_DRIVER
+extern int ufshcd_query_descriptor_retry(struct ufs_hba *hba,
+  				  enum query_opcode opcode,
+  				  enum desc_idn idn, u8 index,
+  				  u8 selector,
+  				  u8 *desc_buf, int *buf_len);
+#endif
+static void recordTimeStamp(
+	struct signal_quality *record,
+	enum ufs_event_type type
+) {
+	ktime_t cur_time = ktime_get();
+	switch (type)
+	{
+	case UFS_EVT_PA_ERR:
+	case UFS_EVT_DL_ERR:
+	case UFS_EVT_NL_ERR:
+	case UFS_EVT_TL_ERR:
+	case UFS_EVT_DME_ERR:
+		if (STAMP_RECORD_MAX <= record->stamp_pos)
+			return;
+		if (0 == record->stamp_pos)
+			record->stamp[0] = cur_time;
+		else if (cur_time > (record->stamp[record->stamp_pos - 1] +
+				STAMP_MIN_INTERVAL))
+			record->stamp[record->stamp_pos++] = cur_time;
+		return;
+	default:
+		return;
+	}
+}
+
+void recordUniproErr(
+	struct unipro_signal_quality_ctrl *signalCtrl,
+	u32 reg,
+	enum ufs_event_type type
+) {
+	unsigned long err_bits;
+	int ec;
+	struct signal_quality *rec = &signalCtrl->record;
+	recordTimeStamp(rec, type);
+	switch (type)
+	{
+	case UFS_EVT_FATAL_ERR:
+		if (DEVICE_FATAL_ERROR & reg)
+			rec->ufs_device_err_cnt++;
+		if (CONTROLLER_FATAL_ERROR & reg)
+			rec->ufs_host_err_cnt++;
+		if (SYSTEM_BUS_FATAL_ERROR & reg)
+			rec->ufs_bus_err_cnt++;
+		if (CRYPTO_ENGINE_FATAL_ERROR & reg)
+			rec->ufs_crypto_err_cnt++;
+		break;
+	case UFS_EVT_LINK_STARTUP_FAIL:
+		if (UIC_LINK_LOST & reg)
+			rec->ufs_link_lost_cnt++;
+		break;
+	case UFS_EVT_PA_ERR:
+		err_bits = reg & UIC_PHY_ADAPTER_LAYER_ERROR_CODE_MASK;
+		for_each_set_bit(ec, &err_bits, UNIPRO_PA_ERR_MAX) {
+			rec->unipro_PA_err_total_cnt++;
+			rec->unipro_PA_err_cnt[ec]++;
+		}
+		break;
+	case UFS_EVT_DL_ERR:
+		err_bits = reg & UIC_DATA_LINK_LAYER_ERROR_CODE_MASK;
+		for_each_set_bit(ec, &err_bits, UNIPRO_DL_ERR_MAX) {
+			rec->unipro_DL_err_total_cnt++;
+			rec->unipro_DL_err_cnt[ec]++;
+		}
+		break;
+	case UFS_EVT_NL_ERR:
+		err_bits = reg & UIC_NETWORK_LAYER_ERROR_CODE_MASK;
+		for_each_set_bit(ec, &err_bits, UNIPRO_NL_ERR_MAX) {
+			rec->unipro_NL_err_total_cnt++;
+			rec->unipro_NL_err_cnt[ec]++;
+		}
+		break;
+	case UFS_EVT_TL_ERR:
+		err_bits = reg & UIC_TRANSPORT_LAYER_ERROR_CODE_MASK;
+		for_each_set_bit(ec, &err_bits, UNIPRO_TL_ERR_MAX) {
+			rec->unipro_TL_err_total_cnt++;
+			rec->unipro_TL_err_cnt[ec]++;
+		}
+		break;
+	case UFS_EVT_DME_ERR:
+		err_bits = reg & UIC_DME_ERROR_CODE_MASK;
+		for_each_set_bit(ec, &err_bits, UNIPRO_DME_ERR_MAX) {
+			rec->unipro_DME_err_total_cnt++;
+			rec->unipro_DME_err_cnt[ec]++;
+		}
+		break;
+	case UFS_EVT_ABORT:
+		rec->task_abort_cnt++;
+		break;
+	case UFS_EVT_HOST_RESET:
+		rec->host_reset_cnt++;
+		break;
+	case UFS_EVT_DEV_RESET:
+		rec->dev_reset_cnt++;
+		break;
+	default:
+		break;
+	}
+}
+
+#define SEQ_EASY_PRINT(x)   seq_printf(s, #x"\t%d\n", signalCtrl->record.x)
+#define SEQ_PA_PRINT(x)     \
+	seq_printf(s, #x"\t%d\n", signalCtrl->record.unipro_PA_err_cnt[x])
+#define SEQ_DL_PRINT(x)     \
+	seq_printf(s, #x"\t%d\n", signalCtrl->record.unipro_DL_err_cnt[x])
+#define SEQ_NL_PRINT(x)     \
+	seq_printf(s, #x"\t%d\n", signalCtrl->record.unipro_NL_err_cnt[x])
+#define SEQ_TL_PRINT(x)     \
+	seq_printf(s, #x"\t%d\n", signalCtrl->record.unipro_TL_err_cnt[x])
+#define SEQ_DME_PRINT(x)    \
+	seq_printf(s, #x"\t%d\n", signalCtrl->record.unipro_DME_err_cnt[x])
+#define SEQ_STAMP_PRINT(x)  \
+	seq_printf(s, #x"\t%lld\n", signalCtrl->record.stamp[x])
+
+#define SEQ_GEAR_PRINT(x)  \
+	seq_printf(s, #x"\t%d\n", signalCtrl->record.gear_err_cnt[x])
+
+static int record_read_func(struct seq_file *s, void *v)
+{
+	struct unipro_signal_quality_ctrl *signalCtrl =
+		(struct unipro_signal_quality_ctrl *)(s->private);
+	if (!signalCtrl)
+		return -EINVAL;
+	SEQ_EASY_PRINT(ufs_device_err_cnt);
+	SEQ_EASY_PRINT(ufs_host_err_cnt);
+	SEQ_EASY_PRINT(ufs_bus_err_cnt);
+	SEQ_EASY_PRINT(ufs_crypto_err_cnt);
+	SEQ_EASY_PRINT(ufs_link_lost_cnt);
+	SEQ_EASY_PRINT(task_abort_cnt);
+	SEQ_EASY_PRINT(host_reset_cnt);
+	SEQ_EASY_PRINT(dev_reset_cnt);
+	SEQ_EASY_PRINT(unipro_PA_err_total_cnt);
+	SEQ_PA_PRINT(UNIPRO_PA_LANE0_ERR_CNT);
+	SEQ_PA_PRINT(UNIPRO_PA_LANE1_ERR_CNT);
+	SEQ_PA_PRINT(UNIPRO_PA_LANE2_ERR_CNT);
+	SEQ_PA_PRINT(UNIPRO_PA_LANE3_ERR_CNT);
+	SEQ_PA_PRINT(UNIPRO_PA_LINE_RESET);
+	SEQ_EASY_PRINT(unipro_DL_err_total_cnt);
+	SEQ_DL_PRINT(UNIPRO_DL_NAC_RECEIVED);
+	SEQ_DL_PRINT(UNIPRO_DL_TCX_REPLAY_TIMER_EXPIRED);
+	SEQ_DL_PRINT(UNIPRO_DL_AFCX_REQUEST_TIMER_EXPIRED);
+	SEQ_DL_PRINT(UNIPRO_DL_FCX_PROTECTION_TIMER_EXPIRED);
+	SEQ_DL_PRINT(UNIPRO_DL_CRC_ERROR);
+	SEQ_DL_PRINT(UNIPRO_DL_RX_BUFFER_OVERFLOW);
+	SEQ_DL_PRINT(UNIPRO_DL_MAX_FRAME_LENGTH_EXCEEDED);
+	SEQ_DL_PRINT(UNIPRO_DL_WRONG_SEQUENCE_NUMBER);
+	SEQ_DL_PRINT(UNIPRO_DL_AFC_FRAME_SYNTAX_ERROR);
+	SEQ_DL_PRINT(UNIPRO_DL_NAC_FRAME_SYNTAX_ERROR);
+	SEQ_DL_PRINT(UNIPRO_DL_EOF_SYNTAX_ERROR);
+	SEQ_DL_PRINT(UNIPRO_DL_FRAME_SYNTAX_ERROR);
+	SEQ_DL_PRINT(UNIPRO_DL_BAD_CTRL_SYMBOL_TYPE);
+	SEQ_DL_PRINT(UNIPRO_DL_PA_INIT_ERROR);
+	SEQ_DL_PRINT(UNIPRO_DL_PA_ERROR_IND_RECEIVED);
+	SEQ_DL_PRINT(UNIPRO_DL_PA_INIT);
+	SEQ_EASY_PRINT(unipro_NL_err_total_cnt);
+	SEQ_NL_PRINT(UNIPRO_NL_UNSUPPORTED_HEADER_TYPE);
+	SEQ_NL_PRINT(UNIPRO_NL_BAD_DEVICEID_ENC);
+	SEQ_NL_PRINT(UNIPRO_NL_LHDR_TRAP_PACKET_DROPPING);
+	SEQ_EASY_PRINT(unipro_TL_err_total_cnt);
+	SEQ_TL_PRINT(UNIPRO_TL_UNSUPPORTED_HEADER_TYPE);
+	SEQ_TL_PRINT(UNIPRO_TL_UNKNOWN_CPORTID);
+	SEQ_TL_PRINT(UNIPRO_TL_NO_CONNECTION_RX);
+	SEQ_TL_PRINT(UNIPRO_TL_CONTROLLED_SEGMENT_DROPPING);
+	SEQ_TL_PRINT(UNIPRO_TL_BAD_TC);
+	SEQ_TL_PRINT(UNIPRO_TL_E2E_CREDIT_OVERFLOW);
+	SEQ_TL_PRINT(UNIPRO_TL_SAFETY_VALVE_DROPPING);
+	SEQ_EASY_PRINT(unipro_DME_err_total_cnt);
+	SEQ_DME_PRINT(UNIPRO_DME_GENERIC);
+	SEQ_DME_PRINT(UNIPRO_DME_TX_QOS);
+	SEQ_DME_PRINT(UNIPRO_DME_RX_QOS);
+	SEQ_DME_PRINT(UNIPRO_DME_PA_INIT_QOS);
+	SEQ_GEAR_PRINT(UFS_HS_G1);
+	SEQ_GEAR_PRINT(UFS_HS_G2);
+	SEQ_GEAR_PRINT(UFS_HS_G3);
+	SEQ_GEAR_PRINT(UFS_HS_G4);
+	SEQ_GEAR_PRINT(UFS_HS_G5);
+	return 0;
+}
+
+static int record_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, record_read_func, pde_data(inode));
+}
+
+static const struct proc_ops record_fops = {
+	.proc_open = record_open,
+	.proc_read = seq_read,
+	.proc_release = single_release,
+};
+
+#define SEQ_UPLOAD_PRINT(x) \
+	seq_printf(s, #x": %d\n", signalCtrl->record.x \
+		-signalCtrl->record_upload.x);\
+	signalCtrl->record_upload.x = signalCtrl->record.x;
+#define SEQ_UPLOAD_STAMP_PRINT(x) \
+	seq_printf(s, #x": %lld\n", signalCtrl->record.stamp[x] \
+		-signalCtrl->record_upload.stamp[x]);\
+	signalCtrl->record_upload.stamp[x] = signalCtrl->record.stamp[x];
+
+#define SEQ_PA_UPLOAD_PRINT(x) \
+	seq_printf(s, #x": %d\n", signalCtrl->record.unipro_PA_err_cnt[x] \
+		-signalCtrl->record_upload.unipro_PA_err_cnt[x]);\
+	signalCtrl->record_upload.unipro_PA_err_cnt[x] = signalCtrl->record.unipro_PA_err_cnt[x];
+
+#define SEQ_DL_UPLOAD_PRINT(x) \
+		seq_printf(s, #x": %d\n", signalCtrl->record.unipro_DL_err_cnt[x] \
+			-signalCtrl->record_upload.unipro_DL_err_cnt[x]);\
+		signalCtrl->record_upload.unipro_DL_err_cnt[x] = signalCtrl->record.unipro_DL_err_cnt[x];
+
+#define SEQ_DL_UPLOAD_PRINT(x) \
+			seq_printf(s, #x": %d\n", signalCtrl->record.unipro_DL_err_cnt[x] \
+				-signalCtrl->record_upload.unipro_DL_err_cnt[x]);\
+			signalCtrl->record_upload.unipro_DL_err_cnt[x] = signalCtrl->record.unipro_DL_err_cnt[x];
+
+#define SEQ_NL_UPLOAD_PRINT(x) \
+				seq_printf(s, #x": %d\n", signalCtrl->record.unipro_NL_err_cnt[x] \
+					-signalCtrl->record_upload.unipro_NL_err_cnt[x]);\
+				signalCtrl->record_upload.unipro_NL_err_cnt[x] = signalCtrl->record.unipro_NL_err_cnt[x];
+
+#define SEQ_TL_UPLOAD_PRINT(x) \
+					seq_printf(s, #x": %d\n", signalCtrl->record.unipro_TL_err_cnt[x] \
+						-signalCtrl->record_upload.unipro_TL_err_cnt[x]);\
+					signalCtrl->record_upload.unipro_TL_err_cnt[x] = signalCtrl->record.unipro_TL_err_cnt[x];
+
+#define SEQ_DME_UPLOAD_PRINT(x) \
+						seq_printf(s, #x": %d\n", signalCtrl->record.unipro_DME_err_cnt[x] \
+							-signalCtrl->record_upload.unipro_DME_err_cnt[x]);\
+						signalCtrl->record_upload.unipro_DME_err_cnt[x] = signalCtrl->record.unipro_DME_err_cnt[x];
+
+#define SEQ_GEAR_UPLOAD_PRINT(x) \
+						seq_printf(s, #x": %d\n", signalCtrl->record.gear_err_cnt[x] \
+							-signalCtrl->record_upload.gear_err_cnt[x]);\
+						signalCtrl->record_upload.gear_err_cnt[x] = signalCtrl->record.gear_err_cnt[x];
+
+static int record_upload_read_func(struct seq_file *s, void *v)
+{
+	struct unipro_signal_quality_ctrl *signalCtrl =
+		(struct unipro_signal_quality_ctrl *)(s->private);
+	if (!signalCtrl)
+		return -EINVAL;
+	SEQ_UPLOAD_PRINT(ufs_device_err_cnt);
+	SEQ_UPLOAD_PRINT(ufs_host_err_cnt);
+	SEQ_UPLOAD_PRINT(ufs_bus_err_cnt);
+	SEQ_UPLOAD_PRINT(ufs_crypto_err_cnt);
+	SEQ_UPLOAD_PRINT(ufs_link_lost_cnt);
+	SEQ_UPLOAD_PRINT(task_abort_cnt);
+	SEQ_UPLOAD_PRINT(host_reset_cnt);
+	SEQ_UPLOAD_PRINT(dev_reset_cnt);
+	SEQ_UPLOAD_PRINT(unipro_PA_err_total_cnt);
+	SEQ_UPLOAD_PRINT(unipro_DL_err_total_cnt);
+	SEQ_UPLOAD_PRINT(unipro_NL_err_total_cnt);
+	SEQ_UPLOAD_PRINT(unipro_TL_err_total_cnt);
+	SEQ_UPLOAD_PRINT(unipro_DME_err_total_cnt);
+	SEQ_PA_UPLOAD_PRINT(UNIPRO_PA_LANE0_ERR_CNT);
+	SEQ_PA_UPLOAD_PRINT(UNIPRO_PA_LANE1_ERR_CNT);
+	SEQ_PA_UPLOAD_PRINT(UNIPRO_PA_LANE2_ERR_CNT);
+	SEQ_PA_UPLOAD_PRINT(UNIPRO_PA_LANE3_ERR_CNT);
+	SEQ_PA_UPLOAD_PRINT(UNIPRO_PA_LINE_RESET);
+
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_NAC_RECEIVED);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_TCX_REPLAY_TIMER_EXPIRED);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_AFCX_REQUEST_TIMER_EXPIRED);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_FCX_PROTECTION_TIMER_EXPIRED);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_CRC_ERROR);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_RX_BUFFER_OVERFLOW);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_MAX_FRAME_LENGTH_EXCEEDED);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_WRONG_SEQUENCE_NUMBER);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_AFC_FRAME_SYNTAX_ERROR);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_NAC_FRAME_SYNTAX_ERROR);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_EOF_SYNTAX_ERROR);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_FRAME_SYNTAX_ERROR);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_BAD_CTRL_SYMBOL_TYPE);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_PA_INIT_ERROR);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_PA_ERROR_IND_RECEIVED);
+	SEQ_DL_UPLOAD_PRINT(UNIPRO_DL_PA_INIT);
+
+	SEQ_NL_UPLOAD_PRINT(UNIPRO_NL_UNSUPPORTED_HEADER_TYPE);
+	SEQ_NL_UPLOAD_PRINT(UNIPRO_NL_BAD_DEVICEID_ENC);
+	SEQ_NL_UPLOAD_PRINT(UNIPRO_NL_LHDR_TRAP_PACKET_DROPPING);
+
+	SEQ_TL_UPLOAD_PRINT(UNIPRO_TL_UNSUPPORTED_HEADER_TYPE);
+	SEQ_TL_UPLOAD_PRINT(UNIPRO_TL_UNKNOWN_CPORTID);
+	SEQ_TL_UPLOAD_PRINT(UNIPRO_TL_NO_CONNECTION_RX);
+	SEQ_TL_UPLOAD_PRINT(UNIPRO_TL_CONTROLLED_SEGMENT_DROPPING);
+	SEQ_TL_UPLOAD_PRINT(UNIPRO_TL_BAD_TC);
+	SEQ_TL_UPLOAD_PRINT(UNIPRO_TL_E2E_CREDIT_OVERFLOW);
+	SEQ_TL_UPLOAD_PRINT(UNIPRO_TL_SAFETY_VALVE_DROPPING);
+
+	SEQ_DME_UPLOAD_PRINT(UNIPRO_DME_GENERIC);
+	SEQ_DME_UPLOAD_PRINT(UNIPRO_DME_TX_QOS);
+	SEQ_DME_UPLOAD_PRINT(UNIPRO_DME_RX_QOS);
+	SEQ_DME_UPLOAD_PRINT(UNIPRO_DME_PA_INIT_QOS);
+
+	SEQ_GEAR_UPLOAD_PRINT(UFS_HS_G1);
+	SEQ_GEAR_UPLOAD_PRINT(UFS_HS_G2);
+	SEQ_GEAR_UPLOAD_PRINT(UFS_HS_G3);
+	SEQ_GEAR_UPLOAD_PRINT(UFS_HS_G4);
+	SEQ_GEAR_UPLOAD_PRINT(UFS_HS_G5);
+	return 0;
+}
+
+static int record_upload_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, record_upload_read_func, pde_data(inode));
+}
+
+static const struct proc_ops record_upload_fops = {
+	.proc_open = record_upload_open,
+	.proc_read = seq_read,
+	.proc_release = single_release,
+};
+
+int create_signal_quality_proc(struct unipro_signal_quality_ctrl *signalCtrl)
+{
+	struct proc_dir_entry *d_entry;
+	signalCtrl->ctrl_dir = proc_mkdir("ufs_signalShow", NULL);
+	if (!signalCtrl->ctrl_dir)
+		return -ENOMEM;
+	d_entry = proc_create_data("record", S_IRUGO, signalCtrl->ctrl_dir,
+			&record_fops, signalCtrl);
+	if (!d_entry)
+		return -ENOMEM;
+	d_entry = proc_create_data("record_upload", S_IRUGO, signalCtrl->ctrl_dir,
+			&record_upload_fops, signalCtrl);
+	if (!d_entry)
+		return -ENOMEM;
+	return 0;
+}
+
+void remove_signal_quality_proc(struct unipro_signal_quality_ctrl *signalCtrl)
+{
+	if (signalCtrl->ctrl_dir) {
+		remove_proc_entry("record", signalCtrl->ctrl_dir);
+		remove_proc_entry("record_upload", signalCtrl->ctrl_dir);
+	}
+	return;
+}
+
+void recordGearErr(struct unipro_signal_quality_ctrl *signalCtrl, struct ufs_hba *hba)
+{
+	struct ufs_pa_layer_attr *pwr_info = &hba->pwr_info;
+	u32 dev_gear = min_t(u32, pwr_info->gear_rx, pwr_info->gear_tx);
+
+	if (dev_gear > UFS_HS_G5)
+		return;
+
+	signalCtrl->record.gear_err_cnt[dev_gear]++;
+}
+
+void recordSignalerr(struct ufs_hba *hba, unsigned int val, enum ufs_event_type evt)
+{
+	recordUniproErr(&signalCtrl, val, evt);
+	recordGearErr(&signalCtrl, hba);
+}
+EXPORT_SYMBOL_GPL(recordSignalerr);
+
+int get_rtc_time(struct rtc_time *tm)
+{
+	struct rtc_device *rtc;
+	int rc = 0;
+
+	rtc = rtc_class_open("rtc0");
+	if (rtc == NULL)
+		return -1;
+
+	rc = rtc_read_time(rtc, tm);
+	if (rc)
+		goto close_time;
+
+	rc = rtc_valid_tm(tm);
+	if (rc)
+		goto close_time;
+
+close_time:
+	rtc_class_close(rtc);
+
+	return rc;
+}
+
+void ufs_active_time_get(struct ufs_hba *hba)
+{
+	struct rtc_time tm;
+	int rc = 0;
+	ufs_transmission_status.active_count++;
+	rc = get_rtc_time(&tm);
+	if (rc != 0) {
+		dev_err(hba->dev,"ufs_transmission_status: get_rtc_time failed\n");
+		return;
+	}
+	ufs_transmission_status.resume_timing = (tm.tm_hour * 3600 + tm.tm_min * 60 + tm.tm_sec);
+	if (ufs_transmission_status.resume_timing < ufs_transmission_status.suspend_timing) {
+		ufs_transmission_status.sleep_time += ((ufs_transmission_status.resume_timing
+			+ ONE_DAY_SEC) - ufs_transmission_status.suspend_timing);
+		return;
+	}
+	if(ufs_transmission_status.suspend_timing == 0)
+		return;
+
+	ufs_transmission_status.sleep_time += (ufs_transmission_status.resume_timing
+		- ufs_transmission_status.suspend_timing);
+	return;
+}
+EXPORT_SYMBOL_GPL(ufs_active_time_get);
+
+
+void ufs_sleep_time_get(struct ufs_hba *hba)
+{
+	struct rtc_time tm;
+	int rc = 0;
+	ufs_transmission_status.sleep_count++;
+	rc = get_rtc_time(&tm);
+	if (rc != 0) {
+		dev_err(hba->dev,"ufs_transmission_status: get_rtc_time failed\n");
+		return;
+	}
+	ufs_transmission_status.suspend_timing = (tm.tm_hour * 3600 + tm.tm_min * 60 + tm.tm_sec);
+	if (ufs_transmission_status.suspend_timing < ufs_transmission_status.resume_timing) {
+		ufs_transmission_status.active_time += ((ufs_transmission_status.suspend_timing
+			+ ONE_DAY_SEC) - ufs_transmission_status.resume_timing);
+		return;
+	}
+	if(ufs_transmission_status.resume_timing == 0)
+		return;
+
+	ufs_transmission_status.active_time += (ufs_transmission_status.suspend_timing
+		- ufs_transmission_status.resume_timing);
+	return;
+}
+EXPORT_SYMBOL_GPL(ufs_sleep_time_get);
+
+static void ufshcd_lrb_scsicmd_time_statistics(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)
+{
+	if (lrbp->cmd->cmnd[0] == WRITE_10 || lrbp->cmd->cmnd[0] == WRITE_16) {
+		if (hba->pwr_info.gear_tx == 1) {
+			ufs_transmission_status.gear_min_write_sec += blk_rq_sectors(scsi_cmd_to_rq(lrbp->cmd));
+			ufs_transmission_status.gear_min_write_us +=
+				ktime_us_delta(lrbp->compl_time_stamp, lrbp->issue_time_stamp);
+		}
+
+		if (hba->pwr_info.gear_tx == 3 || hba->pwr_info.gear_tx == 4) {
+			ufs_transmission_status.gear_max_write_sec += blk_rq_sectors(scsi_cmd_to_rq(lrbp->cmd));
+			ufs_transmission_status.gear_max_write_us +=
+				ktime_us_delta(lrbp->compl_time_stamp, lrbp->issue_time_stamp);
+		}
+	} else if (lrbp->cmd->cmnd[0] == READ_10 || lrbp->cmd->cmnd[0] == READ_16) {
+		if (hba->pwr_info.gear_rx == 1) {
+			ufs_transmission_status.gear_min_read_sec += blk_rq_sectors(scsi_cmd_to_rq(lrbp->cmd));
+			ufs_transmission_status.gear_min_read_us +=
+				ktime_us_delta(lrbp->compl_time_stamp, lrbp->issue_time_stamp);
+		}
+
+		if (hba->pwr_info.gear_rx == 3 || hba->pwr_info.gear_rx == 4) {
+			ufs_transmission_status.gear_max_read_sec += blk_rq_sectors(scsi_cmd_to_rq(lrbp->cmd));
+			ufs_transmission_status.gear_max_read_us +=
+				ktime_us_delta(lrbp->compl_time_stamp, lrbp->issue_time_stamp);
+		}
+	} else {
+		if (hba->pwr_info.gear_rx == 1) {
+			ufs_transmission_status.gear_min_other_sec += blk_rq_sectors(scsi_cmd_to_rq(lrbp->cmd));
+			ufs_transmission_status.gear_min_other_us += ktime_us_delta(lrbp->compl_time_stamp, lrbp->issue_time_stamp);
+		}
+
+		if (hba->pwr_info.gear_rx == 3 || hba->pwr_info.gear_rx == 4) {
+			ufs_transmission_status.gear_max_other_sec += blk_rq_sectors(scsi_cmd_to_rq(lrbp->cmd));
+			ufs_transmission_status.gear_max_other_us += ktime_us_delta(lrbp->compl_time_stamp, lrbp->issue_time_stamp);
+		}
+	}
+
+	return;
+}
+
+static void ufshcd_lrb_devcmd_time_statistics(struct ufs_hba *hba, struct ufshcd_lrb *lrbp)
+{
+	if (hba->pwr_info.gear_tx == 1) {
+		ufs_transmission_status.gear_min_dev_us +=
+			ktime_us_delta(lrbp->compl_time_stamp, lrbp->issue_time_stamp);
+	}
+
+	if (hba->pwr_info.gear_tx == 3 || hba->pwr_info.gear_tx == 4) {
+		ufs_transmission_status.gear_max_dev_us +=
+			ktime_us_delta(lrbp->compl_time_stamp, lrbp->issue_time_stamp);
+	}
+}
+
+void ufs_send_cmd_handle(void *data, struct ufs_hba *hba, struct ufshcd_lrb *lrbp)
+{
+	if (ufs_transmission_status.transmission_status_enable) {
+		if(lrbp->cmd) {
+			ufs_transmission_status.scsi_send_count++;
+		} else {
+			ufs_transmission_status.dev_cmd_count++;
+		}
+	}
+}
+
+void ufs_compl_cmd_handle(void *data, struct ufs_hba *hba, struct ufshcd_lrb *lrbp)
+{
+	if (lrbp->cmd) {
+		if (ufs_transmission_status.transmission_status_enable) {
+			lrbp->compl_time_stamp = ktime_get();
+			ufshcd_lrb_scsicmd_time_statistics(hba, lrbp);
+		}
+	} else if (lrbp->command_type == UTP_CMD_TYPE_DEV_MANAGE ||
+			lrbp->command_type == UTP_CMD_TYPE_UFS_STORAGE) {
+		if (ufs_transmission_status.transmission_status_enable) {
+			ufshcd_lrb_devcmd_time_statistics(hba, lrbp);
+		}
+	}
+}
+
+static ssize_t ufshcd_transmission_status_data_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE,
+					"transmission_status_enable:%u\n"
+					"gear_min_write_sec:%llu\n"
+					"gear_max_write_sec:%llu\n"
+					"gear_min_read_sec:%llu\n"
+					"gear_max_read_sec:%llu\n"
+					"gear_min_write_us:%llu\n"
+					"gear_max_write_us:%llu\n"
+					"gear_min_read_us:%llu\n"
+					"gear_max_read_us:%llu\n"
+					"gear_min_dev_us:%llu\n"
+					"gear_max_dev_us:%llu\n"
+					"gear_min_other_sec:%llu\n"
+					"gear_max_other_sec:%llu\n"
+					"gear_min_other_us:%llu\n"
+					"gear_max_other_us:%llu\n"
+					"scsi_send_count:%llu\n"
+					"dev_cmd_count:%llu\n"
+					"active_count:%llu\n"
+					"active_time:%llu\n"
+					"sleep_count:%llu\n"
+					"sleep_time:%llu\n"
+					"powerdown_count:%llu\n"
+					"powerdown_time:%llu\n"
+					"power_total_count:%llu\n"
+					"current_pwr_mode:%u\n",
+					ufs_transmission_status.transmission_status_enable,
+					ufs_transmission_status.gear_min_write_sec,
+					ufs_transmission_status.gear_max_write_sec,
+					ufs_transmission_status.gear_min_read_sec,
+					ufs_transmission_status.gear_max_read_sec,
+					ufs_transmission_status.gear_min_write_us,
+					ufs_transmission_status.gear_max_write_us,
+					ufs_transmission_status.gear_min_read_us,
+					ufs_transmission_status.gear_max_read_us,
+					ufs_transmission_status.gear_min_dev_us,
+					ufs_transmission_status.gear_max_dev_us,
+					ufs_transmission_status.gear_min_other_sec,
+					ufs_transmission_status.gear_max_other_sec,
+					ufs_transmission_status.gear_min_other_us,
+					ufs_transmission_status.gear_max_other_us,
+					ufs_transmission_status.scsi_send_count,
+					ufs_transmission_status.dev_cmd_count,
+					ufs_transmission_status.active_count,
+					ufs_transmission_status.active_time,
+					ufs_transmission_status.sleep_count,
+					ufs_transmission_status.sleep_time,
+					ufs_transmission_status.powerdown_count,
+					ufs_transmission_status.powerdown_time,
+					ufs_transmission_status.power_total_count,
+					ufs_transmission_status.current_pwr_mode);
+}
+
+static ssize_t ufshcd_transmission_status_data_store(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t count)
+{
+	u32 value;
+
+	if (kstrtou32(buf, 0, &value))
+		return -EINVAL;
+
+	value = !!value;
+
+	if (value) {
+		ufs_transmission_status.transmission_status_enable = 1;
+	} else {
+		ufs_transmission_status.transmission_status_enable = 0;
+		memset(&ufs_transmission_status, 0, sizeof(struct ufs_transmission_status_t));
+	}
+
+	return count;
+}
+
+static void ufshcd_transmission_status_init_sysfs(struct ufs_hba *hba)
+{
+	printk("tianwen: ufshcd_transmission_status_init_sysfs start\n");
+	ufs_transmission_status_attr.show = ufshcd_transmission_status_data_show;
+	ufs_transmission_status_attr.store = ufshcd_transmission_status_data_store;
+	sysfs_attr_init(&ufs_transmission_status_attr.attr);
+	ufs_transmission_status_attr.attr.name = "ufs_transmission_status";
+	ufs_transmission_status_attr.attr.mode = 0644;
+	if (device_create_file(hba->dev, &ufs_transmission_status_attr))
+		dev_err(hba->dev, "Failed to create sysfs for ufs_transmission_status_attr\n");
+
+	/*init the struct ufs_transmission_status*/
+	memset(&ufs_transmission_status, 0, sizeof(struct ufs_transmission_status_t));
+	ufs_transmission_status.transmission_status_enable = 1;
+}
+
+#ifdef CONFIG_SCSI_UFS_HPB
+static bool is_ufshpb_allowed(struct ufs_hba *hba)
+{
+	return !(hba->ufshpb_dev.hpb_disabled);
+}
+#else
+static bool is_ufshpb_allowed(struct ufs_hba *hba)
+{
+	pr_warn("ufshpb macro definition is not opened\n");
+	return false;
+}
+#endif /* CONFIG_SCSI_UFS_HPB */
+
+static void create_devinfo_ufs(void *data, async_cookie_t c)
+{
+	struct scsi_device *sdev = data;
+	static char temp_version[5] = {0};
+	static char vendor[9] = {0};
+	static char model[17] = {0};
+	int ret = 0;
+	struct ufs_hba *hba = NULL;
+
+	pr_info("get ufs device vendor/model/rev\n");
+	WARN_ON(!sdev);
+	strncpy(temp_version, sdev->rev, 4);
+	strncpy(vendor, sdev->vendor, 8);
+	strncpy(model, sdev->model, 16);
+
+	ret = register_device_proc("ufs_version", temp_version, vendor);
+
+	if (ret) {
+		pr_err("%s create ufs_version fail, ret=%d",__func__,ret);
+		return;
+	}
+
+	ret = register_device_proc("ufs", model, vendor);
+
+	if (ret) {
+		pr_err("%s create ufs fail, ret=%d",__func__,ret);
+	}
+
+	hba = shost_priv(sdev->host);
+	if (hba && ufshcd_is_wb_allowed(hba)) {
+		ufsplus_wb_status = 1;
+	}
+	if (hba && is_ufshpb_allowed(hba)) {
+		ufsplus_hpb_status = 1;
+	}
+	ret = register_device_proc_for_ufsplus("ufsplus_status", &ufsplus_hpb_status, &ufsplus_wb_status);
+	if (ret) {
+		pr_err("%s create , ret=%d",__func__,ret);
+	}
+
+}
+
+static int monitor_verify_command(unsigned char *cmd)
+{
+    if (cmd[0] != 0x3B && cmd[0] != 0x3C && cmd[0] != 0xC0)
+        return false;
+
+    return true;
+}
+
+/**
+ * ufs_ioctl_monitor - special cmd for memory monitor
+ * @hba: per-adapter instance
+ * @buf_user: user space buffer for ioctl data
+ * @return: 0 for success negative error code otherwise
+ *
+ */
+int ufs_ioctl_monitor(struct scsi_device *dev, void __user *buf_user)
+{
+	struct request_queue *q = dev->request_queue;
+	struct request *rq;
+	struct scsi_cmnd *req;
+	struct scsi_ioctl_command __user *sic = (struct scsi_ioctl_command __user *)buf_user;
+	int err;
+	unsigned int in_len, out_len, bytes, opcode, cmdlen;
+	char *buffer = NULL;
+
+	/*
+	 * get in an out lengths, verify they don't exceed a page worth of data
+	 */
+	if (get_user(in_len, &sic->inlen))
+		return -EFAULT;
+	if (get_user(out_len, &sic->outlen))
+		return -EFAULT;
+	if (in_len > PAGE_SIZE || out_len > PAGE_SIZE)
+		return -EINVAL;
+	if (get_user(opcode, sic->data))
+		return -EFAULT;
+
+	bytes = max(in_len, out_len);
+	if (bytes) {
+		buffer = kzalloc(bytes, GFP_NOIO | GFP_USER| __GFP_NOWARN);
+		if (!buffer)
+			return -ENOMEM;
+
+	}
+
+	rq = scsi_alloc_request(q, in_len ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN, 0);
+	if (IS_ERR(rq)) {
+		err = PTR_ERR(rq);
+		goto error_free_buffer;
+	}
+	req = blk_mq_rq_to_pdu(rq);
+
+	cmdlen = COMMAND_SIZE(opcode);
+	if (((VENDOR_SPECIFIC_CDB == opcode) && (0 == strncmp(dev->vendor, "SAMSUNG ", 8)))
+	         || ((READ_BUFFER == opcode) && (0 == strncmp(dev->vendor, "XBSTOR ", 7)))) {
+		cmdlen = 16;
+	}
+
+	/*
+	 * get command and data to send to device, if any
+	 */
+	err = -EFAULT;
+	req->cmd_len = cmdlen;
+	if (copy_from_user(req->cmnd, sic->data, cmdlen))
+		goto error;
+
+	if (in_len && copy_from_user(buffer, sic->data + cmdlen, in_len))
+		goto error;
+
+	if (!monitor_verify_command(req->cmnd))
+		goto error;
+
+	/* default.  possible overriden later */
+	req->retries = 5;
+
+	if (bytes) {
+		err = blk_rq_map_kern(q, rq, buffer, bytes, GFP_NOIO);
+		if (err)
+			goto error;
+	}
+	blk_execute_rq(rq, 0);
+
+#define OMAX_SB_LEN 16          /* For backward compatibility */
+	err = req->result & 0xff;	/* only 8 bit SCSI status */
+	if (err) {
+		if (req->sense_len && req->sense_buffer) {
+			bytes = (OMAX_SB_LEN > req->sense_len) ?
+				req->sense_len : OMAX_SB_LEN;
+			if (copy_to_user(sic->data, req->sense_buffer, bytes))
+				err = -EFAULT;
+		}
+	} else {
+		if (copy_to_user(sic->data, buffer, out_len))
+			err = -EFAULT;
+	}
+
+error:
+	blk_mq_free_request(rq);
+
+error_free_buffer:
+	kfree(buffer);
+
+	return err;
+}
+
+static void probe_android_vh_ufs_update_sdev(void *data, struct scsi_device *sdev)
+{
+	if (strcmp(sdev->model, ufs_null_device_strs) && atomic_inc_return(&ufs_init_done) == 1) {
+		async_schedule(create_devinfo_ufs, sdev);
+	}
+
+}
+
+static int oplus_ufs_regist_tracepoint(void)
+{
+	int rc;
+	printk("oplus ufs trace point init");
+	rc = register_trace_android_vh_ufs_send_command(ufs_send_cmd_handle, NULL);
+	if (rc != 0)
+		pr_err("register_trace_android_vh_ufs_send_command failed! rc=%d\n", rc);
+
+	rc = register_trace_android_vh_ufs_compl_command(ufs_compl_cmd_handle, NULL);
+	if (rc != 0)
+		pr_err("register_trace_android_vh_ufs_compl_command failed! rc=%d\n", rc);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)
+	rc = register_trace_android_vh_ufs_update_sdev(probe_android_vh_ufs_update_sdev, NULL);
+	if (rc != 0)
+		pr_err("register_trace_android_vh_ufs_update_sdev failed! rc=%d\n", rc);
+#endif /*  */
+	return rc;
+}
+
+static void oplus_ufs_unregist_tracepoint(void)
+{
+	unregister_trace_android_vh_ufs_send_command(ufs_send_cmd_handle, NULL);
+	unregister_trace_android_vh_ufs_compl_command(ufs_compl_cmd_handle, NULL);
+	unregister_trace_android_vh_ufs_update_sdev(probe_android_vh_ufs_update_sdev, NULL);
+}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0)) || defined(CONFIG_OPLUS_QCOM_UFS_DRIVER)
+/**
+ * ufs_oplus_query_ioctl - perform user read queries
+ * @hba: per-adapter instance
+ * @lun: used for lun specific queries
+ * @buffer: user space buffer for reading and submitting query data and params
+ * @return: 0 for success negative error code otherwise
+ *
+ * Expected/Submitted buffer structure is struct ufs_ioctl_query_data.
+ * It will read the opcode, idn and buf_length parameters, and, put the
+ * response in the buffer field while updating the used size in buf_length.
+ */
+static int
+ufs_oplus_query_ioctl(struct ufs_hba *hba, u8 lun, void __user *buffer)
+{
+	struct ufs_ioctl_query_data *ioctl_data;
+	int err = 0;
+	int length = 0;
+	void *data_ptr;
+	bool flag;
+	u32 att;
+	u8 index;
+	u8 *desc = NULL;
+
+	ioctl_data = kzalloc(sizeof(*ioctl_data), GFP_KERNEL);
+	if (!ioctl_data) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	/* extract params from user buffer */
+	err = copy_from_user(ioctl_data, buffer,
+			     sizeof(struct ufs_ioctl_query_data));
+	if (err) {
+		dev_err(hba->dev,
+			"%s: Failed copying buffer from user, err %d\n",
+			__func__, err);
+		goto out_release_mem;
+	}
+
+	/* verify legal parameters & send query */
+	switch (ioctl_data->opcode) {
+	case UPIU_QUERY_OPCODE_READ_DESC:
+		switch (ioctl_data->idn) {
+		case QUERY_DESC_IDN_DEVICE:
+		case QUERY_DESC_IDN_CONFIGURATION:
+		case QUERY_DESC_IDN_INTERCONNECT:
+		case QUERY_DESC_IDN_GEOMETRY:
+		case QUERY_DESC_IDN_POWER:
+		case QUERY_DESC_IDN_HEALTH:
+			index = 0;
+			break;
+		case QUERY_DESC_IDN_UNIT:
+			if (!ufs_is_valid_unit_desc_lun(&hba->dev_info, lun)) {
+				dev_err(hba->dev,
+					"%s: No unit descriptor for lun 0x%x\n",
+					__func__, lun);
+				err = -EINVAL;
+				goto out_release_mem;
+			}
+			index = lun;
+			break;
+		default:
+			goto out_einval;
+		}
+		length = min_t(int, QUERY_DESC_MAX_SIZE,
+			       ioctl_data->buf_size);
+		desc = kzalloc(length, GFP_KERNEL);
+		if (!desc) {
+			dev_err(hba->dev, "%s: Failed allocating %d bytes\n",
+				__func__, length);
+			err = -ENOMEM;
+			goto out_release_mem;
+		}
+		err = ufshcd_query_descriptor_retry(hba, ioctl_data->opcode,
+						    ioctl_data->idn, index, 0,
+						    desc, &length);
+		break;
+	case UPIU_QUERY_OPCODE_READ_ATTR:
+		switch (ioctl_data->idn) {
+		case QUERY_ATTR_IDN_BOOT_LU_EN:
+		case QUERY_ATTR_IDN_POWER_MODE:
+		case QUERY_ATTR_IDN_ACTIVE_ICC_LVL:
+		case QUERY_ATTR_IDN_OOO_DATA_EN:
+		case QUERY_ATTR_IDN_BKOPS_STATUS:
+		case QUERY_ATTR_IDN_PURGE_STATUS:
+		case QUERY_ATTR_IDN_MAX_DATA_IN:
+		case QUERY_ATTR_IDN_MAX_DATA_OUT:
+		case QUERY_ATTR_IDN_REF_CLK_FREQ:
+		case QUERY_ATTR_IDN_CONF_DESC_LOCK:
+		case QUERY_ATTR_IDN_MAX_NUM_OF_RTT:
+		case QUERY_ATTR_IDN_EE_CONTROL:
+		case QUERY_ATTR_IDN_EE_STATUS:
+		case QUERY_ATTR_IDN_SECONDS_PASSED:
+			index = 0;
+			break;
+		case QUERY_ATTR_IDN_DYN_CAP_NEEDED:
+		case QUERY_ATTR_IDN_CORR_PRG_BLK_NUM:
+			index = lun;
+			break;
+		default:
+			goto out_einval;
+		}
+		err = ufshcd_query_attr(hba, ioctl_data->opcode,
+					ioctl_data->idn, index, 0, &att);
+		break;
+
+	case UPIU_QUERY_OPCODE_WRITE_ATTR:
+		err = copy_from_user(&att,
+				     buffer +
+				     sizeof(struct ufs_ioctl_query_data),
+				     sizeof(u32));
+		if (err) {
+			dev_err(hba->dev,
+				"%s: Failed copying buffer from user, err %d\n",
+				__func__, err);
+			goto out_release_mem;
+		}
+
+		switch (ioctl_data->idn) {
+		case QUERY_ATTR_IDN_BOOT_LU_EN:
+			index = 0;
+			if (!att) {
+				dev_err(hba->dev,
+					"%s: Illegal ufs query ioctl data, opcode 0x%x, idn 0x%x, att 0x%x\n",
+					__func__, ioctl_data->opcode,
+					(unsigned int)ioctl_data->idn, att);
+				err = -EINVAL;
+				goto out_release_mem;
+			}
+			break;
+		default:
+			goto out_einval;
+		}
+		err = ufshcd_query_attr(hba, ioctl_data->opcode,
+					ioctl_data->idn, index, 0, &att);
+		break;
+
+	case UPIU_QUERY_OPCODE_READ_FLAG:
+		switch (ioctl_data->idn) {
+		case QUERY_FLAG_IDN_FDEVICEINIT:
+		case QUERY_FLAG_IDN_PERMANENT_WPE:
+		case QUERY_FLAG_IDN_PWR_ON_WPE:
+		case QUERY_FLAG_IDN_BKOPS_EN:
+		case QUERY_FLAG_IDN_PURGE_ENABLE:
+		case QUERY_FLAG_IDN_FPHYRESOURCEREMOVAL:
+		case QUERY_FLAG_IDN_BUSY_RTC:
+			break;
+		default:
+			goto out_einval;
+		}
+		err = ufshcd_query_flag(hba, ioctl_data->opcode,
+					ioctl_data->idn, 0, &flag);
+		break;
+	default:
+		goto out_einval;
+	}
+
+	if (err) {
+		dev_err(hba->dev, "%s: Query for idn %d failed\n", __func__,
+			ioctl_data->idn);
+		goto out_release_mem;
+	}
+
+	/*
+	 * copy response data
+	 * As we might end up reading less data than what is specified in
+	 * "ioctl_data->buf_size". So we are updating "ioctl_data->
+	 * buf_size" to what exactly we have read.
+	 */
+	switch (ioctl_data->opcode) {
+	case UPIU_QUERY_OPCODE_READ_DESC:
+		ioctl_data->buf_size = min_t(int, ioctl_data->buf_size, length);
+		data_ptr = desc;
+		break;
+	case UPIU_QUERY_OPCODE_READ_ATTR:
+		ioctl_data->buf_size = sizeof(u32);
+		data_ptr = &att;
+		break;
+	case UPIU_QUERY_OPCODE_READ_FLAG:
+		ioctl_data->buf_size = 1;
+		data_ptr = &flag;
+		break;
+	case UPIU_QUERY_OPCODE_WRITE_ATTR:
+		goto out_release_mem;
+	default:
+		goto out_einval;
+	}
+
+	/* copy to user */
+	err = copy_to_user(buffer, ioctl_data,
+			   sizeof(struct ufs_ioctl_query_data));
+	if (err)
+		dev_err(hba->dev, "%s: Failed copying back to user.\n",
+			__func__);
+	err = copy_to_user(buffer + sizeof(struct ufs_ioctl_query_data),
+			   data_ptr, ioctl_data->buf_size);
+	if (err)
+		dev_err(hba->dev, "%s: err %d copying back to user.\n",
+			__func__, err);
+	goto out_release_mem;
+
+out_einval:
+	dev_err(hba->dev,
+		"%s: illegal ufs query ioctl data, opcode 0x%x, idn 0x%x\n",
+		__func__, ioctl_data->opcode, (unsigned int)ioctl_data->idn);
+	err = -EINVAL;
+out_release_mem:
+	kfree(ioctl_data);
+	kfree(desc);
+out:
+	return err;
+}
+
+
+static int
+ufs_oplus_ioctl(struct scsi_device *dev, unsigned int cmd, void __user *buffer)
+{
+	struct ufs_hba *hba = shost_priv(dev->host);
+	int err = 0;
+
+	if (!hba)
+		return -ENOTTY;
+	if (!buffer) {
+		dev_err(hba->dev, "%s: User buffer is NULL!\n", __func__);
+		return -EINVAL;
+	}
+
+	switch (cmd) {
+	case UFS_IOCTL_QUERY:
+		down(&hba->host_sem);
+		if (!ufshcd_is_user_access_allowed(hba)) {
+			up(&hba->host_sem);
+			err = -EBUSY;
+			goto err_out;
+		}
+		ufshcd_rpm_get_sync(hba);
+		err = ufs_oplus_query_ioctl(hba,
+					   ufshcd_scsi_to_upiu_lun(dev->lun),
+					   buffer);
+		ufshcd_rpm_put_sync(hba);
+		up(&hba->host_sem);
+	break;
+	case UFS_IOCTL_MONITOR:
+		ufshcd_rpm_get_sync(hba);
+		err = ufs_ioctl_monitor(dev, buffer);
+		ufshcd_rpm_put_sync(hba);
+	break;
+	default:
+		err = -ENOIOCTLCMD;
+		dev_err(hba->dev, "%s: Unsupported ioctl cmd %d\n", __func__,
+			cmd);
+	break;
+	}
+
+err_out:
+	return err;
+}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0) || defined(CONFIG_OPLUS_QCOM_UFS_DRIVER) */
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0))
+static void ufs_update_sdev(struct scsi_device *sdev)
+{
+	async_schedule(create_devinfo_ufs, sdev);
+}
+
+static long ufs_common_oplus_ioctl (struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct scsi_device *dev = pde_data(file_inode(filp));
+	long err = 0;
+
+	if (dev == NULL)
+		return -ENOTTY;
+
+	if (_IOC_TYPE(cmd) != UFS_OPLUS_IOCTL_MAGIC)
+		return -ENOTTY;
+
+	switch (cmd)
+	{
+		case UFS_OPLUS_QUERY_IOCTL:
+			err = ufs_oplus_ioctl(dev, UFS_IOCTL_QUERY, (void *)arg);
+		break;
+		case UFS_OPLUS_MONITOR_IOCTL:
+			err = ufs_oplus_ioctl(dev, UFS_IOCTL_MONITOR, (void *)arg);
+		break;
+	}
+	return err;
+}
+
+static struct proc_ops proc_ioctl_fops = {
+	.proc_ioctl = ufs_common_oplus_ioctl,
+};
+
+static void ufs_oplus_ioctl_init(struct scsi_device *sdev) {
+	struct proc_dir_entry *oplus_ufs_proc_dir = proc_mkdir("ufs_oplus_dir", NULL);
+	struct proc_dir_entry *d_entry;
+
+	if (!oplus_ufs_proc_dir)
+		return;
+
+	d_entry = proc_create_data("ufs_oplus_ioctl", 0644, oplus_ufs_proc_dir, &proc_ioctl_fops, sdev);
+	if (!d_entry)
+		return;
+	return;
+}
+
+void ufs_oplus_init_sdev(struct scsi_device *sdev) {
+	if (scsi_is_wlun(sdev->lun))
+            return;
+
+	if (atomic_inc_return(&ufs_init_done) == 1) {
+        	ufs_update_sdev(sdev);
+		ufs_oplus_ioctl_init(sdev);
+	}
+}
+EXPORT_SYMBOL_GPL(ufs_oplus_init_sdev);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0) */
+
+void ufs_init_oplus_dbg(struct ufs_hba *hba)
+{
+	oplus_ufs_regist_tracepoint();
+	ufshcd_transmission_status_init_sysfs(hba);
+	create_signal_quality_proc(&signalCtrl);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 6, 0)
+#ifdef CONFIG_OPLUS_QCOM_UFS_DRIVER
+	hba->host->hostt->ioctl = (int (*)(struct scsi_device *, unsigned int,
+				   void __user *))ufs_oplus_ioctl;
+#ifdef CONFIG_COMPAT
+	hba->host->hostt->compat_ioctl = (int (*)(struct scsi_device *,
+					  unsigned int,
+					  void __user *))ufs_oplus_ioctl;
+#endif /* CONFIG_COMPAT */
+#endif /* CONFIG_OPLUS_QCOM_UFS_DRIVER */
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0) */
+}
+EXPORT_SYMBOL_GPL(ufs_init_oplus_dbg);
+
+void ufs_remove_oplus_dbg(void)
+{
+	oplus_ufs_unregist_tracepoint();
+	remove_signal_quality_proc(&signalCtrl);
+}
+EXPORT_SYMBOL_GPL(ufs_remove_oplus_dbg);
+
+
+static void __exit ufs_oplus_dbg_exit(void)
+{
+	return;
+}
+
+static int __init ufs_oplus_dbg_init(void)
+{
+	atomic_set(&ufs_init_done, 0);
+	return 0;
+}
+
+module_init(ufs_oplus_dbg_init)
+module_exit(ufs_oplus_dbg_exit)
+
+MODULE_DESCRIPTION("Oplus UFS Debugging Facility");
+MODULE_AUTHOR("oplus");
+MODULE_LICENSE("GPL v2");
+
+
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/ufs-oplus-dbg.h b/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/ufs-oplus-dbg.h
new file mode 100644
index 000000000..14cef8726
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/ufs_oplus_dbg/ufs-oplus-dbg.h
@@ -0,0 +1,209 @@
+#ifndef _UFS_OPLUS_DBG_H
+#define _UFS_OPLUS_DBG_H
+
+#include <linux/reset-controller.h>
+#include <linux/reset.h>
+#include <linux/phy/phy.h>
+#include <linux/pm_qos.h>
+#include <linux/notifier.h>
+#include <linux/panic_notifier.h>
+#include <linux/version.h>
+#include <ufs/ufshcd.h>
+#include <ufs/unipro.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0) || defined(CONFIG_OPLUS_QCOM_UFS_DRIVER)
+#define UFS_IOCTL_QUERY			0x5388
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0) || defined(CONFIG_OPLUS_QCOM_UFS_DRIVER) */
+#define UFS_IOCTL_MONITOR               0x5392  /* For monitor access */
+
+/*define ufs uic error code*/
+enum unipro_pa_errCode {
+	UNIPRO_PA_LANE0_ERR_CNT,
+	UNIPRO_PA_LANE1_ERR_CNT,
+	UNIPRO_PA_LANE2_ERR_CNT,
+	UNIPRO_PA_LANE3_ERR_CNT,
+	UNIPRO_PA_LINE_RESET,
+	UNIPRO_PA_ERR_MAX
+};
+
+enum unipro_dl_errCode {
+	UNIPRO_DL_NAC_RECEIVED,
+	UNIPRO_DL_TCX_REPLAY_TIMER_EXPIRED,
+	UNIPRO_DL_AFCX_REQUEST_TIMER_EXPIRED,
+	UNIPRO_DL_FCX_PROTECTION_TIMER_EXPIRED,
+	UNIPRO_DL_CRC_ERROR,
+	UNIPRO_DL_RX_BUFFER_OVERFLOW,
+	UNIPRO_DL_MAX_FRAME_LENGTH_EXCEEDED,
+	UNIPRO_DL_WRONG_SEQUENCE_NUMBER,
+	UNIPRO_DL_AFC_FRAME_SYNTAX_ERROR,
+	UNIPRO_DL_NAC_FRAME_SYNTAX_ERROR,
+	UNIPRO_DL_EOF_SYNTAX_ERROR,
+	UNIPRO_DL_FRAME_SYNTAX_ERROR,
+	UNIPRO_DL_BAD_CTRL_SYMBOL_TYPE,
+	UNIPRO_DL_PA_INIT_ERROR,
+	UNIPRO_DL_PA_ERROR_IND_RECEIVED,
+	UNIPRO_DL_PA_INIT,
+	UNIPRO_DL_ERR_MAX
+};
+
+enum unipro_nl_errCode {
+	UNIPRO_NL_UNSUPPORTED_HEADER_TYPE,
+	UNIPRO_NL_BAD_DEVICEID_ENC,
+	UNIPRO_NL_LHDR_TRAP_PACKET_DROPPING,
+	UNIPRO_NL_ERR_MAX
+};
+
+enum unipro_tl_errCode {
+	UNIPRO_TL_UNSUPPORTED_HEADER_TYPE,
+	UNIPRO_TL_UNKNOWN_CPORTID,
+	UNIPRO_TL_NO_CONNECTION_RX,
+	UNIPRO_TL_CONTROLLED_SEGMENT_DROPPING,
+	UNIPRO_TL_BAD_TC,
+	UNIPRO_TL_E2E_CREDIT_OVERFLOW,
+	UNIPRO_TL_SAFETY_VALVE_DROPPING,
+	UNIPRO_TL_ERR_MAX
+};
+
+enum unipro_dme_errCode {
+	UNIPRO_DME_GENERIC,
+	UNIPRO_DME_TX_QOS,
+	UNIPRO_DME_RX_QOS,
+	UNIPRO_DME_PA_INIT_QOS,
+	UNIPRO_DME_ERR_MAX
+};
+
+enum unipro_err_time_stamp {
+	UNIPRO_0_STAMP,
+	UNIPRO_1_STAMP,
+	UNIPRO_2_STAMP,
+	UNIPRO_3_STAMP,
+	UNIPRO_4_STAMP,
+	UNIPRO_5_STAMP,
+	UNIPRO_6_STAMP,
+	UNIPRO_7_STAMP,
+	UNIPRO_8_STAMP,
+	UNIPRO_9_STAMP,
+	STAMP_RECORD_MAX
+};
+#define STAMP_MIN_INTERVAL ((ktime_t)600000000000) /*ns, 10min*/
+
+struct signal_quality {
+	u32 ufs_device_err_cnt;
+	u32 ufs_host_err_cnt;
+	u32 ufs_bus_err_cnt;
+	u32 ufs_crypto_err_cnt;
+	u32 ufs_link_lost_cnt;
+	u32 task_abort_cnt;
+	u32 host_reset_cnt;
+	u32 dev_reset_cnt;
+	u32 unipro_PA_err_total_cnt;
+	u32 unipro_PA_err_cnt[UNIPRO_PA_ERR_MAX];
+	u32 unipro_DL_err_total_cnt;
+	u32 unipro_DL_err_cnt[UNIPRO_DL_ERR_MAX];
+	u32 unipro_NL_err_total_cnt;
+	u32 unipro_NL_err_cnt[UNIPRO_NL_ERR_MAX];
+	u32 unipro_TL_err_total_cnt;
+	u32 unipro_TL_err_cnt[UNIPRO_TL_ERR_MAX];
+	u32 unipro_DME_err_total_cnt;
+	u32 unipro_DME_err_cnt[UNIPRO_DME_ERR_MAX];
+	u32 gear_err_cnt[UFS_HS_G5 + 1];
+	/* first 10 error cnt, interval is 10min at least */
+	ktime_t stamp[STAMP_RECORD_MAX];
+	int stamp_pos;
+};
+
+struct unipro_signal_quality_ctrl {
+	struct proc_dir_entry *ctrl_dir;
+	struct signal_quality record;
+	struct signal_quality record_upload;
+};
+
+struct ufs_transmission_status_t
+{
+	u8  transmission_status_enable;
+
+	u64 gear_min_write_sec;
+	u64 gear_max_write_sec;
+	u64 gear_min_read_sec;
+	u64 gear_max_read_sec;
+
+	u64 gear_min_write_us;
+	u64 gear_max_write_us;
+	u64 gear_min_read_us;
+	u64 gear_max_read_us;
+
+	u64 gear_min_dev_us;
+	u64 gear_max_dev_us;
+
+	u64 gear_min_other_sec;
+	u64 gear_max_other_sec;
+	u64 gear_min_other_us;
+	u64 gear_max_other_us;
+
+	u64 scsi_send_count;
+	u64 dev_cmd_count;
+
+	u64 active_count;
+	u64 active_time;
+	u64 resume_timing;
+
+	u64 sleep_count;
+	u64 sleep_time;
+	u64 suspend_timing;
+
+	u64 powerdown_count;
+	u64 powerdown_time;
+
+	u64 power_total_count;
+	u32 current_pwr_mode;
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0)
+#ifndef CONFIG_OPLUS_QCOM_UFS_DRIVER
+struct ufs_ioctl_query_data {
+ 	/*
+	 * User should select one of the opcode defined in "enum query_opcode".
+	 * Please check include/uapi/scsi/ufs/ufs.h for the definition of it.
+	 * Note that only UPIU_QUERY_OPCODE_READ_DESC,
+	 * UPIU_QUERY_OPCODE_READ_ATTR & UPIU_QUERY_OPCODE_READ_FLAG are
+	 * supported as of now. All other query_opcode would be considered
+	 * invalid.
+ 	 * As of now only read query operations are supported.
+	 */
+	__u32 opcode;
+	/*
+	 * User should select one of the idn from "enum flag_idn" or "enum
+	 * attr_idn" or "enum desc_idn" based on whether opcode above is
+	 * attribute, flag or descriptor.
+	 * Please check include/uapi/scsi/ufs/ufs.h for the definition of it.
+	 */
+	__u8 idn;
+	/*
+	 * User should specify the size of the buffer (buffer[0] below) where
+	 * it wants to read the query data (attribute/flag/descriptor).
+ 	 * As we might end up reading less data then what is specified in
+	 * buf_size. So we are updating buf_size to what exactly we have read.
+	 */
+	__u16 buf_size;
+	/*
+	 * placeholder for the start of the data buffer where kernel will copy
+	 * the query data (attribute/flag/descriptor) read from the UFS device
+	 * Note:
+	 * For Read/Write Attribute you will have to allocate 4 bytes
+	 * For Read/Write Flag you will have to allocate 1 byte
+	 */
+	__u8 buffer[0];
+};
+#endif
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0) */
+
+void ufs_active_time_get(struct ufs_hba *hba);
+void ufs_sleep_time_get(struct ufs_hba *hba);
+void recordSignalerr(struct ufs_hba *hba, unsigned int val, enum ufs_event_type evt);
+void ufs_init_oplus_dbg(struct ufs_hba *hba);
+void ufs_remove_oplus_dbg(void);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0)
+void ufs_oplus_init_sdev(struct scsi_device *sdev);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0) */
+
+#endif /* !_UFS_OPLUS_DBG_H */
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/Kconfig b/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/Kconfig
new file mode 100644
index 000000000..04032d19c
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/Kconfig
@@ -0,0 +1,6 @@
+config OPLUS_FEATURE_WQ_DYNPRIO
+    tristate "workqueue dynamic priority"
+    default n
+    help
+      define this config to modify workqueue priority for gki.
+
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/Makefile b/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/Makefile
new file mode 100644
index 000000000..87f24ade6
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/Makefile
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: GPL-2.0-only
+# Copyright (C) 2024 Oplus. All rights reserved.
+
+LINUXINCLUDE += -I$(srctree)/
+obj-$(CONFIG_OPLUS_FEATURE_WQ_DYNPRIO) += oplus_wq_dynamic_priority.o
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/oplus_wq_dynamic_priority.c b/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/oplus_wq_dynamic_priority.c
new file mode 100644
index 000000000..8c7ca8def
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/oplus_wq_dynamic_priority.c
@@ -0,0 +1,236 @@
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/kprobes.h>
+#include <linux/string.h>
+#include <linux/sched.h>
+
+#include <../kernel/oplus_cpu/sched/sched_assist/sa_common.h>
+#include "oplus_wq_dynamic_priority.h"
+
+#define WQ_UX    (1 << 14)
+
+#define VIRTUAL_KWORKER_NICE (-1000)
+
+/* ---------alloc_workqueue--------- */
+struct config_wq_flags {
+    char *target_str;
+    unsigned int new_flags;
+};
+
+static struct config_wq_flags oplus_wq_config[] = {
+    { "loop", WQ_UNBOUND | WQ_FREEZABLE | WQ_HIGHPRI | WQ_UX },
+    { "kverityd", WQ_MEM_RECLAIM | WQ_HIGHPRI | WQ_UX | WQ_UNBOUND },
+    // Add more strings and flags as needed.
+    { NULL, 0 } // Terminate array with NULL
+};
+
+static int handler_alloc_workqueue_pre(struct kprobe *p, struct pt_regs *regs)
+{
+    const char *fmt = (const char *)regs->regs[0];
+    unsigned int flags = (unsigned int)regs->regs[1];
+
+    struct config_wq_flags *item = oplus_wq_config;
+    if(fmt) {
+        while (item->target_str) {
+            if (!strncmp(fmt, item->target_str, strlen(item->target_str)) && (item->new_flags != flags)) {
+                printk(KERN_INFO "alloc_workqueue: matching fmt '%s', modifying flags from 0x%x to 0x%x\n", fmt, flags, item->new_flags);
+                regs->regs[1] = item->new_flags;
+                break;
+            }
+            item++;
+        }
+    }
+    return 0;
+}
+
+static struct kprobe oplus_alloc_workqueue_kp = {
+    .symbol_name = "alloc_workqueue",
+    .pre_handler = handler_alloc_workqueue_pre,
+};
+
+/* ---------alloc_unbound_pwq--------- */
+static int handler_alloc_unbound_pwq_pre(struct kprobe *p, struct pt_regs *regs)
+{
+    struct workqueue_struct *wq = (struct workqueue_struct *)regs->regs[0];
+    struct workqueue_attrs *attrs = (struct workqueue_attrs *)regs->regs[1];
+
+    int old_nice=0;
+
+    if (wq && (wq->flags & WQ_UX)) {
+        if (attrs) {
+            old_nice = attrs->nice;
+            if (old_nice != VIRTUAL_KWORKER_NICE) {
+                attrs->nice = VIRTUAL_KWORKER_NICE;
+                printk(KERN_INFO "alloc_unbound_pwq: modifying nice from %d to %d\n", old_nice, attrs->nice);
+            }
+        }
+    }
+    return 0;
+}
+
+static struct kprobe oplus_alloc_unbound_pwq_kp = {
+    .symbol_name = "alloc_unbound_pwq",
+    .pre_handler = handler_alloc_unbound_pwq_pre,
+};
+
+/* ---------apply_wqattrs_prepare--------- */
+static int handler_apply_wqattrs_ret(struct kretprobe_instance *ri, struct pt_regs *regs)
+{
+    struct apply_wqattrs_ctx *ctx = (struct apply_wqattrs_ctx *)regs_return_value(regs);
+    if (ctx && (ctx->wq) && (ctx->wq->flags & WQ_UX)) {
+        printk(KERN_INFO "apply_wqattrs_prepare: modifying nice from %d to %d\n", ctx->attrs->nice, VIRTUAL_KWORKER_NICE);
+        ctx->attrs->nice = VIRTUAL_KWORKER_NICE;
+    }
+    return 0;
+}
+
+static struct kretprobe oplus_apply_wqattrs_krp = {
+    .kp = {
+        .symbol_name = "apply_wqattrs_prepare",
+    },
+    .handler = handler_apply_wqattrs_ret,
+};
+
+/* ---------worker_attach_to_pool--------- */
+static int handler_worker_attach_to_pool_pre(struct kprobe *p, struct pt_regs *regs)
+{
+    struct worker *worker = (struct worker *)regs->regs[0];
+    struct worker_pool *pool = (struct worker_pool *)regs->regs[1];
+
+    if ((worker && worker->task) && (pool && pool->attrs)) {
+        if (pool->attrs->nice == VIRTUAL_KWORKER_NICE) {
+        #ifdef CONFIG_OPLUS_SYSTEM_KERNEL_QCOM
+            oplus_set_ux_state_lock(worker->task, SA_TYPE_LIGHT, -1, true);
+            printk(KERN_INFO "worker_attach_to_pool:comm:%s set UX and set nice to %d\n", worker->task->comm, MIN_NICE);
+        #else
+            sched_set_fifo_low(worker->task);
+            printk(KERN_INFO "worker_attach_to_pool:comm:%s set RT and set nice to %d\n", worker->task->comm, MIN_NICE);
+        #endif /* CONFIG_OPLUS_SYSTEM_KERNEL_QCOM */
+            set_user_nice(worker->task, MIN_NICE);
+        }
+    }
+    return 0;
+}
+
+static struct kprobe oplus_worker_attach_to_pool_kp = {
+    .symbol_name = "worker_attach_to_pool",
+    .pre_handler = handler_worker_attach_to_pool_pre,
+};
+
+
+/* ---------kblockd_schedule_work--------- */
+static struct workqueue_struct *oplus_kblockd_workqueue;
+static int handler_kblockd_schedule_work_pre(struct kprobe *p, struct pt_regs *regs) {
+    //printk(KERN_INFO "kblockd_schedule_work:use oplus kblockd workqueue\n");
+    regs->regs[1] = (u64)oplus_kblockd_workqueue;
+    return 0;
+}
+
+static int handler_kblockd_mod_delayed_work_on_pre(struct kprobe *p, struct pt_regs *regs) {
+    //printk(KERN_INFO "kblockd_mod_delayed_work_on:use oplus kblockd workqueue\n");
+    regs->regs[1] = (u64)oplus_kblockd_workqueue;
+    return 0;
+}
+
+static struct kprobe oplus_kblockd_schedule_work_kp = {
+    .symbol_name = "kblockd_schedule_work",
+    .offset = 0x1c,
+    .pre_handler = handler_kblockd_schedule_work_pre,
+};
+
+static struct kprobe oplus_kblockd_mod_delayed_work_on_kp = {
+    .symbol_name = "kblockd_mod_delayed_work_on",
+    .offset = 0x1c,
+    .pre_handler = handler_kblockd_mod_delayed_work_on_pre,
+};
+
+static bool kprobe_init_successful=false;
+static int __init oplus_wq_kprobe_init(void)
+{
+    int ret;
+
+    ret = register_kprobe(&oplus_alloc_workqueue_kp);
+    if (ret < 0) {
+        printk(KERN_ERR "register_kprobe alloc_workqueue failed, returned %d\n", ret);
+        goto kp_alloc_workqueue_fail;
+    }
+
+    ret = register_kprobe(&oplus_alloc_unbound_pwq_kp);
+    if (ret < 0) {
+        printk(KERN_ERR "register_kprobe alloc_unbound_pwq failed, returned %d\n", ret);
+        goto kp_alloc_unbound_pwq_fail;
+    }
+
+    ret = register_kretprobe(&oplus_apply_wqattrs_krp);
+    if (ret < 0) {
+        printk(KERN_ERR "register_kretprobe apply_wqattrs failed, returned %d\n", ret);
+        goto kp_apply_wqattrs_fail;
+    }
+
+    ret = register_kprobe(&oplus_worker_attach_to_pool_kp);
+    if (ret < 0) {
+        printk(KERN_ERR "register_kprobe worker_attach_to_pool failed, returned %d\n", ret);
+        goto kp_worker_attach_to_pool_fail;
+    }
+
+    oplus_kblockd_workqueue = alloc_workqueue("opluskblockd",WQ_MEM_RECLAIM | WQ_HIGHPRI | WQ_UX | WQ_UNBOUND, 0);
+
+    if (!oplus_kblockd_workqueue) {
+        printk(KERN_ERR "alloc  oplus_kblockd_workqueue fail!\n");
+        goto kp_kblockd_schedule_work_kp_fail;
+    }
+
+    ret = register_kprobe(&oplus_kblockd_schedule_work_kp);
+    if (ret < 0) {
+        printk(KERN_ERR "register_kprobe oplus_kblockd_schedule_work_kp failed, returned %d\n", ret);
+        goto kp_kblockd_schedule_work_kp_fail;
+    }
+
+    ret = register_kprobe(&oplus_kblockd_mod_delayed_work_on_kp);
+    if (ret < 0) {
+        printk(KERN_ERR "register_kprobe oplus_kblockd_mod_delayed_work_on_kp failed, returned %d\n", ret);
+        goto kp_kblockd_mod_delayed_work_on_kp_fail;
+    }
+
+    printk(KERN_ERR "%s successful!\n", __func__);
+    kprobe_init_successful = true;
+    return 0;
+
+kp_kblockd_mod_delayed_work_on_kp_fail:
+    unregister_kprobe(&oplus_kblockd_schedule_work_kp);
+kp_kblockd_schedule_work_kp_fail:
+    unregister_kprobe(&oplus_worker_attach_to_pool_kp);
+kp_worker_attach_to_pool_fail:
+    unregister_kretprobe(&oplus_apply_wqattrs_krp);
+kp_apply_wqattrs_fail:
+    unregister_kprobe(&oplus_alloc_unbound_pwq_kp);
+kp_alloc_unbound_pwq_fail:
+    unregister_kprobe(&oplus_alloc_workqueue_kp);
+kp_alloc_workqueue_fail:
+
+    kprobe_init_successful = false;
+    return ret;
+}
+
+static void __exit oplus_wq_kprobe_exit(void)
+{
+	//工作队列未销毁
+    if(kprobe_init_successful) {
+        unregister_kprobe(&oplus_alloc_workqueue_kp);
+        unregister_kprobe(&oplus_alloc_unbound_pwq_kp);
+        unregister_kretprobe(&oplus_apply_wqattrs_krp);
+        unregister_kprobe(&oplus_worker_attach_to_pool_kp);
+        unregister_kprobe(&oplus_kblockd_schedule_work_kp);
+        unregister_kprobe(&oplus_kblockd_mod_delayed_work_on_kp);
+        printk(KERN_INFO "kprobe unregistered\n");
+    } else {
+        printk(KERN_INFO "kprobe needn't unregistered\n");
+    }
+}
+
+module_init(oplus_wq_kprobe_init);
+module_exit(oplus_wq_kprobe_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("lijiang");
+MODULE_DESCRIPTION("A kernel module using kprobe to hook alloc_workqueue function");
diff --git a/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/oplus_wq_dynamic_priority.h b/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/oplus_wq_dynamic_priority.h
new file mode 100644
index 000000000..ab20fafd2
--- /dev/null
+++ b/drivers/soc/oplus/storage/storage_feature_in_module/common/wq_dynamic_priority/oplus_wq_dynamic_priority.h
@@ -0,0 +1,174 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Copyright (C) 2024 Oplus. All rights reserved.
+ */
+
+#ifndef _OPLUS_WQ_DYNAMIC_PRIORITY_H_
+#define _OPLUS_WQ_DYNAMIC_PRIORITY_H_
+
+#include <linux/hashtable.h>
+#include <linux/workqueue.h>
+#include <kernel/workqueue_internal.h>
+
+/* copy from workqueue.c */
+enum {
+	/*
+	 * worker_pool flags
+	 *
+	 * A bound pool is either associated or disassociated with its CPU.
+	 * While associated (!DISASSOCIATED), all workers are bound to the
+	 * CPU and none has %WORKER_UNBOUND set and concurrency management
+	 * is in effect.
+	 *
+	 * While DISASSOCIATED, the cpu may be offline and all workers have
+	 * %WORKER_UNBOUND set and concurrency management disabled, and may
+	 * be executing on any CPU.  The pool behaves as an unbound one.
+	 *
+	 * Note that DISASSOCIATED should be flipped only while holding
+	 * wq_pool_attach_mutex to avoid changing binding state while
+	 * worker_attach_to_pool() is in progress.
+	 */
+	POOL_MANAGER_ACTIVE	= 1 << 0,	/* being managed */
+	POOL_DISASSOCIATED	= 1 << 2,	/* cpu can't serve workers */
+
+	/* worker flags */
+	WORKER_DIE		= 1 << 1,	/* die die die */
+	WORKER_IDLE		= 1 << 2,	/* is idle */
+	WORKER_PREP		= 1 << 3,	/* preparing to run works */
+	WORKER_CPU_INTENSIVE	= 1 << 6,	/* cpu intensive */
+	WORKER_UNBOUND		= 1 << 7,	/* worker is unbound */
+	WORKER_REBOUND		= 1 << 8,	/* worker was rebound */
+
+	WORKER_NOT_RUNNING	= WORKER_PREP | WORKER_CPU_INTENSIVE |
+				  WORKER_UNBOUND | WORKER_REBOUND,
+
+	NR_STD_WORKER_POOLS	= 2,		/* # standard pools per cpu */
+
+	UNBOUND_POOL_HASH_ORDER	= 6,		/* hashed by pool->attrs */
+	BUSY_WORKER_HASH_ORDER	= 6,		/* 64 pointers */
+
+	MAX_IDLE_WORKERS_RATIO	= 4,		/* 1/4 of busy can be idle */
+	IDLE_WORKER_TIMEOUT	= 300 * HZ,	/* keep idle ones for 5 mins */
+
+	MAYDAY_INITIAL_TIMEOUT  = HZ / 100 >= 2 ? HZ / 100 : 2,
+						/* call for help after 10ms
+						   (min two ticks) */
+	MAYDAY_INTERVAL		= HZ / 10,	/* and then every 100ms */
+	CREATE_COOLDOWN		= HZ,		/* time to breath after fail */
+
+	/*
+	 * Rescue workers are used only on emergencies and shared by
+	 * all cpus.  Give MIN_NICE.
+	 */
+	RESCUER_NICE_LEVEL	= MIN_NICE,
+	HIGHPRI_NICE_LEVEL	= MIN_NICE,
+
+	WQ_NAME_LEN		= 24,
+};
+
+struct worker_pool {
+	raw_spinlock_t		lock;		/* the pool lock */
+	int			cpu;		/* I: the associated cpu */
+	int			node;		/* I: the associated node ID */
+	int			id;		/* I: pool ID */
+	unsigned int		flags;		/* L: flags */
+
+	unsigned long		watchdog_ts;	/* L: watchdog timestamp */
+	bool			cpu_stall;	/* WD: stalled cpu bound pool */
+
+	/*
+	 * The counter is incremented in a process context on the associated CPU
+	 * w/ preemption disabled, and decremented or reset in the same context
+	 * but w/ pool->lock held. The readers grab pool->lock and are
+	 * guaranteed to see if the counter reached zero.
+	 */
+	int			nr_running;
+
+	struct list_head	worklist;	/* L: list of pending works */
+
+	int			nr_workers;	/* L: total number of workers */
+	int			nr_idle;	/* L: currently idle workers */
+
+	struct list_head	idle_list;	/* L: list of idle workers */
+	struct timer_list	idle_timer;	/* L: worker idle timeout */
+	struct work_struct      idle_cull_work; /* L: worker idle cleanup */
+
+	struct timer_list	mayday_timer;	  /* L: SOS timer for workers */
+
+	/* a workers is either on busy_hash or idle_list, or the manager */
+	DECLARE_HASHTABLE(busy_hash, BUSY_WORKER_HASH_ORDER);
+						/* L: hash of busy workers */
+
+	struct worker		*manager;	/* L: purely informational */
+	struct list_head	workers;	/* A: attached workers */
+	struct list_head        dying_workers;  /* A: workers about to die */
+	struct completion	*detach_completion; /* all workers detached */
+
+	struct ida		worker_ida;	/* worker IDs for task name */
+
+	struct workqueue_attrs	*attrs;		/* I: worker attributes */
+	struct hlist_node	hash_node;	/* PL: unbound_pool_hash node */
+	int			refcnt;		/* PL: refcnt for unbound pools */
+
+	/*
+	 * Destruction of pool is RCU protected to allow dereferences
+	 * from get_work_pool().
+	 */
+	struct rcu_head		rcu;
+};
+
+struct workqueue_struct {
+	struct list_head	pwqs;		/* WR: all pwqs of this wq */
+	struct list_head	list;		/* PR: list of all workqueues */
+
+	struct mutex		mutex;		/* protects this wq */
+	int			work_color;	/* WQ: current work color */
+	int			flush_color;	/* WQ: current flush color */
+	atomic_t		nr_pwqs_to_flush; /* flush in progress */
+	struct wq_flusher	*first_flusher;	/* WQ: first flusher */
+	struct list_head	flusher_queue;	/* WQ: flush waiters */
+	struct list_head	flusher_overflow; /* WQ: flush overflow list */
+
+	struct list_head	maydays;	/* MD: pwqs requesting rescue */
+	struct worker		*rescuer;	/* MD: rescue worker */
+
+	int			nr_drainers;	/* WQ: drain in progress */
+	int			saved_max_active; /* WQ: saved pwq max_active */
+
+	struct workqueue_attrs	*unbound_attrs;	/* PW: only for unbound wqs */
+	struct pool_workqueue	*dfl_pwq;	/* PW: only for unbound wqs */
+
+#ifdef CONFIG_SYSFS
+	struct wq_device	*wq_dev;	/* I: for sysfs interface */
+#endif
+#ifdef CONFIG_LOCKDEP
+	char			*lock_name;
+	struct lock_class_key	key;
+	struct lockdep_map	lockdep_map;
+#endif
+	char			name[WQ_NAME_LEN]; /* I: workqueue name */
+
+	/*
+	 * Destruction of workqueue_struct is RCU protected to allow walking
+	 * the workqueues list without grabbing wq_pool_mutex.
+	 * This is used to dump all workqueues from sysrq.
+	 */
+	struct rcu_head		rcu;
+
+	/* hot fields used during command issue, aligned to cacheline */
+	unsigned int		flags ____cacheline_aligned; /* WQ: WQ_* flags */
+	struct pool_workqueue __percpu __rcu **cpu_pwq; /* I: per-cpu pwqs */
+};
+
+/* context to store the prepared attrs & pwqs before applying */
+struct apply_wqattrs_ctx {
+	struct workqueue_struct	*wq;		/* target workqueue */
+	struct workqueue_attrs	*attrs;		/* attrs to apply */
+	struct list_head	list;		/* queued for batching commit */
+	struct pool_workqueue	*dfl_pwq;
+	struct pool_workqueue	*pwq_tbl[];
+};
+
+
+
+#endif /* _OPLUS_WQ_DYNAMIC_PRIORITY_H_ */
diff --git a/fs/f2fs/Kconfig b/fs/f2fs/Kconfig
index 03ef08753..896352f83 100644
--- a/fs/f2fs/Kconfig
+++ b/fs/f2fs/Kconfig
@@ -99,6 +99,13 @@ config F2FS_FS_COMPRESSION
 	  Enable filesystem-level compression on f2fs regular files,
 	  multiple back-end compression algorithms are supported.
 
+config F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	bool "F2FS compression feature (fixed output)"
+	depends on F2FS_FS_COMPRESSION
+	default y
+	help
+	  Enable fixed-output compression.
+
 config F2FS_FS_LZO
 	bool "LZO compression support"
 	depends on F2FS_FS_COMPRESSION
@@ -150,3 +157,22 @@ config F2FS_UNFAIR_RWSEM
 	help
 	  Use unfair rw_semaphore, if system configured IO priority by block
 	  cgroup.
+
+config F2FS_APPBOOST
+       bool
+       default y
+       help
+         Enable Appboost
+
+config F2FS_FS_DEDUP
+	bool "F2FS dedup feature"
+	depends on F2FS_FS
+	default y
+	help
+	  Enable the file dedup function.
+
+config F2FS_SEQZONE
+	bool "F2FS SeqZone feature"
+	depends on F2FS_FS && FS_ENCRYPTION_INLINE_CRYPT && F2FS_FS_DEDUP
+	help
+	  Improve Random Write Speed when inlinecrypt is enabled.
diff --git a/fs/f2fs/Makefile b/fs/f2fs/Makefile
index 8a7322d22..183c96303 100644
--- a/fs/f2fs/Makefile
+++ b/fs/f2fs/Makefile
@@ -10,3 +10,6 @@ f2fs-$(CONFIG_F2FS_FS_POSIX_ACL) += acl.o
 f2fs-$(CONFIG_FS_VERITY) += verity.o
 f2fs-$(CONFIG_F2FS_FS_COMPRESSION) += compress.o
 f2fs-$(CONFIG_F2FS_IOSTAT) += iostat.o
+ifeq ($(CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT),y)
+f2fs-$(CONFIG_ARM64) += $(addprefix lz4armv8/, lz4accel.o lz4armv8.o)
+endif
diff --git a/fs/f2fs/checkpoint.c b/fs/f2fs/checkpoint.c
index ad1a31911..cad89452f 100644
--- a/fs/f2fs/checkpoint.c
+++ b/fs/f2fs/checkpoint.c
@@ -21,7 +21,7 @@
 #include "iostat.h"
 #include <trace/events/f2fs.h>
 
-#define DEFAULT_CHECKPOINT_IOPRIO (IOPRIO_PRIO_VALUE(IOPRIO_CLASS_RT, 3))
+#define DEFAULT_CHECKPOINT_IOPRIO (IOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE, 3))
 
 static struct kmem_cache *ino_entry_slab;
 struct kmem_cache *f2fs_inode_entry_slab;
@@ -173,9 +173,12 @@ static bool __is_bitmap_valid(struct f2fs_sb_info *sbi, block_t blkaddr,
 	return exist;
 }
 
-static bool __f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
+bool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
 					block_t blkaddr, int type)
 {
+	if (time_to_inject(sbi, FAULT_BLKADDR))
+		return false;
+
 	switch (type) {
 	case META_NAT:
 		break;
@@ -230,20 +233,6 @@ static bool __f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
 	return true;
 }
 
-bool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
-					block_t blkaddr, int type)
-{
-	if (time_to_inject(sbi, FAULT_BLKADDR_VALIDITY))
-		return false;
-	return __f2fs_is_valid_blkaddr(sbi, blkaddr, type);
-}
-
-bool f2fs_is_valid_blkaddr_raw(struct f2fs_sb_info *sbi,
-					block_t blkaddr, int type)
-{
-	return __f2fs_is_valid_blkaddr(sbi, blkaddr, type);
-}
-
 /*
  * Readahead CP/NAT/SIT/SSA/POR pages
  */
@@ -706,6 +695,25 @@ static int recover_orphan_inode(struct f2fs_sb_info *sbi, nid_t ino)
 		goto err_out;
 	}
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (is_inode_flag_set(inode, FI_REVOKE_DEDUP)) {
+		f2fs_notice(sbi, "recover orphan: ino[%u] set revoke, flags[%lu]",
+				ino, F2FS_I(inode)->flags[0]);
+		f2fs_bug_on(sbi, is_inode_flag_set(inode, FI_DOING_DEDUP));
+		err = f2fs_truncate_dedup_inode(inode, FI_REVOKE_DEDUP);
+		iput(inode);
+		return err;
+	}
+
+	if (is_inode_flag_set(inode, FI_DOING_DEDUP)) {
+		f2fs_notice(sbi, "recover orphan: ino[%u] set doing dedup, flags[%lu]",
+				ino, F2FS_I(inode)->flags[0]);
+		err = f2fs_truncate_dedup_inode(inode, FI_DOING_DEDUP);
+		iput(inode);
+		return err;
+	}
+#endif
+
 	clear_nlink(inode);
 
 	/* truncate all the data during iput */
@@ -1598,9 +1606,8 @@ static int do_checkpoint(struct f2fs_sb_info *sbi, struct cp_control *cpc)
 	 */
 	if (f2fs_sb_has_encrypt(sbi) || f2fs_sb_has_verity(sbi) ||
 		f2fs_sb_has_compression(sbi))
-		f2fs_bug_on(sbi,
-			invalidate_inode_pages2_range(META_MAPPING(sbi),
-				MAIN_BLKADDR(sbi), MAX_BLKADDR(sbi) - 1));
+		f2fs_truncate_meta_inode_pages(sbi, MAIN_BLKADDR(sbi),
+					MAX_BLKADDR(sbi) - MAIN_BLKADDR(sbi));
 
 	f2fs_release_ino_entry(sbi, false);
 
diff --git a/fs/f2fs/compress.c b/fs/f2fs/compress.c
index 2ca8cb607..caa7b1f67 100644
--- a/fs/f2fs/compress.c
+++ b/fs/f2fs/compress.c
@@ -19,6 +19,10 @@
 #include "node.h"
 #include "segment.h"
 #include <trace/events/f2fs.h>
+#if defined(CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT) || defined(__ARCH_HAS_LZ4_ACCELERATOR)
+#include "lz4armv8/lz4accel.h"
+#include "f2fs_lz4.h"
+#endif
 
 static struct kmem_cache *cic_entry_slab;
 static struct kmem_cache *dic_entry_slab;
@@ -56,6 +60,9 @@ struct f2fs_compress_ops {
 	void (*destroy_decompress_ctx)(struct decompress_io_ctx *dic);
 	int (*decompress_pages)(struct decompress_io_ctx *dic);
 	bool (*is_level_valid)(int level);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	int (*inplace_margin)(int decompressed_sz);
+#endif
 };
 
 static unsigned int offset_in_cluster(struct compress_ctx *cc, pgoff_t index)
@@ -97,6 +104,17 @@ static void f2fs_set_compressed_page(struct page *page,
 	page->mapping = inode->i_mapping;
 }
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+static void f2fs_restore_compressed_page(struct page *page, pgoff_t index)
+{
+	/*
+	 * restore original page index, which is changed for fscrypt iv
+	 * index by inplace io
+	 */
+	page->index = index;
+}
+#endif
+
 static void f2fs_drop_rpages(struct compress_ctx *cc, int len, bool unlock)
 {
 	int i;
@@ -156,6 +174,11 @@ void f2fs_destroy_compress_ctx(struct compress_ctx *cc, bool reuse)
 	cc->nr_rpages = 0;
 	cc->nr_cpages = 0;
 	cc->valid_nr_cpages = 0;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	memset(cc->di, 0, sizeof(cc->di));
+	cc->rofs = 0;
+	cc->cofs = 0;
+#endif
 	if (!reuse)
 		cc->cluster_idx = NULL_CLUSTER;
 }
@@ -267,43 +290,104 @@ static void lz4_destroy_compress_ctx(struct compress_ctx *cc)
 
 static int lz4_compress_pages(struct compress_ctx *cc)
 {
-	int len = -EINVAL;
-	unsigned char level = F2FS_I(cc->inode)->i_compress_level;
+	if (f2fs_compress_layout(cc->inode) == COMPRESS_FIXED_INPUT) {
+		int len = -EINVAL;
+		unsigned char level = F2FS_I(cc->inode)->i_compress_level;
 
-	if (!level)
-		len = LZ4_compress_default(cc->rbuf, cc->cbuf->cdata, cc->rlen,
+		if (!level)
+			len = LZ4_compress_default(cc->rbuf, cc->cbuf->cdata, cc->rlen,
 						cc->clen, cc->private);
+
 #ifdef CONFIG_F2FS_FS_LZ4HC
-	else
-		len = LZ4_compress_HC(cc->rbuf, cc->cbuf->cdata, cc->rlen,
+		else
+			len = LZ4_compress_HC(cc->rbuf, cc->cbuf->cdata, cc->rlen,
 					cc->clen, level, cc->private);
 #endif
-	if (len < 0)
-		return len;
-	if (!len)
-		return -EAGAIN;
+		if (len < 0)
+			return len;
+		if (!len)
+			return -EAGAIN;
+
+		cc->clen = len;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	} else {
+		int slen = (cc->nr_rpages << PAGE_SHIFT) - cc->rofs;
+		void *src = cc->rbuf + cc->rofs;
+		void *dst = (void *)cc->cbuf + cc->cofs;
+		int dlen;
+
+		dlen = LZ4_compress_destSize(src, dst, &slen, PAGE_SIZE, cc->private);
+		if (!dlen)
+			return -EAGAIN;
+
+		if (dlen != PAGE_SIZE && cc->rofs + slen != cc->nr_rpages << PAGE_SHIFT) {
+			dlen = round_up(dlen, PAGE_SIZE);
+		}
+
+		cc->rofs += slen;
+		cc->cofs += dlen;
+#endif
+	}
 
-	cc->clen = len;
 	return 0;
 }
 
 static int lz4_decompress_pages(struct decompress_io_ctx *dic)
 {
-	int ret;
+	unsigned long expected;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	bool accel = false;
+#endif
+	int ret = 0;
 
-	ret = LZ4_decompress_safe(dic->cbuf->cdata, dic->rbuf,
+	if (f2fs_compress_layout(dic->inode) == COMPRESS_FIXED_INPUT) {
+		expected = PAGE_SIZE << dic->log_cluster_size;
+		ret = LZ4_decompress_safe(dic->cbuf->cdata, dic->rbuf,
 						dic->clen, dic->rlen);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	} else {
+		uint8_t *dst = (uint8_t *)dic->rbuf + dic->rofs;
+		const uint8_t *src = (uint8_t *)dic->cbuf + dic->cofs;
+		uint8_t *dstptr = dst;
+		const uint8_t *srcptr = src;
+
+		expected = dic->rlen;
+#ifdef __ARCH_HAS_LZ4_ACCELERATOR
+		if (lz4_decompress_accel_enable() &&
+		    dic->rlen > LZ4_FAST_MARGIN &&
+		    dic->clen > LZ4_FAST_MARGIN) {
+			accel = true;
+
+			ret = lz4_decompress_asm(&dstptr, dst, dst + dic->rlen - LZ4_FAST_MARGIN,
+						  &srcptr, src + dic->clen - LZ4_FAST_MARGIN,
+						  !!dic->inplace_io[dic->current_blk]);
+			if (ret) {
+				printk_ratelimited("%sF2FS-fs (%s): lz4 decompress accel failed, ret:%d\n",
+					KERN_ERR, F2FS_I_SB(dic->inode)->sb->s_id, ret);
+				return -EIO;
+			}
+
+			ret = __lz4_decompress_safe_partial(dstptr, srcptr, dst,
+						  dic->rlen, src, dic->clen, false);
+		}
+#endif
+		if (!accel)
+			ret = LZ4_decompress_safe_partial(srcptr, dstptr,
+						  dic->clen, dic->rlen, dic->rlen);
+#endif
+	}
+
 	if (ret < 0) {
 		printk_ratelimited("%sF2FS-fs (%s): lz4 decompress failed, ret:%d\n",
 				KERN_ERR, F2FS_I_SB(dic->inode)->sb->s_id, ret);
 		return -EIO;
 	}
 
-	if (ret != PAGE_SIZE << dic->log_cluster_size) {
+	if (ret != expected) {
 		printk_ratelimited("%sF2FS-fs (%s): lz4 invalid ret:%d, "
 					"expected:%lu\n", KERN_ERR,
 					F2FS_I_SB(dic->inode)->sb->s_id, ret,
-					PAGE_SIZE << dic->log_cluster_size);
+					expected);
 		return -EIO;
 	}
 	return 0;
@@ -318,16 +402,28 @@ static bool lz4_is_level_valid(int lvl)
 #endif
 }
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+static int lz4_inplace_margin(int decompressed_sz)
+{
+	return (decompressed_sz >> 8) + 65;
+}
+#endif
+
 static const struct f2fs_compress_ops f2fs_lz4_ops = {
 	.init_compress_ctx	= lz4_init_compress_ctx,
 	.destroy_compress_ctx	= lz4_destroy_compress_ctx,
 	.compress_pages		= lz4_compress_pages,
 	.decompress_pages	= lz4_decompress_pages,
 	.is_level_valid		= lz4_is_level_valid,
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	.inplace_margin		= lz4_inplace_margin,
+#endif
 };
 #endif
 
 #ifdef CONFIG_F2FS_FS_ZSTD
+#define F2FS_ZSTD_DEFAULT_CLEVEL	1
+
 static int zstd_init_compress_ctx(struct compress_ctx *cc)
 {
 	zstd_parameters params;
@@ -336,7 +432,6 @@ static int zstd_init_compress_ctx(struct compress_ctx *cc)
 	unsigned int workspace_size;
 	unsigned char level = F2FS_I(cc->inode)->i_compress_level;
 
-	/* Need to remain this for backward compatibility */
 	if (!level)
 		level = F2FS_ZSTD_DEFAULT_CLEVEL;
 
@@ -574,22 +669,79 @@ module_param(num_compress_pages, uint, 0444);
 MODULE_PARM_DESC(num_compress_pages,
 		"Number of intermediate compress pages to preallocate");
 
-int __init f2fs_init_compress_mempool(void)
-{
-	compress_page_pool = mempool_create_page_pool(num_compress_pages, 0);
-	return compress_page_pool ? 0 : -ENOMEM;
-}
+#define MPOOL_NR 1024
+struct page_pool {
+	spinlock_t lock;
+	struct page *pages[MPOOL_NR];
+	int head;
+};
+static struct page_pool *mpool;
 
 void f2fs_destroy_compress_mempool(void)
 {
+	unsigned long flags;
+	int i;
+
 	mempool_destroy(compress_page_pool);
+
+	if (!mpool)
+		return;
+
+	spin_lock_irqsave(&mpool->lock, flags);
+	for (i = 0; i < MPOOL_NR; i++) {
+		if (!mpool->pages[i])
+			break;
+		__free_page(mpool->pages[i]);
+		mpool->pages[i] = NULL;
+	}
+	spin_unlock_irqrestore(&mpool->lock, flags);
+	kfree(mpool);
+	mpool = NULL;
+}
+
+int __init f2fs_init_compress_mempool(void)
+{
+	int i;
+
+	compress_page_pool = mempool_create_page_pool(num_compress_pages, 0);
+	if (!compress_page_pool)
+		return -ENOMEM;
+
+	mpool = kzalloc(sizeof(struct page_pool), GFP_KERNEL);
+	if (!mpool)
+		goto out;
+
+	spin_lock_init(&mpool->lock);
+	mpool->head = 0;
+	for (i = 0; i < MPOOL_NR; i++) {
+		mpool->pages[i] = alloc_page(GFP_KERNEL);
+		if (!mpool->pages[i])
+			goto out;
+	}
+
+	return 0;
+
+out:
+	f2fs_destroy_compress_mempool();
+	return -ENOMEM;
 }
 
 static struct page *f2fs_compress_alloc_page(void)
 {
-	struct page *page;
+	struct page *page = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&mpool->lock, flags);
+	if (mpool->head < MPOOL_NR) {
+		page = mpool->pages[mpool->head];
+		mpool->pages[mpool->head] = NULL;
+		mpool->head++;
+	}
+	spin_unlock_irqrestore(&mpool->lock, flags);
+
+	if (!page)
+		page = mempool_alloc(compress_page_pool, GFP_NOFS);
 
-	page = mempool_alloc(compress_page_pool, GFP_NOFS);
 	lock_page(page);
 
 	return page;
@@ -597,21 +749,36 @@ static struct page *f2fs_compress_alloc_page(void)
 
 static void f2fs_compress_free_page(struct page *page)
 {
+	unsigned long flags;
+
 	if (!page)
 		return;
 	detach_page_private(page);
 	page->mapping = NULL;
 	unlock_page(page);
+
+	spin_lock_irqsave(&mpool->lock, flags);
+	if (mpool->head > 0) {
+		mpool->head--;
+		mpool->pages[mpool->head] = page;
+		spin_unlock_irqrestore(&mpool->lock, flags);
+		return;
+	}
+	spin_unlock_irqrestore(&mpool->lock, flags);
 	mempool_free(page, compress_page_pool);
 }
 
 #define MAX_VMAP_RETRIES	3
 
-static void *f2fs_vmap(struct page **pages, unsigned int count)
+static void *f2fs_vmap(struct f2fs_sb_info *sbi,
+		struct page **pages, unsigned int count)
 {
 	int i;
 	void *buf = NULL;
 
+	if (time_to_inject(sbi, FAULT_COMPRESS_VMAP))
+		return NULL;
+
 	for (i = 0; i < MAX_VMAP_RETRIES; i++) {
 		buf = vm_map_ram(pages, count, -1);
 		if (buf)
@@ -621,7 +788,7 @@ static void *f2fs_vmap(struct page **pages, unsigned int count)
 	return buf;
 }
 
-static int f2fs_compress_pages(struct compress_ctx *cc)
+static int f2fs_fixed_input_compress_pages(struct compress_ctx *cc)
 {
 	struct f2fs_inode_info *fi = F2FS_I(cc->inode);
 	const struct f2fs_compress_ops *cops =
@@ -630,9 +797,6 @@ static int f2fs_compress_pages(struct compress_ctx *cc)
 	u32 chksum = 0;
 	int i, ret;
 
-	trace_f2fs_compress_pages_start(cc->inode, cc->cluster_idx,
-				cc->cluster_size, fi->i_compress_algorithm);
-
 	if (cops->init_compress_ctx) {
 		ret = cops->init_compress_ctx(cc);
 		if (ret)
@@ -657,13 +821,13 @@ static int f2fs_compress_pages(struct compress_ctx *cc)
 		}
 	}
 
-	cc->rbuf = f2fs_vmap(cc->rpages, cc->cluster_size);
+	cc->rbuf = f2fs_vmap(F2FS_I_SB(cc->inode), cc->rpages, cc->cluster_size);
 	if (!cc->rbuf) {
 		ret = -ENOMEM;
 		goto out_free_cpages;
 	}
 
-	cc->cbuf = f2fs_vmap(cc->cpages, cc->nr_cpages);
+	cc->cbuf = f2fs_vmap(F2FS_I_SB(cc->inode), cc->cpages, cc->nr_cpages);
 	if (!cc->cbuf) {
 		ret = -ENOMEM;
 		goto out_vunmap_rbuf;
@@ -729,6 +893,225 @@ static int f2fs_compress_pages(struct compress_ctx *cc)
 	if (cops->destroy_compress_ctx)
 		cops->destroy_compress_ctx(cc);
 out:
+	return ret;
+}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+static int f2fs_fixed_output_compress_pages(struct compress_ctx *cc)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(cc->inode);
+	struct f2fs_inode_info *fi = F2FS_I(cc->inode);
+	const struct f2fs_compress_ops *cops =
+				f2fs_cops[fi->i_compress_algorithm];
+	unsigned int max_len, new_nr_cpages;
+	struct decompress_index *di;
+	unsigned int blkidx = 0;
+	unsigned int rbuf_size = cc->nr_rpages << PAGE_SHIFT;
+	int i, ret;
+
+	if (cops->init_compress_ctx) {
+		if (time_to_inject(sbi, FAULT_COMPRESS_INIT_CTX)) {
+			ret = -EIO;
+			goto out;
+		}
+		ret = cops->init_compress_ctx(cc);
+		if (ret)
+			goto out;
+	}
+	di = cc->di;
+
+	max_len = COMPRESS_HEADER_SIZE + cc->clen;
+	cc->nr_cpages = DIV_ROUND_UP(max_len, PAGE_SIZE);
+	cc->valid_nr_cpages = cc->nr_cpages;
+
+	if (time_to_inject(sbi, FAULT_COMPRESS_PAGE_ARRAY)) {
+		cc->cpages = NULL;
+		ret = -ENOMEM;
+		goto destroy_compress_ctx;
+	}
+
+	cc->cpages = page_array_alloc(cc->inode, cc->nr_cpages);
+	if (!cc->cpages) {
+		ret = -ENOMEM;
+		goto destroy_compress_ctx;
+	}
+
+	for (i = 0; i < cc->nr_cpages; i++) {
+		cc->cpages[i] = f2fs_compress_alloc_page();
+		if (!cc->cpages[i]) {
+			ret = -ENOMEM;
+			goto out_free_cpages;
+		}
+	}
+
+	cc->rbuf = f2fs_vmap(sbi, cc->rpages, cc->cluster_size);
+	if (!cc->rbuf) {
+		ret = -ENOMEM;
+		goto out_free_cpages;
+	}
+
+	cc->cbuf = f2fs_vmap(sbi, cc->cpages, cc->nr_cpages);
+	if (!cc->cbuf) {
+		ret = -ENOMEM;
+		goto out_vunmap_rbuf;
+	}
+
+	while (cc->rofs < rbuf_size) {
+		int prev_src_ofs = cc->rofs;
+		int prev_dst_ofs = cc->cofs;
+		int src_len, dst_len;
+		int dist0 = 0; /* distance from first page */
+		int curofs = 0;
+		int is_compress = 1;
+
+		if (prev_dst_ofs >= (cc->nr_cpages << PAGE_SHIFT)) {
+			ret = -EAGAIN;
+			goto out_vunmap_cbuf;
+		}
+
+		if (prev_src_ofs + PAGE_SIZE >= rbuf_size) {
+			/* left raw data is less than 4K, keep it uncompressed */
+			src_len = rbuf_size - prev_src_ofs;
+			dst_len = src_len;
+			memcpy((void *)cc->cbuf + prev_dst_ofs,
+			       cc->rbuf + prev_src_ofs, src_len);
+			cc->cofs = prev_dst_ofs + dst_len;
+			cc->rofs = prev_src_ofs + src_len;
+			is_compress = 0;
+			goto skip_compress;
+		}
+
+		ret = cops->compress_pages(cc);
+		if (ret)
+			goto out_vunmap_cbuf;
+
+		src_len = cc->rofs - prev_src_ofs;
+		dst_len = cc->cofs - prev_dst_ofs;
+
+		if (dst_len < PAGE_SIZE && cops->inplace_margin &&
+		    (PAGE_SIZE - dst_len) >= cops->inplace_margin(src_len)) {
+			void *last_page_start = (void *)cc->cbuf + prev_dst_ofs;
+			void *last_page_end = last_page_start + PAGE_SIZE;
+			/*
+			 * move compressed data to tail of block in advance,
+			 * so that it can be read directly to page cache
+			 * and it can trigger inplace decompression without
+			 * moving data during read
+			 */
+
+			f2fs_bug_on(sbi, prev_dst_ofs & ~PAGE_MASK);
+			memmove(last_page_end - dst_len, last_page_start, dst_len);
+			memset(last_page_start, 0, PAGE_SIZE - dst_len);
+		} else if (dst_len >= src_len) {
+			/* bad compression ratio, keep it uncompresed */
+			memcpy((void *)cc->cbuf + prev_dst_ofs,
+			       cc->rbuf + prev_src_ofs, PAGE_SIZE);
+			cc->cofs = prev_dst_ofs + PAGE_SIZE;
+			cc->rofs = prev_src_ofs + PAGE_SIZE;
+			src_len = PAGE_SIZE;
+			dst_len = PAGE_SIZE;
+			is_compress = 0;
+		}
+
+skip_compress:
+		/* update decompress index */
+		do {
+			f2fs_bug_on(sbi, (di - cc->di) >= cc->cluster_size);
+			di->is_valid = 1;
+			di->is_compress = is_compress;
+			di->blkidx = blkidx;
+
+			if (!dist0) { // first raw page
+				di->first_page = 1;
+				if (prev_src_ofs & ~PAGE_MASK)
+					di->cross_block = 1;
+				else
+					di->cross_block = 0;
+				di->ofs = prev_src_ofs & ~PAGE_MASK;
+
+				curofs = PAGE_SIZE - (prev_src_ofs & ~PAGE_MASK);
+			} else { // the following raw pages
+				di->first_page = 0;
+				di->cross_block = 0;
+				di->ofs = dist0;
+
+				curofs += PAGE_SIZE;
+			}
+
+			di++;
+			dist0++;
+		} while (curofs + PAGE_SIZE <= src_len);
+
+		blkidx++;
+	}
+	cc->clen = cc->cofs;
+
+	max_len = PAGE_SIZE * (cc->cluster_size - 1);
+
+	if (time_to_inject(sbi, FAULT_COMPRESS_LOW_RATIO)) {
+		ret = -EAGAIN;
+		goto out_vunmap_cbuf;
+	}
+
+	if (cc->clen > max_len) {
+		ret = -EAGAIN;
+		goto out_vunmap_cbuf;
+	}
+
+	new_nr_cpages = DIV_ROUND_UP(cc->clen, PAGE_SIZE);
+
+	vm_unmap_ram(cc->cbuf, cc->nr_cpages);
+	vm_unmap_ram(cc->rbuf, cc->cluster_size);
+
+	for (i = 0; i < cc->nr_cpages; i++) {
+		if (i < new_nr_cpages)
+			continue;
+		f2fs_compress_free_page(cc->cpages[i]);
+		cc->cpages[i] = NULL;
+	}
+
+	if (cops->destroy_compress_ctx)
+		cops->destroy_compress_ctx(cc);
+
+	cc->valid_nr_cpages = new_nr_cpages;
+
+	f2fs_bug_on(sbi, cc->nr_cpages >= cc->cluster_size);
+	return 0;
+
+out_vunmap_cbuf:
+	vm_unmap_ram(cc->cbuf, cc->nr_cpages);
+out_vunmap_rbuf:
+	vm_unmap_ram(cc->rbuf, cc->cluster_size);
+out_free_cpages:
+	for (i = 0; i < cc->nr_cpages; i++) {
+		if (cc->cpages[i])
+			f2fs_compress_free_page(cc->cpages[i]);
+	}
+	page_array_free(cc->inode, cc->cpages, cc->nr_cpages);
+	cc->cpages = NULL;
+destroy_compress_ctx:
+	if (cops->destroy_compress_ctx)
+		cops->destroy_compress_ctx(cc);
+out:
+	return ret;
+}
+#endif
+
+static int f2fs_compress_pages(struct compress_ctx *cc)
+{
+	struct f2fs_inode_info *fi = F2FS_I(cc->inode);
+	int ret = 0;
+
+	trace_f2fs_compress_pages_start(cc->inode, cc->cluster_idx,
+				cc->cluster_size, fi->i_compress_algorithm);
+
+	if (f2fs_compress_layout(cc->inode) == COMPRESS_FIXED_INPUT)
+		ret = f2fs_fixed_input_compress_pages(cc);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	else
+		ret = f2fs_fixed_output_compress_pages(cc);
+#endif
+
 	trace_f2fs_compress_pages_end(cc->inode, cc->cluster_idx,
 							cc->clen, ret);
 	return ret;
@@ -739,7 +1122,8 @@ static int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,
 static void f2fs_release_decomp_mem(struct decompress_io_ctx *dic,
 		bool bypass_destroy_callback, bool pre_alloc);
 
-void f2fs_decompress_cluster(struct decompress_io_ctx *dic, bool in_task)
+static int f2fs_fixed_input_decompress_cluster(struct decompress_io_ctx *dic,
+						bool in_task)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dic->inode);
 	struct f2fs_inode_info *fi = F2FS_I(dic->inode);
@@ -748,12 +1132,9 @@ void f2fs_decompress_cluster(struct decompress_io_ctx *dic, bool in_task)
 	bool bypass_callback = false;
 	int ret;
 
-	trace_f2fs_decompress_pages_start(dic->inode, dic->cluster_idx,
-				dic->cluster_size, fi->i_compress_algorithm);
-
 	if (dic->failed) {
 		ret = -EIO;
-		goto out_end_io;
+		goto out;
 	}
 
 	ret = f2fs_prepare_decomp_mem(dic, false);
@@ -797,7 +1178,211 @@ void f2fs_decompress_cluster(struct decompress_io_ctx *dic, bool in_task)
 out_release:
 	f2fs_release_decomp_mem(dic, bypass_callback, false);
 
-out_end_io:
+out:
+	return ret;
+};
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+struct fixed_output_pcpubuf {
+	u8 data[PAGE_SIZE];
+};
+
+static struct fixed_output_pcpubuf __percpu *f2fs_pcpubuf;
+
+static int __init f2fs_init_pcpubuf(void)
+{
+	f2fs_pcpubuf = alloc_percpu(struct fixed_output_pcpubuf);
+	if (!f2fs_pcpubuf)
+		return -ENOMEM;
+	return 0;
+}
+
+static void f2fs_destroy_pcpubuf(void)
+{
+	free_percpu(f2fs_pcpubuf);
+}
+
+/* must not sleep between copy_inplace_data and put_inplace_data */
+static void *copy_inplace_data(void *src)
+{
+	void *buf = get_cpu_ptr(f2fs_pcpubuf);
+
+	memcpy(buf, src, PAGE_SIZE);
+	return buf;
+}
+
+static void put_inplace_data(void)
+{
+	put_cpu_ptr(f2fs_pcpubuf);
+}
+
+static int f2fs_fixed_output_decompress_cluster(struct decompress_io_ctx *dic,
+						bool in_task)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dic->inode);
+	struct f2fs_inode_info *fi = F2FS_I(dic->inode);
+	const struct f2fs_compress_ops *cops =
+			f2fs_cops[fi->i_compress_algorithm];
+	unsigned int blkidx;
+	int i;
+	int ret = -ENOSYS;
+
+	if (dic->failed)
+		return -EIO;
+
+	for (blkidx = 0; blkidx < dic->nr_cpages; blkidx++) {
+		int end_ofs = 0;
+		bool copied = false;
+		unsigned int lstart = -1, lend = 0, count = 0, last;
+		void *unmap_addr = NULL;
+
+		if (!dic->cpages[blkidx])
+			continue;
+
+		/* get the start di and di count in current cpage
+		 * get the last di which will be decompressed in rpage
+		 */
+		for (i = 0; i < dic->cluster_size; i++) {
+			if (dic->di[i].is_valid == 0)
+				/* the left di should be all not valid */
+				break;
+			if (dic->di[i].blkidx != blkidx)
+				continue;
+			if (lstart == -1)
+				lstart = i;
+			count++;
+			if (dic->rpages[i])
+				lend = lstart + count;
+		}
+
+		/*fix coverity error: Out-of-bounds read array dic->di*/
+		if(lstart >= dic->cluster_size) {
+			f2fs_bug_on(sbi, 1);
+			return -EIO;
+		}
+
+		/* should consider last cluster is not full, di[lstart + count]
+		 * could be invalid
+		 */
+		f2fs_bug_on(sbi, lstart + count < dic->cluster_size &&
+				dic->di[lstart + count].first_page != 1);
+
+		last = lstart + count;
+		/* if next is cross_page, we need one more rpage to save decompressed data at the tail */
+		if (last < dic->cluster_size && dic->di[last].cross_block) {
+			end_ofs = PAGE_SIZE - dic->di[last].ofs;
+			count++;
+			if (dic->rpages[last])
+				lend = lstart + count;
+		}
+
+		/* only partial rpages will be decompressed, so we don't have to
+		 * care about cross_block rpage*/
+		if (lend - lstart < count) {
+			count = lend - lstart;
+			end_ofs = 0;
+		}
+
+		/* padding hole in rpages */
+		for (i = 0; i < count; i++) {
+			if (dic->rpages[lstart + i]) {
+				dic->tpages[lstart + i] = dic->rpages[lstart + i];
+				continue;
+			}
+
+			dic->tpages[lstart + i] = f2fs_compress_alloc_page();
+			if (!dic->tpages[lstart + i])
+				return -ENOMEM;
+		}
+
+		if (cops->init_decompress_ctx) {
+			ret = cops->init_decompress_ctx(dic);
+			if (ret)
+				return ret;
+		}
+
+		dic->rbuf = f2fs_vmap(sbi, dic->tpages + lstart, count);
+		if (!dic->rbuf) {
+			ret = -ENOMEM;
+			goto out_destroy;
+		}
+		dic->rofs = 0;
+
+		if (dic->di[lstart].cross_block)
+			dic->rofs = dic->di[lstart].ofs;
+
+		dic->rlen = (count << PAGE_SHIFT) - dic->rofs - end_ofs;
+
+		dic->cbuf = kmap_atomic(dic->cpages[blkidx]);
+		dic->cofs = 0;
+
+		/* calculate margin length */
+		if (dic->di[lstart].is_compress) {
+			for (i = 0; i < PAGE_SIZE; i++) {
+				if (((u8 *)dic->cbuf)[i])
+					break;
+				dic->cofs++;
+			}
+			dic->clen = PAGE_SIZE - dic->cofs;
+		} else {
+			dic->clen = PAGE_SIZE;
+		}
+
+		/*
+		 * inplace decompression can only be triggered in first-page
+		 * which crosses two compressed blocks, and there is enough
+		 * free space of first-page to store compressed data to avoid
+		 * overlapping durint compression.
+		 */
+		if (dic->inplace_io[blkidx] &&
+		    end_ofs < cops->inplace_margin(dic->rlen)) {
+			unmap_addr = dic->cbuf;
+			dic->cbuf = copy_inplace_data(unmap_addr);
+			kunmap_atomic(unmap_addr);
+			copied = true;
+		}
+
+		if (!dic->di[lstart].is_compress) {
+			memcpy(dic->rbuf + dic->rofs, dic->cbuf + dic->cofs, dic->rlen);
+			ret = 0;
+		} else {
+			dic->current_blk = blkidx;
+			ret = cops->decompress_pages(dic);
+		}
+
+		if (copied)
+			put_inplace_data();
+		else
+			kunmap_atomic(unmap_addr);
+		vm_unmap_ram(dic->rbuf, count);
+
+out_destroy:
+		if (cops->destroy_decompress_ctx)
+			cops->destroy_decompress_ctx(dic);
+
+		if (ret)
+			break;
+	}
+
+	return ret;
+}
+#endif
+
+void f2fs_decompress_cluster(struct decompress_io_ctx *dic, bool in_task)
+{
+	struct f2fs_inode_info *fi = F2FS_I(dic->inode);
+	int ret = -ENOSYS;
+
+	trace_f2fs_decompress_pages_start(dic->inode, dic->cluster_idx,
+				dic->cluster_size, fi->i_compress_algorithm);
+
+	if (f2fs_compress_layout(dic->inode) == COMPRESS_FIXED_INPUT)
+		ret = f2fs_fixed_input_decompress_cluster(dic, in_task);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	else
+		ret = f2fs_fixed_output_decompress_cluster(dic, in_task);
+#endif
+
 	trace_f2fs_decompress_pages_end(dic->inode, dic->cluster_idx,
 							dic->clen, ret);
 	f2fs_decompress_end_io(dic, ret, in_task);
@@ -1381,7 +1966,7 @@ static int f2fs_write_compressed_pages(struct compress_ctx *cc,
 			if (blkaddr == COMPRESS_ADDR)
 				fio.compr_blocks++;
 			if (__is_valid_data_blkaddr(blkaddr))
-				f2fs_invalidate_blocks(sbi, blkaddr, 1);
+				f2fs_invalidate_blocks(sbi, blkaddr);
 			f2fs_update_data_blkaddr(&dn, COMPRESS_ADDR);
 			goto unlock_continue;
 		}
@@ -1391,7 +1976,7 @@ static int f2fs_write_compressed_pages(struct compress_ctx *cc,
 
 		if (i > cc->valid_nr_cpages) {
 			if (__is_valid_data_blkaddr(blkaddr)) {
-				f2fs_invalidate_blocks(sbi, blkaddr, 1);
+				f2fs_invalidate_blocks(sbi, blkaddr);
 				f2fs_update_data_blkaddr(&dn, NEW_ADDR);
 			}
 			goto unlock_continue;
@@ -1417,6 +2002,19 @@ static int f2fs_write_compressed_pages(struct compress_ctx *cc,
 		}
 		(*submitted)++;
 unlock_continue:
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (f2fs_compress_layout(inode) == COMPRESS_FIXED_OUTPUT) {
+			__le32 *addr = decompress_index_addr(inode, dn.node_page);
+			if (cc->di[i].is_valid) {
+				decompress_index_t val = serialize_decompress_index(&cc->di[i]);
+				addr[dn.ofs_in_node] = cpu_to_le32(val);
+			} else {
+				addr[dn.ofs_in_node] = cpu_to_le32(0);
+			}
+			set_page_dirty(dn.node_page);
+		}
+#endif
+
 		inode_dec_dirty_pages(cc->inode);
 		unlock_page(fio.page);
 	}
@@ -1477,8 +2075,6 @@ void f2fs_compress_write_end_io(struct bio *bio, struct page *page)
 	struct f2fs_sb_info *sbi = bio->bi_private;
 	struct compress_io_ctx *cic =
 			(struct compress_io_ctx *)page_private(page);
-	enum count_type type = WB_DATA_TYPE(page,
-				f2fs_is_compressed_page(page));
 	int i;
 
 	if (unlikely(bio->bi_status))
@@ -1486,7 +2082,7 @@ void f2fs_compress_write_end_io(struct bio *bio, struct page *page)
 
 	f2fs_compress_free_page(page);
 
-	dec_page_count(sbi, type);
+	dec_page_count(sbi, F2FS_WB_DATA);
 
 	if (atomic_dec_return(&cic->pending_pages))
 		return;
@@ -1502,14 +2098,12 @@ void f2fs_compress_write_end_io(struct bio *bio, struct page *page)
 }
 
 static int f2fs_write_raw_pages(struct compress_ctx *cc,
-					int *submitted_p,
+					int *submitted,
 					struct writeback_control *wbc,
 					enum iostat_type io_type)
 {
 	struct address_space *mapping = cc->inode->i_mapping;
-	struct f2fs_sb_info *sbi = F2FS_M_SB(mapping);
-	int submitted, compr_blocks, i;
-	int ret = 0;
+	int _submitted, compr_blocks, ret, i;
 
 	compr_blocks = f2fs_compressed_blocks(cc);
 
@@ -1524,10 +2118,6 @@ static int f2fs_write_raw_pages(struct compress_ctx *cc,
 	if (compr_blocks < 0)
 		return compr_blocks;
 
-	/* overwrite compressed cluster w/ normal cluster */
-	if (compr_blocks > 0)
-		f2fs_lock_op(sbi);
-
 	for (i = 0; i < cc->cluster_size; i++) {
 		if (!cc->rpages[i])
 			continue;
@@ -1552,7 +2142,7 @@ static int f2fs_write_raw_pages(struct compress_ctx *cc,
 		if (!clear_page_dirty_for_io(cc->rpages[i]))
 			goto continue_unlock;
 
-		ret = f2fs_write_single_data_page(cc->rpages[i], &submitted,
+		ret = f2fs_write_single_data_page(cc->rpages[i], &_submitted,
 						NULL, NULL, wbc, io_type,
 						compr_blocks, false);
 		if (ret) {
@@ -1560,29 +2150,26 @@ static int f2fs_write_raw_pages(struct compress_ctx *cc,
 				unlock_page(cc->rpages[i]);
 				ret = 0;
 			} else if (ret == -EAGAIN) {
-				ret = 0;
 				/*
 				 * for quota file, just redirty left pages to
 				 * avoid deadlock caused by cluster update race
 				 * from foreground operation.
 				 */
 				if (IS_NOQUOTA(cc->inode))
-					goto out;
+					return 0;
+				ret = 0;
 				f2fs_io_schedule_timeout(DEFAULT_IO_TIMEOUT);
 				goto retry_write;
 			}
-			goto out;
+			return ret;
 		}
 
-		*submitted_p += submitted;
+		*submitted += _submitted;
 	}
 
-out:
-	if (compr_blocks > 0)
-		f2fs_unlock_op(sbi);
+	f2fs_balance_fs(F2FS_M_SB(mapping), true);
 
-	f2fs_balance_fs(sbi, true);
-	return ret;
+	return 0;
 }
 
 int f2fs_write_multi_pages(struct compress_ctx *cc,
@@ -1628,6 +2215,7 @@ static inline bool allow_memalloc_for_decomp(struct f2fs_sb_info *sbi,
 static int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,
 		bool pre_alloc)
 {
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dic->inode);
 	const struct f2fs_compress_ops *cops =
 		f2fs_cops[F2FS_I(dic->inode)->i_compress_algorithm];
 	int i;
@@ -1635,10 +2223,6 @@ static int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,
 	if (!allow_memalloc_for_decomp(F2FS_I_SB(dic->inode), pre_alloc))
 		return 0;
 
-	dic->tpages = page_array_alloc(dic->inode, dic->cluster_size);
-	if (!dic->tpages)
-		return -ENOMEM;
-
 	for (i = 0; i < dic->cluster_size; i++) {
 		if (dic->rpages[i]) {
 			dic->tpages[i] = dic->rpages[i];
@@ -1650,11 +2234,11 @@ static int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,
 			return -ENOMEM;
 	}
 
-	dic->rbuf = f2fs_vmap(dic->tpages, dic->cluster_size);
+	dic->rbuf = f2fs_vmap(sbi, dic->tpages, dic->cluster_size);
 	if (!dic->rbuf)
 		return -ENOMEM;
 
-	dic->cbuf = f2fs_vmap(dic->cpages, dic->nr_cpages);
+	dic->cbuf = f2fs_vmap(sbi, dic->cpages, dic->nr_cpages);
 	if (!dic->cbuf)
 		return -ENOMEM;
 
@@ -1689,6 +2273,7 @@ static void f2fs_free_dic(struct decompress_io_ctx *dic,
 struct decompress_io_ctx *f2fs_alloc_dic(struct compress_ctx *cc)
 {
 	struct decompress_io_ctx *dic;
+	struct page *page;
 	pgoff_t start_idx = start_idx_of_cluster(cc);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(cc->inode);
 	int i, ret;
@@ -1697,15 +2282,8 @@ struct decompress_io_ctx *f2fs_alloc_dic(struct compress_ctx *cc)
 	if (!dic)
 		return ERR_PTR(-ENOMEM);
 
-	dic->rpages = page_array_alloc(cc->inode, cc->cluster_size);
-	if (!dic->rpages) {
-		kmem_cache_free(dic_entry_slab, dic);
-		return ERR_PTR(-ENOMEM);
-	}
-
 	dic->magic = F2FS_COMPRESSED_PAGE_MAGIC;
 	dic->inode = cc->inode;
-	atomic_set(&dic->remaining_pages, cc->nr_cpages);
 	dic->cluster_idx = cc->cluster_idx;
 	dic->cluster_size = cc->cluster_size;
 	dic->log_cluster_size = cc->log_cluster_size;
@@ -1718,29 +2296,152 @@ struct decompress_io_ctx *f2fs_alloc_dic(struct compress_ctx *cc)
 		dic->rpages[i] = cc->rpages[i];
 	dic->nr_rpages = cc->cluster_size;
 
-	dic->cpages = page_array_alloc(dic->inode, dic->nr_cpages);
-	if (!dic->cpages) {
-		ret = -ENOMEM;
-		goto out_free;
-	}
+	if (f2fs_compress_layout(dic->inode) == COMPRESS_FIXED_INPUT) {
+		for (i = 0; i < dic->nr_cpages; i++) {
+			page = f2fs_compress_alloc_page();
+			if (!page) {
+				ret = -ENOMEM;
+				goto out_free;
+			}
 
-	for (i = 0; i < dic->nr_cpages; i++) {
-		struct page *page;
+			f2fs_set_compressed_page(page, cc->inode,
+						start_idx + i + 1, dic);
+			dic->cpages[i] = page;
+		}
+		atomic_set(&dic->remaining_pages, cc->nr_cpages);
 
-		page = f2fs_compress_alloc_page();
-		if (!page) {
-			ret = -ENOMEM;
+		ret = f2fs_prepare_decomp_mem(dic, true);
+		if (ret)
 			goto out_free;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	} else {
+		int prev_blkidx = -1, blkidx;
+		int cnt = 0;
+
+		for (i = 0; i < dic->cluster_size; i++)
+			dic->di[i] = cc->di[i];
+
+		/*
+		 * One cpage could be decomrpessed into several continous
+		 * rpages[n]. If rpages[n-1] is cross_block, rpages[0] ~
+		 * rpage[n-2] have the same blkidx and are not cross_block,
+		 * and rpages[n-1] has blkidx+1 and is cross_block.
+		 *
+		 * In this case, cpage could reuse rpage[n-1] to do inplace
+		 * io, and could try to do inplace decompression later.
+		 *
+		 * If rpages[n-1] is not cross_block, inplace-io cannot be
+		 * used.
+		 */
+		for (i = 0; i < dic->cluster_size; i++) {
+			/* only care about pages will be read */
+			if (!cc->rpages[i]) {
+				/*
+				 * if there are NULL in rpages[], need alloc
+				 * previous cpage and reset all states, e.g
+				 *
+				 *           NULL          NULL
+				 * rpages: |------|-+----|------|--+---|
+				 *                  /              /
+				 *                 /       .------'
+				 *                |       /
+				 * cpages: |------|------|------|
+				 *
+				 * cpages[1] is allocated here
+				 */
+				if (prev_blkidx != -1 && !dic->cpages[prev_blkidx]) {
+					page = f2fs_compress_alloc_page();
+					if (!page) {
+						ret = -ENOMEM;
+						goto out_free;
+					}
+
+					f2fs_set_compressed_page(page, cc->inode,
+								 start_idx + prev_blkidx + 1, dic);
+					dic->cpages[prev_blkidx] = page;
+					prev_blkidx = -1;
+					cnt++;
+				}
+				continue;
+			}
+
+			blkidx = dic->di[i].blkidx;
+			if (prev_blkidx == -1) {
+				prev_blkidx = blkidx;
+				/*
+				 * if we start from not the first rpage
+				 * (first rpage must be not cross_page) in
+				 * the cluster, and the current rpage is
+				 * cross-block, we have to read the previous
+				 * cpage
+				 */
+				if (dic->di[i].cross_block) {
+					prev_blkidx--;
+					f2fs_bug_on(sbi, i == 0);
+				}
+			}
+
+			/*
+			 * it is the first page, continue to check if it can
+			 * be inplace_io
+			 */
+			if (prev_blkidx == blkidx)
+				continue;
+			/*
+			 * if cpage already allocated, skip it, e.g
+			 *
+			 *           NULL          NULL
+			 * rpages: |------|-+----|------|--+---|
+			 *                  /              /
+			 *                 /       .------'
+			 *                |       /
+			 * cpages: |------|------|------|
+			 *
+			 * cpages[1] is allocated by above alloc
+			 */
+			if (dic->cpages[prev_blkidx]) {
+				prev_blkidx = blkidx;
+				continue;
+			}
+
+			/* If di[i] is cross_blocks, di[i-1] must exist */
+			if (dic->di[i].cross_block && dic->di[i-1].is_compress) {
+				page = dic->rpages[i];
+				f2fs_bug_on(sbi, PagePrivate(page));
+				dic->inplace_io[prev_blkidx] = 1;
+				f2fs_set_compressed_page(page, cc->inode,
+						start_idx + prev_blkidx + 1, dic);
+			} else {
+				page = f2fs_compress_alloc_page();
+				if (!page) {
+					ret = -ENOMEM;
+					goto out_free;
+				}
+
+				f2fs_set_compressed_page(page, cc->inode,
+						 start_idx + prev_blkidx + 1, dic);
+			}
+			dic->cpages[prev_blkidx] = page;
+			prev_blkidx = blkidx;
+			cnt++;
 		}
 
-		f2fs_set_compressed_page(page, cc->inode,
-					start_idx + i + 1, dic);
-		dic->cpages[i] = page;
-	}
+		if (prev_blkidx != -1 && !dic->cpages[prev_blkidx]) {
+			page = f2fs_compress_alloc_page();
+			if (!page) {
+				ret = -ENOMEM;
+				goto out_free;
+			}
 
-	ret = f2fs_prepare_decomp_mem(dic, true);
-	if (ret)
-		goto out_free;
+			f2fs_set_compressed_page(page, cc->inode,
+						 start_idx + prev_blkidx + 1, dic);
+			dic->cpages[prev_blkidx] = page;
+			cnt++;
+		}
+
+		atomic_set(&dic->remaining_pages, cnt);
+#endif
+	}
 
 	return dic;
 
@@ -1754,29 +2455,36 @@ static void f2fs_free_dic(struct decompress_io_ctx *dic,
 {
 	int i;
 
-	f2fs_release_decomp_mem(dic, bypass_destroy_callback, true);
+	if (f2fs_compress_layout(dic->inode) == COMPRESS_FIXED_INPUT)
+		f2fs_release_decomp_mem(dic, bypass_destroy_callback, true);
 
-	if (dic->tpages) {
-		for (i = 0; i < dic->cluster_size; i++) {
-			if (dic->rpages[i])
-				continue;
-			if (!dic->tpages[i])
-				continue;
-			f2fs_compress_free_page(dic->tpages[i]);
-		}
-		page_array_free(dic->inode, dic->tpages, dic->cluster_size);
+	for (i = 0; i < dic->cluster_size; i++) {
+		if (dic->rpages[i])
+			continue;
+		if (!dic->tpages[i])
+			continue;
+		f2fs_compress_free_page(dic->tpages[i]);
 	}
 
-	if (dic->cpages) {
-		for (i = 0; i < dic->nr_cpages; i++) {
-			if (!dic->cpages[i])
-				continue;
-			f2fs_compress_free_page(dic->cpages[i]);
+	for (i = 0; i < dic->nr_cpages; i++) {
+		struct page *cpage;
+
+		if (!dic->cpages[i])
+			continue;
+
+		cpage = dic->cpages[i];
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (dic->inplace_io[i]) {
+			detach_page_private(cpage);
+			unlock_page(cpage);
+		} else {
+			f2fs_compress_free_page(cpage);
 		}
-		page_array_free(dic->inode, dic->cpages, dic->nr_cpages);
+#else
+		f2fs_compress_free_page(cpage);
+#endif
 	}
 
-	page_array_free(dic->inode, dic->rpages, dic->nr_rpages);
 	kmem_cache_free(dic_entry_slab, dic);
 }
 
@@ -1805,6 +2513,10 @@ static void f2fs_verify_cluster(struct work_struct *work)
 {
 	struct decompress_io_ctx *dic =
 		container_of(work, struct decompress_io_ctx, verity_work);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	pgoff_t start_idx = dic->cluster_idx << dic->log_cluster_size;
+	bool skip_unlock;
+#endif
 	int i;
 
 	/* Verify, update, and unlock the decompressed pages. */
@@ -1814,10 +2526,29 @@ static void f2fs_verify_cluster(struct work_struct *work)
 		if (!rpage)
 			continue;
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		/*
+		 * if cpage is inplace io, it reuses rpage. In order to avoid
+		 * double unlock_page, keep the rpage locked here, and unlock
+		 * it when cpage is released later in f2fs_free_dic.
+		 */
+		skip_unlock = false;
+		if (f2fs_is_compressed_page(rpage)) {
+			f2fs_restore_compressed_page(rpage, start_idx + i);
+			skip_unlock = true;
+		}
+#endif
+
 		if (fsverity_verify_page(rpage))
 			SetPageUptodate(rpage);
 		else
 			ClearPageUptodate(rpage);
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (skip_unlock)
+			continue;
+#endif
+
 		unlock_page(rpage);
 	}
 
@@ -1831,6 +2562,10 @@ static void f2fs_verify_cluster(struct work_struct *work)
 void f2fs_decompress_end_io(struct decompress_io_ctx *dic, bool failed,
 				bool in_task)
 {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	pgoff_t start_idx = dic->cluster_idx << dic->log_cluster_size;
+	bool skip_unlock;
+#endif
 	int i;
 
 	if (!failed && dic->need_verity) {
@@ -1852,10 +2587,29 @@ void f2fs_decompress_end_io(struct decompress_io_ctx *dic, bool failed,
 		if (!rpage)
 			continue;
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		/*
+		 * if cpage is inplace io, it reuses rpage. In order to avoid
+		 * double unlock_page, keep the rpage locked here, and unlock
+		 * it when cpage is released later in f2fs_free_dic.
+		 */
+		skip_unlock = false;
+		if (f2fs_is_compressed_page(rpage)) {
+			f2fs_restore_compressed_page(rpage, start_idx + i);
+			skip_unlock = true;
+		}
+#endif
+
 		if (failed)
 			ClearPageUptodate(rpage);
 		else
 			SetPageUptodate(rpage);
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (skip_unlock)
+			continue;
+#endif
+
 		unlock_page(rpage);
 	}
 
@@ -1916,12 +2670,11 @@ struct address_space *COMPRESS_MAPPING(struct f2fs_sb_info *sbi)
 	return sbi->compress_inode->i_mapping;
 }
 
-void f2fs_invalidate_compress_pages_range(struct f2fs_sb_info *sbi,
-				block_t blkaddr, unsigned int len)
+void f2fs_invalidate_compress_page(struct f2fs_sb_info *sbi, block_t blkaddr)
 {
 	if (!sbi->compress_inode)
 		return;
-	invalidate_mapping_pages(COMPRESS_MAPPING(sbi), blkaddr, blkaddr + len - 1);
+	invalidate_mapping_pages(COMPRESS_MAPPING(sbi), blkaddr, blkaddr);
 }
 
 void f2fs_cache_compressed_page(struct f2fs_sb_info *sbi, struct page *page,
@@ -2085,10 +2838,17 @@ void f2fs_destroy_page_array_cache(struct f2fs_sb_info *sbi)
 
 int __init f2fs_init_compress_cache(void)
 {
+	int err;
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	err = f2fs_init_pcpubuf();
+	if (err)
+		goto out;
+#endif
 	cic_entry_slab = f2fs_kmem_cache_create("f2fs_cic_entry",
 					sizeof(struct compress_io_ctx));
 	if (!cic_entry_slab)
-		return -ENOMEM;
+		goto free_pcpubuf;
 	dic_entry_slab = f2fs_kmem_cache_create("f2fs_dic_entry",
 					sizeof(struct decompress_io_ctx));
 	if (!dic_entry_slab)
@@ -2096,6 +2856,11 @@ int __init f2fs_init_compress_cache(void)
 	return 0;
 free_cic:
 	kmem_cache_destroy(cic_entry_slab);
+free_pcpubuf:
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	f2fs_destroy_pcpubuf();
+#endif
+out:
 	return -ENOMEM;
 }
 
@@ -2103,4 +2868,7 @@ void f2fs_destroy_compress_cache(void)
 {
 	kmem_cache_destroy(dic_entry_slab);
 	kmem_cache_destroy(cic_entry_slab);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	f2fs_destroy_pcpubuf();
+#endif
 }
diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index d2d848993..158ac9e37 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -49,14 +49,14 @@ void f2fs_destroy_bioset(void)
 	bioset_exit(&f2fs_bioset);
 }
 
-bool f2fs_is_cp_guaranteed(struct page *page)
+static bool __is_cp_guaranteed(struct page *page)
 {
 	struct address_space *mapping = page->mapping;
 	struct inode *inode;
 	struct f2fs_sb_info *sbi;
 
-	if (fscrypt_is_bounce_page(page))
-		return page_private_gcing(fscrypt_pagecache_page(page));
+	if (!mapping)
+		return false;
 
 	inode = mapping->host;
 	sbi = F2FS_I_SB(inode);
@@ -66,6 +66,8 @@ bool f2fs_is_cp_guaranteed(struct page *page)
 			S_ISDIR(inode->i_mode))
 		return true;
 
+	if (f2fs_is_compressed_page(page))
+		return false;
 	if ((S_ISREG(inode->i_mode) && IS_NOQUOTA(inode)) ||
 			page_private_gcing(page))
 		return true;
@@ -98,14 +100,21 @@ enum bio_post_read_step {
 #endif
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 	STEP_DECOMPRESS	= BIT(1),
+	STEP_DECOMPRESS_FIXED_OUTPUT = BIT(3),
 #else
 	STEP_DECOMPRESS	= 0,	/* compile out the decompression-related code */
+	STEP_DECOMPRESS_FIXED_OUTPUT = 0,
 #endif
 #ifdef CONFIG_FS_VERITY
 	STEP_VERITY	= BIT(2),
 #else
 	STEP_VERITY	= 0,	/* compile out the verity-related code */
 #endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+	STEP_READ_ACCOUNT = BIT(4),
+#else
+	STEP_READ_ACCOUNT = 0,
+#endif
 };
 
 struct bio_post_read_ctx {
@@ -120,6 +129,9 @@ struct bio_post_read_ctx {
 	 */
 	bool decompression_attempted;
 	block_t fs_blkaddr;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inode;
+#endif
 };
 
 /*
@@ -142,6 +154,15 @@ static void f2fs_finish_read_bio(struct bio *bio, bool in_task)
 	struct bvec_iter_all iter_all;
 	struct bio_post_read_ctx *ctx = bio->bi_private;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (ctx && (ctx->enabled_steps & STEP_READ_ACCOUNT))
+		inode_dec_read_io(ctx->inode);
+#endif
+
+	/*
+	 * Update and unlock the bio's pagecache pages, and put the
+	 * decompression context for any compressed pages.
+	 */
 	bio_for_each_segment_all(bv, bio, iter_all) {
 		struct page *page = bv->bv_page;
 
@@ -171,7 +192,8 @@ static void f2fs_verify_bio(struct work_struct *work)
 	struct bio_post_read_ctx *ctx =
 		container_of(work, struct bio_post_read_ctx, work);
 	struct bio *bio = ctx->bio;
-	bool may_have_compressed_pages = (ctx->enabled_steps & STEP_DECOMPRESS);
+	bool may_have_compressed_pages = (ctx->enabled_steps &
+			(STEP_DECOMPRESS | STEP_DECOMPRESS_FIXED_OUTPUT));
 
 	/*
 	 * fsverity_verify_bio() may call readahead() again, and while verity
@@ -268,18 +290,88 @@ static void f2fs_handle_step_decompress(struct bio_post_read_ctx *ctx,
 		ctx->enabled_steps &= ~STEP_VERITY;
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+/*
+ * This function copy from fscrypt_decrypt_pagecache_blocks.
+ *
+ * The inode is a FI_DEDUPED inode. It should be consistent with
+ * the issuer to avoid geting incorrect crypt info during dedup or revoke.
+ */
+static int f2fs_fscrypt_decrypt_pagecache_blocks(struct page *page, unsigned int len,
+				     unsigned int offs, struct inode *inode)
+{
+	const unsigned int blockbits = inode->i_blkbits;
+	const unsigned int blocksize = 1 << blockbits;
+	u64 lblk_num = ((u64)page->index << (PAGE_SHIFT - blockbits)) +
+		       (offs >> blockbits);
+	unsigned int i;
+	int err = 0;
+
+	if (WARN_ON_ONCE(!PageLocked(page))) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	if (WARN_ON_ONCE(len <= 0 || !IS_ALIGNED(len | offs, blocksize))) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	for (i = offs; i < offs + len; lblk_num++) {
+		err = fscrypt_encrypt_block_inplace(inode, page,
+				blocksize, i, lblk_num, GFP_NOFS);
+		i += blocksize;
+		if (err)
+			goto out;
+	}
+out:
+	return err;
+}
+
+/*
+ * copy from fscrypt_decrypt_bio, but may use inner inode
+ */
+static bool f2fs_fscrypt_decrypt_bio(struct bio *bio, struct inode *inode)
+{
+	struct bio_vec *bv;
+	struct bvec_iter_all iter_all;
+
+	bio_for_each_segment_all(bv, bio, iter_all) {
+		struct page *page = bv->bv_page;
+		int ret = f2fs_fscrypt_decrypt_pagecache_blocks(page, bv->bv_len,
+							   bv->bv_offset, inode);
+		if (ret) {
+			SetPageError(page);
+			return false;
+		}
+	}
+	return true;
+}
+#endif
+
 static void f2fs_post_read_work(struct work_struct *work)
 {
 	struct bio_post_read_ctx *ctx =
 		container_of(work, struct bio_post_read_ctx, work);
 	struct bio *bio = ctx->bio;
+	bool ret = true;
 
-	if ((ctx->enabled_steps & STEP_DECRYPT) && !fscrypt_decrypt_bio(bio)) {
-		f2fs_finish_read_bio(bio, true);
-		return;
+	if (ctx->enabled_steps & STEP_DECRYPT) {
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if ((ctx->enabled_steps & STEP_READ_ACCOUNT) && ctx->inode)
+			ret = f2fs_fscrypt_decrypt_bio(ctx->bio, ctx->inode);
+		else
+			ret = fscrypt_decrypt_bio(ctx->bio);
+#else
+		ret = fscrypt_decrypt_bio(ctx->bio);
+#endif
+		if(!ret) {
+			f2fs_finish_read_bio(bio, true);
+			return;
+		}
 	}
 
-	if (ctx->enabled_steps & STEP_DECOMPRESS)
+	if (ctx->enabled_steps & (STEP_DECOMPRESS | STEP_DECOMPRESS_FIXED_OUTPUT))
 		f2fs_handle_step_decompress(ctx, true);
 
 	f2fs_verify_and_finish_bio(bio, true);
@@ -289,7 +381,7 @@ static void f2fs_read_end_io(struct bio *bio)
 {
 	struct f2fs_sb_info *sbi = F2FS_P_SB(bio_first_page_all(bio));
 	struct bio_post_read_ctx *ctx;
-	bool intask = in_task() && !irqs_disabled();
+	bool intask = in_task();
 
 	iostat_update_and_unbind_ctx(bio);
 	ctx = bio->bi_private;
@@ -304,11 +396,14 @@ static void f2fs_read_end_io(struct bio *bio)
 
 	if (ctx) {
 		unsigned int enabled_steps = ctx->enabled_steps &
-					(STEP_DECRYPT | STEP_DECOMPRESS);
+					(STEP_DECRYPT | STEP_DECOMPRESS |
+					 STEP_DECOMPRESS_FIXED_OUTPUT);
 
 		/*
 		 * If we have only decompression step between decompression and
 		 * decrypt, we don't need post processing for this.
+		 * fixed-output should not decompress in irq, since it needs to
+		 * do allocating and mapping.
 		 */
 		if (enabled_steps == STEP_DECOMPRESS &&
 				!f2fs_low_mem_mode(sbi)) {
@@ -337,7 +432,7 @@ static void f2fs_write_end_io(struct bio *bio)
 
 	bio_for_each_segment_all(bvec, bio, iter_all) {
 		struct page *page = bvec->bv_page;
-		enum count_type type = WB_DATA_TYPE(page, false);
+		enum count_type type = WB_DATA_TYPE(page);
 
 		if (page_private_dummy(page)) {
 			clear_page_private_dummy(page);
@@ -491,30 +586,52 @@ static void f2fs_set_bio_crypt_ctx(struct bio *bio, const struct inode *inode,
 				  const struct f2fs_io_info *fio,
 				  gfp_t gfp_mask)
 {
+	struct inode *inner = NULL, *outer = (struct inode*)inode;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if(is_inode_flag_set(outer, FI_SNAPSHOT_PREPARED))
+		inner = get_inner_inode(outer);
+#endif
 	/*
 	 * The f2fs garbage collector sets ->encrypted_page when it wants to
 	 * read/write raw data without encryption.
 	 */
 	if (!fio || !fio->encrypted_page)
-		fscrypt_set_bio_crypt_ctx(bio, inode, first_idx, gfp_mask);
-	else if (fscrypt_inode_should_skip_dm_default_key(inode))
+		fscrypt_set_bio_crypt_ctx(bio, inner ? :inode, first_idx, gfp_mask);
+	else if (fscrypt_inode_should_skip_dm_default_key(inner ? :inode))
 		bio_set_skip_dm_default_key(bio);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner)
+		put_inner_inode(inner);
+#endif
 }
 
 static bool f2fs_crypt_mergeable_bio(struct bio *bio, const struct inode *inode,
 				     pgoff_t next_idx,
 				     const struct f2fs_io_info *fio)
 {
+	bool ret;
+	struct inode *inner = NULL, *outer = (struct inode*)inode;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if(is_inode_flag_set(outer, FI_SNAPSHOT_PREPARED))
+		inner = get_inner_inode(outer);
+#endif
 	/*
 	 * The f2fs garbage collector sets ->encrypted_page when it wants to
 	 * read/write raw data without encryption.
 	 */
 	if (fio && fio->encrypted_page)
-		return !bio_has_crypt_ctx(bio) &&
+		ret = !bio_has_crypt_ctx(bio) &&
 			(bio_should_skip_dm_default_key(bio) ==
-			 fscrypt_inode_should_skip_dm_default_key(inode));
-
-	return fscrypt_mergeable_bio(bio, inode, next_idx);
+			 fscrypt_inode_should_skip_dm_default_key(inner ? :inode));
+	else
+		ret = fscrypt_mergeable_bio(bio, inner ? :inode, next_idx);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner)
+		put_inner_inode(inner);
+#endif
+	return ret;
 }
 
 void f2fs_submit_read_bio(struct f2fs_sb_info *sbi, struct bio *bio,
@@ -639,6 +756,11 @@ int f2fs_init_write_merge_io(struct f2fs_sb_info *sbi)
 		int n = (i == META) ? 1 : NR_TEMP_TYPE;
 		int j;
 
+#ifdef CONFIG_F2FS_SEQZONE
+		if (f2fs_sb_has_seqzone(sbi) && i == DATA)
+			n = 2 * F2FS_NR_CPUS + 1; /* 8 hot 8 warm 1 cold */
+#endif
+
 		sbi->write_io[i] = f2fs_kmalloc(sbi,
 				array_size(n, sizeof(struct f2fs_bio_info)),
 				GFP_KERNEL);
@@ -693,9 +815,15 @@ static void __submit_merged_write_cond(struct f2fs_sb_info *sbi,
 {
 	enum temp_type temp;
 	bool ret = true;
+	int sum = NR_TEMP_TYPE;
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi) && type == DATA)
+		sum = 2 * F2FS_NR_CPUS + 1;
+#endif
 
-	for (temp = HOT; temp < NR_TEMP_TYPE; temp++) {
-		if (!force)	{
+	for (temp = HOT; temp < sum; temp++) {
+		if (!force) {
 			enum page_type btype = PAGE_TYPE_OF_BIO(type);
 			struct f2fs_bio_info *io = sbi->write_io[btype] + temp;
 
@@ -731,6 +859,16 @@ void f2fs_flush_merged_writes(struct f2fs_sb_info *sbi)
 	f2fs_submit_merged_write(sbi, META);
 }
 
+#ifdef CONFIG_F2FS_SEQZONE
+static noinline u32 get_seqzone_index(struct inode *inode, pgoff_t index);
+
+void f2fs_seqzone_fio_check(struct f2fs_io_info *fio)
+{
+	f2fs_bug_on(fio->sbi, !f2fs_seqzone_file(fio->page->mapping->host));
+	f2fs_bug_on(fio->sbi, fio->encrypted_page);
+}
+#endif
+
 /*
  * Fill the locked page with data located in the block address.
  * A caller needs to unlock the page on failure.
@@ -752,10 +890,19 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 
 	/* Allocate a new bio */
 	bio = __bio_alloc(fio, 1);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone) {
+		f2fs_seqzone_fio_check(fio);
+		f2fs_bug_on(fio->sbi, fio->seqzone_index == SEQZONE_ADDR);
+	}
 
+	f2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,
+			       fio->use_seqzone ? fio->seqzone_index : fio->page->index,
+			       fio, GFP_NOIO);
+#else
 	f2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,
 			       fio->page->index, fio, GFP_NOIO);
-
+#endif
 	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
 		bio_put(bio);
 		return -EFAULT;
@@ -765,7 +912,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 		wbc_account_cgroup_owner(fio->io_wbc, fio->page, PAGE_SIZE);
 
 	inc_page_count(fio->sbi, is_read_io(fio->op) ?
-			__read_io_type(page) : WB_DATA_TYPE(fio->page, false));
+			__read_io_type(page) : WB_DATA_TYPE(fio->page));
 
 	if (is_read_io(bio_op(bio)))
 		f2fs_submit_read_bio(fio->sbi, bio, fio->type);
@@ -817,7 +964,14 @@ static bool io_is_mergeable(struct f2fs_sb_info *sbi, struct bio *bio,
 static void add_bio_entry(struct f2fs_sb_info *sbi, struct bio *bio,
 				struct page *page, enum temp_type temp)
 {
+	/* ipu bio add to #0 bio cache */
+#ifdef CONFIG_F2FS_SEQZONE
+	struct f2fs_bio_info *io = f2fs_sb_has_seqzone(sbi) ?
+		sbi->write_io[DATA] + temp * F2FS_NR_CPUS :
+		sbi->write_io[DATA] + temp;
+#else
 	struct f2fs_bio_info *io = sbi->write_io[DATA] + temp;
+#endif
 	struct bio_entry *be;
 
 	be = f2fs_kmem_cache_alloc(bio_entry_slab, GFP_NOFS, true, NULL);
@@ -846,8 +1000,21 @@ static int add_ipu_page(struct f2fs_io_info *fio, struct bio **bio,
 	bool found = false;
 	int ret = -EAGAIN;
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone) {
+		f2fs_seqzone_fio_check(fio);
+		f2fs_bug_on(sbi, fio->seqzone_index == SEQZONE_ADDR);
+	}
+#endif
+
 	for (temp = HOT; temp < NR_TEMP_TYPE && !found; temp++) {
+#ifdef CONFIG_F2FS_SEQZONE
+		struct f2fs_bio_info *io = f2fs_sb_has_seqzone(sbi) ?
+			sbi->write_io[DATA] + temp * F2FS_NR_CPUS :
+			sbi->write_io[DATA] + temp;
+#else
 		struct f2fs_bio_info *io = sbi->write_io[DATA] + temp;
+#endif
 		struct list_head *head = &io->bio_list;
 		struct bio_entry *be;
 
@@ -861,15 +1028,26 @@ static int add_ipu_page(struct f2fs_io_info *fio, struct bio **bio,
 			f2fs_bug_on(sbi, !page_is_mergeable(sbi, *bio,
 							    *fio->last_block,
 							    fio->new_blkaddr));
+#ifdef CONFIG_F2FS_SEQZONE
 			if (f2fs_crypt_mergeable_bio(*bio,
 					fio->page->mapping->host,
+					fio->use_seqzone ? fio->seqzone_index :
 					fio->page->index, fio) &&
 			    bio_add_page(*bio, page, PAGE_SIZE, 0) ==
 					PAGE_SIZE) {
 				ret = 0;
 				break;
 			}
-
+#else
+			if (f2fs_crypt_mergeable_bio(*bio,
+					fio->page->mapping->host,
+					fio->page->index, fio) &&
+			    bio_add_page(*bio, page, PAGE_SIZE, 0) ==
+					PAGE_SIZE) {
+				ret = 0;
+				break;
+			}
+#endif
 			/* page can't be merged into bio; submit the bio */
 			del_bio_entry(be);
 			f2fs_submit_write_bio(sbi, *bio, DATA);
@@ -892,10 +1070,15 @@ void f2fs_submit_merged_ipu_write(struct f2fs_sb_info *sbi,
 	enum temp_type temp;
 	bool found = false;
 	struct bio *target = bio ? *bio : NULL;
+	int types = NR_TEMP_TYPE;
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi))
+		types = F2FS_NR_CPUS * 2 + 1;
+#endif
 
 	f2fs_bug_on(sbi, !target && !page);
 
-	for (temp = HOT; temp < NR_TEMP_TYPE && !found; temp++) {
+	for (temp = HOT; temp < types && !found; temp++) {
 		struct f2fs_bio_info *io = sbi->write_io[DATA] + temp;
 		struct list_head *head = &io->bio_list;
 		struct bio_entry *be;
@@ -964,9 +1147,19 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 alloc_new:
 	if (!bio) {
 		bio = __bio_alloc(fio, BIO_MAX_VECS);
+#ifdef CONFIG_F2FS_SEQZONE
+		if (fio->use_seqzone) {
+			f2fs_seqzone_fio_check(fio);
+			f2fs_bug_on(fio->sbi, fio->seqzone_index == SEQZONE_ADDR);
+		}
+
+		f2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,
+				       fio->use_seqzone ? fio->seqzone_index : fio->page->index,
+				       fio, GFP_NOIO);
+#else
 		f2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,
 				       fio->page->index, fio, GFP_NOIO);
-
+#endif
 		add_bio_entry(fio->sbi, bio, page, fio->temp);
 	} else {
 		if (add_ipu_page(fio, &bio, page))
@@ -976,7 +1169,7 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 	if (fio->io_wbc)
 		wbc_account_cgroup_owner(fio->io_wbc, fio->page, PAGE_SIZE);
 
-	inc_page_count(fio->sbi, WB_DATA_TYPE(page, false));
+	inc_page_count(fio->sbi, WB_DATA_TYPE(page));
 
 	*fio->last_block = fio->new_blkaddr;
 	*fio->bio = bio;
@@ -1008,10 +1201,17 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
-	struct f2fs_bio_info *io = sbi->write_io[btype] + fio->temp;
+	struct f2fs_bio_info *io;
 	struct page *bio_page;
-	enum count_type type;
+	int real_temp = fio->temp;
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone) {
+		f2fs_seqzone_fio_check(fio);
+		real_temp = fio->real_temp;
+	}
+#endif
+	io = sbi->write_io[btype] + real_temp;
 	f2fs_bug_on(sbi, is_read_io(fio->op));
 
 	f2fs_down_write(&io->io_rwsem);
@@ -1049,22 +1249,35 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 	/* set submitted = true as a return value */
 	fio->submitted = 1;
 
-	type = WB_DATA_TYPE(bio_page, fio->compressed_page);
-	inc_page_count(sbi, type);
-
+	inc_page_count(sbi, WB_DATA_TYPE(bio_page));
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (io->bio) {
+		if (!io_is_mergeable(sbi, io->bio, io, fio, io->last_block_in_bio,
+			      fio->new_blkaddr))
+			__submit_merged_bio(io);
+		else if (!f2fs_crypt_mergeable_bio(io->bio, fio->page->mapping->host,
+				       bio_page->index, fio)) {
+			if ((fio->encrypted_page)
+			|| bio_first_page_all(io->bio)->mapping->host != fio->page->mapping->host
+			|| !fio->use_seqzone)
+				__submit_merged_bio(io);
+		}
+	}
+#else
 	if (io->bio &&
 	    (!io_is_mergeable(sbi, io->bio, io, fio, io->last_block_in_bio,
 			      fio->new_blkaddr) ||
 	     !f2fs_crypt_mergeable_bio(io->bio, fio->page->mapping->host,
 				       bio_page->index, fio)))
 		__submit_merged_bio(io);
+#endif
 alloc_new:
 	if (io->bio == NULL) {
 		if (F2FS_IO_ALIGNED(sbi) &&
 				(fio->type == DATA || fio->type == NODE) &&
 				fio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {
-			dec_page_count(sbi, WB_DATA_TYPE(bio_page,
-						fio->compressed_page));
+			dec_page_count(sbi, WB_DATA_TYPE(bio_page));
 			fio->retry = 1;
 			goto skip;
 		}
@@ -1079,6 +1292,12 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 		goto alloc_new;
 	}
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone)
+		fio->seqzone_index = bio_first_page_all(io->bio)->index
+					+ io->bio->bi_iter.bi_size / PAGE_SIZE - 1;
+#endif
+
 	if (fio->io_wbc)
 		wbc_account_cgroup_owner(fio->io_wbc, fio->page, PAGE_SIZE);
 
@@ -1107,6 +1326,46 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 	f2fs_up_write(&io->io_rwsem);
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+static void bio_set_post_read_account(struct inode *inode, struct bio *bio)
+{
+	struct bio_post_read_ctx *ctx = get_post_read_ctx(bio);
+
+	ctx->enabled_steps |= STEP_READ_ACCOUNT;
+	ctx->inode = inode;
+}
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+static noinline u32 get_seqzone_index(struct inode *inode, pgoff_t index)
+{
+	struct dnode_of_data dn;
+	struct f2fs_node *rn;
+	__le32 *addr_array;
+	int base = 0;
+	int err;
+	u32 ret;
+	int addrs;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	if (!f2fs_seqzone_file(inode))
+		return 0;
+	set_new_dnode(&dn, inode, NULL ,NULL, 0);
+	err = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);
+	if (err) {
+		f2fs_info(sbi, "get_seqzone_index get dnode failed!");
+		return 0;
+	}
+	rn = F2FS_NODE(dn.node_page);
+	addr_array = blkaddr_in_node(rn);
+	if (IS_INODE(dn.node_page) && f2fs_has_extra_attr(dn.inode))
+		base = get_extra_isize(dn.inode);
+	addrs = ADDRS_PER_PAGE(dn.node_page, dn.inode);
+	ret = le32_to_cpu(addr_array[base + dn.ofs_in_node + addrs]);
+	f2fs_put_dnode(&dn);
+	return ret;
+}
+#endif
+
 static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 				      unsigned nr_pages, blk_opf_t op_flag,
 				      pgoff_t first_idx, bool for_write)
@@ -1117,14 +1376,25 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 	unsigned int post_read_steps = 0;
 	sector_t sector;
 	struct block_device *bdev = f2fs_target_device(sbi, blkaddr, &sector);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	bool use_seqzone = false;
+	u32 seqzone_index = 0;
+#endif
 	bio = bio_alloc_bioset(bdev, bio_max_segs(nr_pages),
 			       REQ_OP_READ | op_flag,
 			       for_write ? GFP_NOIO : GFP_KERNEL, &f2fs_bioset);
 	if (!bio)
 		return ERR_PTR(-ENOMEM);
 	bio->bi_iter.bi_sector = sector;
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode)) {
+		seqzone_index = get_seqzone_index(inode, first_idx);
+		use_seqzone = true;
+	}
+	f2fs_set_bio_crypt_ctx(bio, inode, (use_seqzone && seqzone_index) ? seqzone_index : first_idx, NULL, GFP_NOFS);
+#else
 	f2fs_set_bio_crypt_ctx(bio, inode, first_idx, NULL, GFP_NOFS);
+#endif
 	bio->bi_end_io = f2fs_read_end_io;
 
 	if (fscrypt_inode_uses_fs_layer_crypto(inode))
@@ -1140,7 +1410,12 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 	 * responsible for enabling STEP_DECOMPRESS if it's actually needed.
 	 */
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (post_read_steps || f2fs_compressed_file(inode) ||
+			f2fs_inode_support_dedup(sbi, inode)) {
+#else
 	if (post_read_steps || f2fs_compressed_file(inode)) {
+#endif
 		/* Due to the mempool, this never fails. */
 		ctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);
 		ctx->bio = bio;
@@ -1181,7 +1456,7 @@ static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 	return 0;
 }
 
-static void __set_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)
+void __set_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)
 {
 	__le32 *addr = get_dnode_addr(dn->inode, dn->node_page);
 
@@ -1486,12 +1761,12 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 	old_blkaddr = dn->data_blkaddr;
 	err = f2fs_allocate_data_block(sbi, NULL, old_blkaddr,
-				&dn->data_blkaddr, &sum, seg_type, NULL);
+				&dn->data_blkaddr, &sum, seg_type, NULL, 0);
 	if (err)
 		return err;
 
 	if (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)
-		f2fs_invalidate_internal_cache(sbi, old_blkaddr, 1);
+		f2fs_invalidate_internal_cache(sbi, old_blkaddr);
 
 	f2fs_update_data_blkaddr(dn, dn->data_blkaddr);
 	return 0;
@@ -1557,6 +1832,11 @@ static bool f2fs_map_blocks_cached(struct inode *inode,
 	pgoff_t pgoff = (pgoff_t)map->m_lblk;
 	struct extent_info ei = {};
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode) && flag == F2FS_GET_BLOCK_DIO)
+		return false;
+#endif
+
 	if (!f2fs_lookup_read_extent_cache(inode, pgoff, &ei))
 		return false;
 
@@ -1603,9 +1883,19 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 	unsigned int start_pgofs;
 	int bidx = 0;
 	bool is_hole;
+#ifdef CONFIG_F2FS_SEQZONE
+	bool dio_ipu = false;
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL, *outer = NULL;
+#endif
 
 	if (!maxblocks)
 		return 0;
+#ifdef CONFIG_F2FS_SEQZONE
+	else if (maxblocks > 2 * DEF_MIN_FSYNC_BLOCKS)
+		dio_ipu = true;
+#endif
 
 	if (!map->m_may_create && f2fs_map_blocks_cached(inode, map, flag))
 		goto out;
@@ -1614,6 +1904,27 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 	map->m_multidev_dio =
 		f2fs_allow_multi_device_dio(F2FS_I_SB(inode), flag);
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	/*
+	 * For generic read, flag is F2FS_GET_BLOCK_DEFAULT,
+	 * caller have decided to use inner or outer inode.
+	 * Here, we just accept. Otherwise, it may cause inode
+	 * inconsistency.
+	 */
+	if (flag != F2FS_GET_BLOCK_DEFAULT &&
+			f2fs_is_outer_inode(inode)) {
+		/* if create != 0 && not snapshot prepared , the inode should not be deduped inode */
+		if (map->m_may_create && !is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED)) {
+			f2fs_err(sbi, "inode[%lu] dedup addr error", inode->i_ino);
+			f2fs_bug_on(sbi, 1);
+		}
+		inner = get_inner_inode(inode);
+		if (inner) {
+			outer = inode;
+			inode = inner;
+		}
+	}
+#endif
 	map->m_len = 0;
 	map->m_flags = 0;
 
@@ -1652,8 +1963,14 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 	}
 
 	/* use out-place-update for direct IO under LFS mode */
+#ifdef CONFIG_F2FS_SEQZONE
+	if (map->m_may_create &&
+	    (is_hole || (f2fs_lfs_mode(sbi) && flag == F2FS_GET_BLOCK_DIO) ||
+		(f2fs_seqzone_file(inode) && flag == F2FS_GET_BLOCK_DIO))) {
+#else
 	if (map->m_may_create &&
 	    (is_hole || (f2fs_lfs_mode(sbi) && flag == F2FS_GET_BLOCK_DIO))) {
+#endif
 		if (unlikely(f2fs_cp_error(sbi))) {
 			err = -EIO;
 			goto sync_out;
@@ -1668,7 +1985,25 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 			break;
 		case F2FS_GET_BLOCK_PRE_DIO:
 		case F2FS_GET_BLOCK_DIO:
+#ifdef CONFIG_F2FS_SEQZONE
+			if (f2fs_seqzone_file(inode)) {
+				dn.seqzone_index = map->m_seqblk + map->m_len;
+
+				if (!is_hole && dio_ipu) {
+					f2fs_wait_on_page_writeback(dn.node_page, NODE, true, true);
+					err = __set_seqzone_index(&dn);
+					if (set_page_dirty(dn.node_page))
+						dn.node_changed = true;
+				} else {
+					dn.use_seqzone = true;
+					err = __allocate_data_block(&dn, map->m_seg_type);
+				}
+			} else {
+				err = __allocate_data_block(&dn, map->m_seg_type);
+			}
+#else
 			err = __allocate_data_block(&dn, map->m_seg_type);
+#endif
 			if (err)
 				goto sync_out;
 			if (flag == F2FS_GET_BLOCK_PRE_DIO)
@@ -1737,6 +2072,13 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 			flag == F2FS_GET_BLOCK_PRE_DIO) {
 		if (map->m_multidev_dio && map->m_bdev != FDEV(bidx).bdev)
 			goto sync_out;
+#ifdef CONFIG_F2FS_SEQZONE
+		if (flag == F2FS_GET_BLOCK_DIO && !map->m_may_create &&
+			f2fs_seqzone_file(inode)
+			&& seqzone_index(inode, dn.node_page, dn.ofs_in_node) != map->m_seqblk + ofs)
+			goto sync_out;
+
+#endif
 		ofs++;
 		map->m_len++;
 	} else {
@@ -1832,6 +2174,13 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 		f2fs_balance_fs(sbi, dn.node_changed);
 	}
 out:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner) {
+		trace_f2fs_dedup_map_blocks(outer, inner);
+		inode = outer;
+		put_inner_inode(inner);
+	}
+#endif
 	trace_f2fs_map_blocks(inode, map, flag, err);
 	return err;
 }
@@ -2114,7 +2463,10 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	sector_t last_block_in_file;
 	sector_t block_nr;
 	int ret = 0;
-
+#ifdef CONFIG_F2FS_SEQZONE
+	bool use_seqzone = false;
+	u32 seqzone_index = 0;
+#endif
 	block_in_file = (sector_t)page_index(page);
 	last_block = block_in_file + nr_pages;
 	last_block_in_file = bytes_to_blks(inode,
@@ -2179,9 +2531,20 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	 * This page will go to BIO.  Do we need to send this
 	 * BIO off first?
 	 */
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode)) {
+		seqzone_index = get_seqzone_index(inode, page->index);
+		use_seqzone = true;
+	}
+	if (bio && (!page_is_mergeable(F2FS_I_SB(inode), bio,
+				       *last_block_in_bio, block_nr) ||
+		    !f2fs_crypt_mergeable_bio(bio, inode, (use_seqzone && seqzone_index) ?
+					      seqzone_index : page->index, NULL))) {
+#else
 	if (bio && (!page_is_mergeable(F2FS_I_SB(inode), bio,
 				       *last_block_in_bio, block_nr) ||
 		    !f2fs_crypt_mergeable_bio(bio, inode, page->index, NULL))) {
+#endif
 submit_and_realloc:
 		f2fs_submit_read_bio(F2FS_I_SB(inode), bio, DATA);
 		bio = NULL;
@@ -2195,6 +2558,18 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 			bio = NULL;
 			goto out;
 		}
+#ifdef CONFIG_F2FS_FS_DEDUP
+		/*
+		 * If read and dedup process concurrently,
+		 * apps may read unexpected data, such as truncated data.
+		 * Let dedup wait all bio of the file complete before
+		 * doing truncate.
+		 */
+		if (f2fs_inode_support_dedup(F2FS_I_SB(inode), inode)) {
+			bio_set_post_read_account(inode, bio);
+			inode_inc_read_io(inode);
+		}
+#endif
 	}
 
 	/*
@@ -2313,6 +2688,17 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 		goto out_put_dnode;
 	}
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compress_layout(inode) == COMPRESS_FIXED_OUTPUT) {
+		__le32 *addr = decompress_index_addr(inode, dn.node_page);
+
+		for (i = 0; i < cc->cluster_size; i++) {
+			decompress_index_t val = le32_to_cpu(addr[dn.ofs_in_node + i]);
+			deserialize_decompress_index(val, &cc->di[i]);
+		}
+	}
+#endif
+
 	dic = f2fs_alloc_dic(cc);
 	if (IS_ERR(dic)) {
 		ret = PTR_ERR(dic);
@@ -2324,6 +2710,14 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 		block_t blkaddr;
 		struct bio_post_read_ctx *ctx;
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (!page) {
+			f2fs_bug_on(sbi, f2fs_compress_layout(inode) !=
+							COMPRESS_FIXED_OUTPUT);
+			continue;
+		}
+#endif
+
 		blkaddr = from_dnode ? data_blkaddr(dn.inode, dn.node_page,
 					dn.ofs_in_node + i + 1) :
 					ei.blk + i;
@@ -2362,6 +2756,10 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 
 		ctx = get_post_read_ctx(bio);
 		ctx->enabled_steps |= STEP_DECOMPRESS;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (f2fs_compress_layout(inode) == COMPRESS_FIXED_OUTPUT)
+			ctx->enabled_steps |= STEP_DECOMPRESS_FIXED_OUTPUT;
+#endif
 		refcount_inc(&dic->refcnt);
 
 		inc_page_count(sbi, F2FS_RD_DATA);
@@ -2416,6 +2814,9 @@ static int f2fs_mpage_readpages(struct inode *inode,
 	unsigned nr_pages = rac ? readahead_count(rac) : 1;
 	unsigned max_nr_pages = nr_pages;
 	int ret = 0;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL, *outer = NULL, *prev = NULL;
+#endif
 
 	map.m_pblk = 0;
 	map.m_lblk = 0;
@@ -2470,10 +2871,40 @@ static int f2fs_mpage_readpages(struct inode *inode,
 			goto next_page;
 		}
 read_single_page:
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+		inode_inc_read_io(inode);
+		inner = get_inner_inode(inode);
+		if (inner) {
+			trace_f2fs_dedup_map_readpage(inode, inner);
+			outer = inode;
+			inode = inner;
+		}
+
+		/*
+		 * If the inode do revoke or dedup during read,
+		 * we should clear previous map to avoid get wrong
+		 * physic block.
+		 */
+		if (!prev)
+			prev = inode;
+		if (unlikely(inode != prev)) {
+			memset(&map, 0, sizeof(map));
+			map.m_seg_type = NO_CHECK_TYPE;
+			prev = inode;
+		}
+
 #endif
 
 		ret = f2fs_read_single_page(inode, page, max_nr_pages, &map,
 					&bio, &last_block_in_bio, rac);
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (inner) {
+			inode = outer;
+			put_inner_inode(inner);
+		}
+		inode_dec_read_io(inode);
+#endif
 		if (ret) {
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 set_error_page:
@@ -2502,6 +2933,7 @@ static int f2fs_mpage_readpages(struct inode *inode,
 	}
 	if (bio)
 		f2fs_submit_read_bio(F2FS_I_SB(inode), bio, DATA);
+	f2fs_update_atime(inode, true);
 	return ret;
 }
 
@@ -2553,6 +2985,9 @@ int f2fs_encrypt_one_page(struct f2fs_io_info *fio)
 
 	page = fio->compressed_page ? fio->compressed_page : fio->page;
 
+	/* wait for GCed page writeback via META_MAPPING */
+	f2fs_wait_on_block_writeback(inode, fio->old_blkaddr);
+
 	if (fscrypt_inode_uses_inline_crypto(inode))
 		return 0;
 
@@ -2649,6 +3084,16 @@ bool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)
 		return true;
 	if (f2fs_used_in_atomic_write(inode))
 		return true;
+	/* rewrite low ratio compress data w/ OPU to avoid fragmentation */
+	if (f2fs_compressed_file(inode) &&
+		F2FS_OPTION(sbi).compress_mode == COMPR_MODE_USER &&
+		is_inode_flag_set(inode, FI_ENABLE_COMPRESS))
+		return true;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_deduped_inode(inode) &&
+		is_inode_flag_set(inode, FI_REVOKE_DEDUP))
+		return true;
+#endif
 
 	/* swap file is migrating in aligned write mode */
 	if (is_inode_flag_set(inode, FI_ALIGNED_WRITE))
@@ -2697,9 +3142,15 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 	else
 		set_new_dnode(&dn, inode, NULL, NULL, 0);
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (need_inplace_update(fio) && !f2fs_seqzone_file(inode) &&
+	    f2fs_lookup_read_extent_cache_block(inode, page->index,
+						&fio->old_blkaddr)) {
+#else
 	if (need_inplace_update(fio) &&
 	    f2fs_lookup_read_extent_cache_block(inode, page->index,
 						&fio->old_blkaddr)) {
+#endif
 		if (!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,
 						DATA_GENERIC_ENHANCE)) {
 			f2fs_handle_error(fio->sbi,
@@ -2752,6 +3203,13 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 		if (err)
 			goto out_writepage;
 
+#ifdef CONFIG_F2FS_SEQZONE
+		if (fio->use_seqzone) {
+			f2fs_seqzone_fio_check(fio);
+			f2fs_bug_on(fio->sbi, fio->seqzone_index != SEQZONE_ADDR);
+			fio->seqzone_index = dn.seqzone_index;
+		}
+#endif
 		set_page_writeback(page);
 		f2fs_put_dnode(&dn);
 		if (fio->need_lock == LOCK_REQ)
@@ -2841,6 +3299,11 @@ int f2fs_write_single_data_page(struct page *page, int *submitted,
 		.io_wbc = wbc,
 		.bio = bio,
 		.last_block = last_block,
+#ifdef CONFIG_F2FS_SEQZONE
+		.use_seqzone = f2fs_seqzone_file(inode),
+		.seqzone_index = f2fs_seqzone_file(inode) ?
+						SEQZONE_ADDR : NEW_ADDR,
+#endif
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -2912,7 +3375,6 @@ int f2fs_write_single_data_page(struct page *page, int *submitted,
 	if (err == -EAGAIN) {
 		err = f2fs_do_write_data_page(&fio);
 		if (err == -EAGAIN) {
-			f2fs_bug_on(sbi, compr_blocks);
 			fio.need_lock = LOCK_REQ;
 			err = f2fs_do_write_data_page(&fio);
 		}
@@ -3008,7 +3470,8 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 {
 	int ret = 0;
 	int done = 0, retry = 0;
-	struct page *pages[F2FS_ONSTACK_PAGES];
+	struct page *pages_local[F2FS_ONSTACK_PAGES];
+	struct page **pages = pages_local;
 	struct f2fs_sb_info *sbi = F2FS_M_SB(mapping);
 	struct bio *bio = NULL;
 	sector_t last_block;
@@ -3030,6 +3493,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	};
 #endif
 	int nr_pages;
+	unsigned int max_pages = F2FS_ONSTACK_PAGES;
 	pgoff_t index;
 	pgoff_t end;		/* Inclusive */
 	pgoff_t done_index;
@@ -3039,6 +3503,15 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	int submitted = 0;
 	int i;
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	if (f2fs_compressed_file(inode) &&
+		1 << cc.log_cluster_size > F2FS_ONSTACK_PAGES) {
+		pages = f2fs_kzalloc(sbi, sizeof(struct page *) <<
+				cc.log_cluster_size, __GFP_NOFAIL | GFP_NOFS);
+		max_pages = 1 << cc.log_cluster_size;
+	}
+#endif
+
 	if (get_dirty_pages(mapping->host) <=
 				SM_I(F2FS_M_SB(mapping))->min_hot_blocks)
 		set_inode_flag(mapping->host, FI_HOT_DATA);
@@ -3064,8 +3537,11 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 		tag_pages_for_writeback(mapping, index, end);
 	done_index = index;
 	while (!done && !retry && (index <= end)) {
+		/* fix coverity error: Dereferencing a pointer that might be NULL pages */
+		if (!pages)
+			break;
 		nr_pages = find_get_pages_range_tag(mapping, &index, end,
-				tag, F2FS_ONSTACK_PAGES, pages);
+				tag, max_pages, pages);
 		if (nr_pages == 0)
 			break;
 
@@ -3238,6 +3714,10 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	if (bio)
 		f2fs_submit_merged_ipu_write(sbi, &bio, NULL);
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	if (pages != pages_local)
+		kfree(pages);
+#endif
 	return ret;
 }
 
@@ -3338,10 +3818,22 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 			    struct writeback_control *wbc)
 {
 	struct inode *inode = mapping->host;
+	struct inode *inner = NULL;
+	int ret = 0;
 
-	return __f2fs_write_data_pages(mapping, wbc,
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if(is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		inner = get_inner_inode(inode);
+#endif
+	ret = __f2fs_write_data_pages(inner ? inner->i_mapping : mapping, wbc,
 			F2FS_I(inode)->cp_task == current ?
 			FS_CP_DATA_IO : FS_DATA_IO);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner)
+		put_inner_inode(inner);
+#endif
+	return ret;
 }
 
 void f2fs_write_failed(struct inode *inode, loff_t to)
@@ -4035,6 +4527,13 @@ static int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,
 			"Swapfile not supported in LFS mode");
 		return -EINVAL;
 	}
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_deduped_inode(inode)) {
+		f2fs_err(F2FS_I_SB(inode),
+			"Swapfile not supported in dedup mode");
+		return -EOPNOTSUPP;
+	}
+#endif
 
 	ret = f2fs_convert_inline_inode(inode);
 	if (ret)
@@ -4166,27 +4665,42 @@ static int f2fs_iomap_begin(struct inode *inode, loff_t offset, loff_t length,
 	struct f2fs_map_blocks map = {};
 	pgoff_t next_pgofs = 0;
 	int err;
+#ifdef CONFIG_F2FS_SEQZONE
+	struct f2fs_inode_info* fi = F2FS_I(inode);
+#endif
 
 	map.m_lblk = bytes_to_blks(inode, offset);
 	map.m_len = bytes_to_blks(inode, offset + length - 1) - map.m_lblk + 1;
 	map.m_next_pgofs = &next_pgofs;
 	map.m_seg_type = f2fs_rw_hint_to_seg_type(inode->i_write_hint);
-	if (flags & IOMAP_WRITE)
+	if (flags & IOMAP_WRITE) {
 		map.m_may_create = true;
-
+#ifdef CONFIG_F2FS_SEQZONE
+		if (f2fs_seqzone_file(inode))
+			map.m_seqblk = fi->dio_cur_seqindex;
+	} else if (f2fs_seqzone_file(inode)) {
+		map.m_seqblk = get_seqzone_index(inode, map.m_lblk);
+#endif
+	}
 	err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DIO);
 	if (err)
 		return err;
 
 	iomap->offset = blks_to_bytes(inode, map.m_lblk);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		iomap->android_kabi_reserved1 = (u64)map.m_seqblk;
+#endif
 	/*
 	 * When inline encryption is enabled, sometimes I/O to an encrypted file
 	 * has to be broken up to guarantee DUN contiguity.  Handle this by
 	 * limiting the length of the mapping returned.
 	 */
 	map.m_len = fscrypt_limit_io_blocks(inode, map.m_lblk, map.m_len);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	if ((flags & IOMAP_WRITE) && f2fs_seqzone_file(inode))
+		fi->dio_cur_seqindex += map.m_len;
+#endif
 	/*
 	 * We should never see delalloc or compressed extents here based on
 	 * prior flushing and checks.
diff --git a/fs/f2fs/dir.c b/fs/f2fs/dir.c
index bde28918e..37f2e9ccd 100644
--- a/fs/f2fs/dir.c
+++ b/fs/f2fs/dir.c
@@ -16,21 +16,6 @@
 #include "xattr.h"
 #include <trace/events/f2fs.h>
 
-static inline bool f2fs_should_fallback_to_linear(struct inode *dir)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(dir);
-
-	switch (f2fs_get_lookup_mode(sbi)) {
-	case LOOKUP_PERF:
-		return false;
-	case LOOKUP_COMPAT:
-		return true;
-	case LOOKUP_AUTO:
-		return !sb_no_casefold_compat_fallback(sbi->sb);
-	}
-	return false;
-}
-
 #if IS_ENABLED(CONFIG_UNICODE)
 extern struct kmem_cache *f2fs_cf_name_slab;
 #endif
@@ -181,8 +166,7 @@ static unsigned long dir_block_index(unsigned int level,
 static struct f2fs_dir_entry *find_in_block(struct inode *dir,
 				struct page *dentry_page,
 				const struct f2fs_filename *fname,
-				int *max_slots,
-				bool use_hash)
+				int *max_slots)
 {
 	struct f2fs_dentry_block *dentry_blk;
 	struct f2fs_dentry_ptr d;
@@ -190,7 +174,7 @@ static struct f2fs_dir_entry *find_in_block(struct inode *dir,
 	dentry_blk = (struct f2fs_dentry_block *)page_address(dentry_page);
 
 	make_dentry_ptr_block(dir, &d, dentry_blk);
-	return f2fs_find_target_dentry(&d, fname, max_slots, use_hash);
+	return f2fs_find_target_dentry(&d, fname, max_slots);
 }
 
 #if IS_ENABLED(CONFIG_UNICODE)
@@ -267,8 +251,7 @@ static inline int f2fs_match_name(const struct inode *dir,
 }
 
 struct f2fs_dir_entry *f2fs_find_target_dentry(const struct f2fs_dentry_ptr *d,
-			const struct f2fs_filename *fname, int *max_slots,
-			bool use_hash)
+			const struct f2fs_filename *fname, int *max_slots)
 {
 	struct f2fs_dir_entry *de;
 	unsigned long bit_pos = 0;
@@ -291,7 +274,7 @@ struct f2fs_dir_entry *f2fs_find_target_dentry(const struct f2fs_dentry_ptr *d,
 			continue;
 		}
 
-		if (!use_hash || de->hash_code == fname->hash) {
+		if (de->hash_code == fname->hash) {
 			res = f2fs_match_name(d->inode, fname,
 					      d->filename[bit_pos],
 					      le16_to_cpu(de->name_len));
@@ -318,12 +301,11 @@ struct f2fs_dir_entry *f2fs_find_target_dentry(const struct f2fs_dentry_ptr *d,
 static struct f2fs_dir_entry *find_in_level(struct inode *dir,
 					unsigned int level,
 					const struct f2fs_filename *fname,
-					struct page **res_page,
-					bool use_hash)
+					struct page **res_page)
 {
 	int s = GET_DENTRY_SLOTS(fname->disk_name.len);
 	unsigned int nbucket, nblock;
-	unsigned int bidx, end_block, bucket_no;
+	unsigned int bidx, end_block;
 	struct page *dentry_page;
 	struct f2fs_dir_entry *de = NULL;
 	pgoff_t next_pgofs;
@@ -333,11 +315,8 @@ static struct f2fs_dir_entry *find_in_level(struct inode *dir,
 	nbucket = dir_buckets(level, F2FS_I(dir)->i_dir_level);
 	nblock = bucket_blocks(level);
 
-	bucket_no = use_hash ? le32_to_cpu(fname->hash) % nbucket : 0;
-
-start_find_bucket:
 	bidx = dir_block_index(level, F2FS_I(dir)->i_dir_level,
-			       bucket_no);
+			       le32_to_cpu(fname->hash) % nbucket);
 	end_block = bidx + nblock;
 
 	while (bidx < end_block) {
@@ -354,7 +333,7 @@ static struct f2fs_dir_entry *find_in_level(struct inode *dir,
 			}
 		}
 
-		de = find_in_block(dir, dentry_page, fname, &max_slots, use_hash);
+		de = find_in_block(dir, dentry_page, fname, &max_slots);
 		if (IS_ERR(de)) {
 			*res_page = ERR_CAST(de);
 			de = NULL;
@@ -371,18 +350,12 @@ static struct f2fs_dir_entry *find_in_level(struct inode *dir,
 		bidx++;
 	}
 
-	if (de)
-		return de;
-
-	if (likely(use_hash)) {
-		if (room && F2FS_I(dir)->chash != fname->hash) {
-			F2FS_I(dir)->chash = fname->hash;
-			F2FS_I(dir)->clevel = level;
-		}
-	} else if (++bucket_no < nbucket) {
-		goto start_find_bucket;
+	if (!de && room && F2FS_I(dir)->chash != fname->hash) {
+		F2FS_I(dir)->chash = fname->hash;
+		F2FS_I(dir)->clevel = level;
 	}
-	return NULL;
+
+	return de;
 }
 
 struct f2fs_dir_entry *__f2fs_find_entry(struct inode *dir,
@@ -393,15 +366,11 @@ struct f2fs_dir_entry *__f2fs_find_entry(struct inode *dir,
 	struct f2fs_dir_entry *de = NULL;
 	unsigned int max_depth;
 	unsigned int level;
-	bool use_hash = true;
 
 	*res_page = NULL;
 
-#if IS_ENABLED(CONFIG_UNICODE)
-start_find_entry:
-#endif
 	if (f2fs_has_inline_dentry(dir)) {
-		de = f2fs_find_in_inline_dir(dir, fname, res_page, use_hash);
+		de = f2fs_find_in_inline_dir(dir, fname, res_page);
 		goto out;
 	}
 
@@ -417,19 +386,11 @@ struct f2fs_dir_entry *__f2fs_find_entry(struct inode *dir,
 	}
 
 	for (level = 0; level < max_depth; level++) {
-		de = find_in_level(dir, level, fname, res_page, use_hash);
+		de = find_in_level(dir, level, fname, res_page);
 		if (de || IS_ERR(*res_page))
 			break;
 	}
-
 out:
-#if IS_ENABLED(CONFIG_UNICODE)
-	if (f2fs_should_fallback_to_linear(dir) &&
-		IS_CASEFOLDED(dir) && !de && use_hash) {
-		use_hash = false;
-		goto start_find_entry;
-	}
-#endif
 	/* This is to increase the speed of f2fs_create */
 	if (!de)
 		F2FS_I(dir)->task = current;
diff --git a/fs/f2fs/extent_cache.c b/fs/f2fs/extent_cache.c
index b8ce02d26..eb1c05abc 100644
--- a/fs/f2fs/extent_cache.c
+++ b/fs/f2fs/extent_cache.c
@@ -74,14 +74,40 @@ static void __set_extent_info(struct extent_info *ei,
 	}
 }
 
+static bool __may_read_extent_tree(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	if (!test_opt(sbi, READ_EXTENT_CACHE))
+		return false;
+	if (is_inode_flag_set(inode, FI_NO_EXTENT))
+		return false;
+	if (is_inode_flag_set(inode, FI_COMPRESSED_FILE) &&
+			 !f2fs_sb_has_readonly(sbi))
+		return false;
+	return S_ISREG(inode->i_mode);
+}
+
+static bool __may_age_extent_tree(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	if (!test_opt(sbi, AGE_EXTENT_CACHE))
+		return false;
+	if (is_inode_flag_set(inode, FI_COMPRESSED_FILE))
+		return false;
+	if (file_is_cold(inode))
+		return false;
+
+	return S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode);
+}
+
 static bool __init_may_extent_tree(struct inode *inode, enum extent_type type)
 {
 	if (type == EX_READ)
-		return test_opt(F2FS_I_SB(inode), READ_EXTENT_CACHE) &&
-			S_ISREG(inode->i_mode);
-	if (type == EX_BLOCK_AGE)
-		return test_opt(F2FS_I_SB(inode), AGE_EXTENT_CACHE) &&
-			(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode));
+		return __may_read_extent_tree(inode);
+	else if (type == EX_BLOCK_AGE)
+		return __may_age_extent_tree(inode);
 	return false;
 }
 
@@ -94,22 +120,7 @@ static bool __may_extent_tree(struct inode *inode, enum extent_type type)
 	if (list_empty(&F2FS_I_SB(inode)->s_list))
 		return false;
 
-	if (!__init_may_extent_tree(inode, type))
-		return false;
-
-	if (type == EX_READ) {
-		if (is_inode_flag_set(inode, FI_NO_EXTENT))
-			return false;
-		if (is_inode_flag_set(inode, FI_COMPRESSED_FILE) &&
-				 !f2fs_sb_has_readonly(F2FS_I_SB(inode)))
-			return false;
-	} else if (type == EX_BLOCK_AGE) {
-		if (is_inode_flag_set(inode, FI_COMPRESSED_FILE))
-			return false;
-		if (file_is_cold(inode))
-			return false;
-	}
-	return true;
+	return __init_may_extent_tree(inode, type);
 }
 
 static void __try_update_largest_extent(struct extent_tree *et,
@@ -391,7 +402,7 @@ void f2fs_init_read_extent_tree(struct inode *inode, struct page *ipage)
 	struct f2fs_extent *i_ext = &F2FS_INODE(ipage)->i_ext;
 	struct extent_tree *et;
 	struct extent_node *en;
-	struct extent_info ei = {0};
+	struct extent_info ei;
 
 	if (!__may_extent_tree(inode, EX_READ)) {
 		/* drop largest read extent */
diff --git a/fs/f2fs/f2fs.h b/fs/f2fs/f2fs.h
index 9dcac4e08..3f90ff82b 100644
--- a/fs/f2fs/f2fs.h
+++ b/fs/f2fs/f2fs.h
@@ -60,18 +60,52 @@ enum {
 	FAULT_SLAB_ALLOC,
 	FAULT_DQUOT_INIT,
 	FAULT_LOCK_OP,
-	FAULT_BLKADDR_VALIDITY,
-	FAULT_BLKADDR_CONSISTENCE,
+	FAULT_BLKADDR,
+#ifdef CONFIG_F2FS_APPBOOST
+	FAULT_READ_ERROR,
+	FAULT_WRITE_ERROR,
+	FAULT_PAGE_ERROR,
+	FAULT_FSYNC_ERROR,
+	FAULT_FLUSH_ERROR,
+	FAULT_WRITE_TAIL_ERROR,
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+	FAULT_DEDUP_WRITEBACK,
+	FAULT_DEDUP_TRUNCATE,
+	FAULT_DEDUP_FILL_INODE,
+	FAULT_DEDUP_SAME_FILE,
+	FAULT_DEDUP_PARAM_CHECK,
+	FAULT_DEDUP_CRYPT_POLICY,
+	FAULT_DEDUP_REVOKE,
+	FAULT_DEDUP_ORPHAN_INODE,
+	FAULT_DEDUP_INIT_INNER,
+	FAULT_DEDUP_HOLE,
+	FAULT_DEDUP_CLONE,
+	FAULT_DEDUP_OPEN,
+	FAULT_DEDUP_SETXATTR,
+#endif
+	FAULT_COMPRESS_REDIRTY,
+	FAULT_COMPRESS_WRITEBACK,
+	FAULT_COMPRESS_RESERVE_NOSPC,
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	FAULT_COMPRESS_VMAP,
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	FAULT_COMPRESS_INIT_CTX,
+	FAULT_COMPRESS_PAGE_ARRAY,
+	FAULT_COMPRESS_LOW_RATIO,
+	FAULT_COMPRESS_GET_DNODE,
+#endif
+#endif
 	FAULT_MAX,
 };
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
-#define F2FS_ALL_FAULT_TYPE		(GENMASK(FAULT_MAX - 1, 0))
+#define F2FS_ALL_FAULT_TYPE		(GENMASK_ULL(FAULT_MAX - 1, 0))
 
 struct f2fs_fault_info {
 	atomic_t inject_ops;
 	int inject_rate;
-	unsigned int inject_type;
+	unsigned long long inject_type;
 };
 
 extern const char *f2fs_fault_name[FAULT_MAX];
@@ -115,6 +149,8 @@ extern const char *f2fs_fault_name[FAULT_MAX];
 #define F2FS_MOUNT_COMPRESS_CACHE	0x40000000
 #define F2FS_MOUNT_AGE_EXTENT_CACHE	0x80000000
 
+#define F2FS_MOUNT_FILE_DEDUP		0x80000000
+
 #define F2FS_OPTION(sbi)	((sbi)->mount_opt)
 #define clear_opt(sbi, option)	(F2FS_OPTION(sbi).opt &= ~F2FS_MOUNT_##option)
 #define set_opt(sbi, option)	(F2FS_OPTION(sbi).opt |= F2FS_MOUNT_##option)
@@ -146,6 +182,9 @@ struct f2fs_rwsem {
 #endif
 };
 
+#define OPLUS_FEAT_COMPR	0x1
+#define OPLUS_FEAT_DEDUP	0x2
+
 struct f2fs_mount_info {
 	unsigned int opt;
 	int write_io_size_bits;		/* Write IO size bits */
@@ -187,6 +226,9 @@ struct f2fs_mount_info {
 	unsigned char compress_ext_cnt;		/* extension count */
 	unsigned char nocompress_ext_cnt;		/* nocompress extension count */
 	int compress_mode;			/* compression mode */
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	unsigned char compress_layout;		/* compression layout */
+#endif
 	unsigned char extensions[COMPRESS_EXT_NUM][F2FS_EXTENSION_LEN];	/* extensions */
 	unsigned char noextensions[COMPRESS_EXT_NUM][F2FS_EXTENSION_LEN]; /* extensions */
 };
@@ -206,6 +248,15 @@ struct f2fs_mount_info {
 #define F2FS_FEATURE_CASEFOLD		0x1000
 #define F2FS_FEATURE_COMPRESSION	0x2000
 #define F2FS_FEATURE_RO			0x4000
+#ifdef CONFIG_F2FS_SEQZONE
+#define F2FS_FEATURE_SEQZONE		0x20000000
+#define F2FS_NR_CPUS			(8)
+enum {
+	ENABLE_SEQZONE_EXCEPT_COLD = 1,
+	ENABLE_SEQZONE_HOT_FILES,
+};
+#endif
+#define F2FS_FEATURE_DEDUP		0x80000000
 
 #define __F2FS_HAS_FEATURE(raw_super, mask)				\
 	((raw_super->feature & cpu_to_le32(mask)) != 0)
@@ -380,6 +431,12 @@ enum {
 	MAX_DPOLICY,
 };
 
+enum {
+	DPOLICY_IO_AWARE_DISABLE,	/* force to not be aware of IO */
+	DPOLICY_IO_AWARE_ENABLE,	/* force to be aware of IO */
+	DPOLICY_IO_AWARE_MAX,
+};
+
 struct discard_policy {
 	int type;			/* type of discard */
 	unsigned int min_interval;	/* used for candidates exist */
@@ -412,6 +469,7 @@ struct discard_cmd_control {
 	unsigned int discard_urgent_util;	/* utilization which issue discard proactively */
 	unsigned int discard_granularity;	/* discard granularity */
 	unsigned int max_ordered_discard;	/* maximum discard granularity issued by lba order */
+	unsigned int discard_io_aware;		/* io_aware policy */
 	unsigned int undiscard_blks;		/* # of undiscard blocks */
 	unsigned int next_pos;			/* next discard position */
 	atomic_t issued_discard;		/* # of issued discard */
@@ -693,6 +751,9 @@ struct f2fs_map_blocks {
 	struct block_device *m_bdev;	/* for multi-device dio */
 	block_t m_pblk;
 	block_t m_lblk;
+#ifdef CONFIG_F2FS_SEQZONE
+	block_t m_seqblk;
+#endif
 	unsigned int m_len;
 	unsigned int m_flags;
 	pgoff_t *m_next_pgofs;		/* point next possible non-hole pgofs */
@@ -800,7 +861,26 @@ enum {
 	FI_ATOMIC_COMMITTED,	/* indicate atomic commit completed except disk sync */
 	FI_ATOMIC_DIRTIED,	/* indicate atomic file is dirtied */
 	FI_ATOMIC_REPLACE,	/* indicate atomic replace */
-	FI_OPENED_FILE,         /* indicate file has been opened */
+#ifdef CONFIG_F2FS_FS_DEDUP
+	/* use for dedup */
+	FI_DEDUPED,
+	FI_INNER_INODE,
+	FI_UPDATED,
+	FI_REVOKE_DEDUP,
+	FI_DOING_DEDUP,
+	FI_META_UN_MODIFY,
+	FI_DATA_UN_MODIFY,
+#ifdef CONFIG_F2FS_APPBOOST
+	FI_MERGED_FILE,
+#endif
+	/* use for snapshot */
+	FI_SNAPSHOT_PREPARED,
+	FI_SNAPSHOTED,
+#ifdef CONFIG_F2FS_SEQZONE
+	FI_SEQZONE,		/* enable seqzone */
+#endif
+#endif
+    FI_OPENED_FILE,         /* indicate file has been opened */
 	FI_MAX,			/* max flag, never be used */
 };
 
@@ -865,6 +945,20 @@ struct f2fs_inode_info {
 
 	unsigned int atomic_write_cnt;
 	loff_t original_i_size;		/* original i_size before atomic write */
+#ifdef CONFIG_F2FS_APPBOOST
+	struct fi_merge_manage *i_boostfile; /* boostfile merge info */
+	atomic_t appboost_abort;
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner_inode;
+	atomic_t inflight_read_io;
+	wait_queue_head_t dedup_wq;
+	unsigned long long dedup_cp_ver;
+#endif
+#ifdef CONFIG_F2FS_SEQZONE
+	unsigned long long dio_cur_seqindex;
+#endif
 };
 
 static inline void get_read_extent_info(struct extent_info *ext,
@@ -976,6 +1070,10 @@ struct dnode_of_data {
 	char cur_level;			/* level of hole node page */
 	char max_level;			/* level of current page located */
 	block_t	data_blkaddr;		/* block address of the node block */
+#ifdef CONFIG_F2FS_SEQZONE
+	block_t seqzone_index;
+	bool use_seqzone;
+#endif
 };
 
 static inline void set_new_dnode(struct dnode_of_data *dn, struct inode *inode,
@@ -1003,7 +1101,11 @@ static inline void set_new_dnode(struct dnode_of_data *dn, struct inode *inode,
  */
 #define	NR_CURSEG_DATA_TYPE	(3)
 #define NR_CURSEG_NODE_TYPE	(3)
+#ifdef CONFIG_F2FS_SEQZONE
+#define NR_CURSEG_INMEM_TYPE	(18)
+#else
 #define NR_CURSEG_INMEM_TYPE	(2)
+#endif
 #define NR_CURSEG_RO_TYPE	(2)
 #define NR_CURSEG_PERSIST_TYPE	(NR_CURSEG_DATA_TYPE + NR_CURSEG_NODE_TYPE)
 #define NR_CURSEG_TYPE		(NR_CURSEG_INMEM_TYPE + NR_CURSEG_PERSIST_TYPE)
@@ -1019,6 +1121,24 @@ enum {
 	CURSEG_COLD_DATA_PINNED = NR_PERSISTENT_LOG,
 				/* pinned file that needs consecutive block address */
 	CURSEG_ALL_DATA_ATGC,	/* SSR alloctor in hot/warm/cold data area */
+#ifdef CONFIG_F2FS_SEQZONE
+	CURSEG_HOT_DATA_0,
+	CURSEG_HOT_DATA_1,
+	CURSEG_HOT_DATA_2,
+	CURSEG_HOT_DATA_3,
+	CURSEG_HOT_DATA_4,
+	CURSEG_HOT_DATA_5,
+	CURSEG_HOT_DATA_6,
+	CURSEG_HOT_DATA_7,
+	CURSEG_WARM_DATA_0,
+	CURSEG_WARM_DATA_1,
+	CURSEG_WARM_DATA_2,
+	CURSEG_WARM_DATA_3,
+	CURSEG_WARM_DATA_4,
+	CURSEG_WARM_DATA_5,
+	CURSEG_WARM_DATA_6,
+	CURSEG_WARM_DATA_7,
+#endif
 	NO_CHECK_TYPE,		/* number of persistent & inmem log */
 };
 
@@ -1084,8 +1204,7 @@ struct f2fs_sm_info {
  * f2fs monitors the number of several block types such as on-writeback,
  * dirty dentry blocks, dirty node blocks, and dirty meta blocks.
  */
-#define WB_DATA_TYPE(p, f)			\
-	(f || f2fs_is_cp_guaranteed(p) ? F2FS_WB_CP_DATA : F2FS_WB_DATA)
+#define WB_DATA_TYPE(p)	(__is_cp_guaranteed(p) ? F2FS_WB_CP_DATA : F2FS_WB_DATA)
 enum count_type {
 	F2FS_DIRTY_DENTS,
 	F2FS_DIRTY_DATA,
@@ -1150,6 +1269,7 @@ enum cp_reason_type {
 	CP_FASTBOOT_MODE,
 	CP_SPEC_LOG_NUM,
 	CP_RECOVER_DIR,
+	CP_DEDUPED,
 	CP_XATTR_DIR,
 };
 
@@ -1199,6 +1319,11 @@ struct f2fs_io_info {
 	blk_opf_t op_flags;	/* req_flag_bits */
 	block_t new_blkaddr;	/* new block address to be written */
 	block_t old_blkaddr;	/* old block address before Cow */
+#ifdef CONFIG_F2FS_SEQZONE
+	bool use_seqzone;
+	u32 seqzone_index;
+	int real_temp;
+#endif
 	struct page *page;	/* page to be written */
 	struct page *encrypted_page;	/* encrypted page */
 	struct page *compressed_page;	/* compressed page */
@@ -1245,7 +1370,7 @@ struct f2fs_bio_info {
 #define RDEV(i)				(raw_super->devs[i])
 struct f2fs_dev_info {
 	struct block_device *bdev;
-	char path[MAX_PATH_LEN + 1];
+	char path[MAX_PATH_LEN];
 	unsigned int total_segments;
 	block_t start_blk;
 	block_t end_blk;
@@ -1428,9 +1553,24 @@ enum compress_algorithm_type {
 
 enum compress_flag {
 	COMPRESS_CHKSUM,
+	COMPRESS_LAYOUT,
+	COMPRESS_ATIME = 3,
+	COMPRESS_RESERVED = 4,
+	COMPRESS_LEVEL = 8,
 	COMPRESS_MAX_FLAG,
 };
 
+#define COMPRESS_CHKSUM_MASK	0x1
+#define COMPRESS_LAYOUT_MASK	0x6
+#define COMPRESS_ATIME_MASK	0x8
+#define COMPRESS_LEVEL_MASK	0xf0
+
+enum compress_layout_type {
+	COMPRESS_FIXED_INPUT,
+	COMPRESS_FIXED_OUTPUT,
+	COMPRESS_LAYOUT_MAX,
+};
+
 #define	COMPRESS_WATERMARK			20
 #define	COMPRESS_PERCENT			20
 
@@ -1446,10 +1586,130 @@ struct compress_data {
 
 #define F2FS_COMPRESSED_PAGE_MAGIC	0xF5F2C000
 
-#define F2FS_ZSTD_DEFAULT_CLEVEL	1
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+/*
+ compress metadata layout at inode/dnode
+
+ f2fs_inode.i_addr[] or direct_node.addr[] space management
+
+ original layout
+ +--------------------+
+ |                    |
+ |                    |
+ |     addrs[]        +--> total block address array size, get via addrs_per_inode() / addrs_per_block()
+ |                    |
+ |                    |
+ +--------------------+
+
+ compression fixed-output layout
+ +--------------------+
+ |     addrs[]        |--> block address array size, get via addrs_per_inode() / addrs_per_block()
+ |                    |
+ +--------------------+--> decompress_index array start blkaddr, get via decompress_index_addr()
+ | decompress_index[] |
+ |                    |--> di array size, the size is the same as blkaddr array size
+ +--------------------+
+
+ [compress fixed-output layout]
+
+  |-- page 0--|-- page 1--|-- page 2--|-- page 3--|-- page 4--|
+  |           |          /           /
+  |          /          /          /
+  |         /          /         /
+  |        /         /        /
+  |       /        /       /
+  |      /       /      /
+  |-- blk 0 --|-- blk 1 --|-- blk 2 --|
+  ^      ^       ^      ^
+  |      |       |      |
+ [0]   [ofs1]   [1]   [ofs2]
+
+  page 0
+  first_page = 1
+  cross_block = 0
+  blkidx = 0
+  ofs = 0
+              page 1
+              first_page = 1
+              cross_block = 1
+              blkidx = 1
+              ofs = [ofs1]
+                          page 2
+                          first_page = 0
+                          cross_block = 0
+                          blkidx = 1
+                          ofs = 1 (distance from first page)
+                                      page 3
+                                      first_page = 1
+                                      cross_block = 1
+                                      blkidx = 2
+                                      ofs = [ofs2]
+ */
+struct decompress_index {
+	unsigned int is_valid:1;	/* indicate entry is valid or not */
+	unsigned int is_compress:1;	/* indicate page is compressed */
+	unsigned int first_page:1;	/* first page in compressed block */
+	unsigned int cross_block:1;	/* cross two compressed blocks */
+	/*
+	 * indicate compressed/raw block index that page locates, if it
+	 * is cross block, its blkidx equals to second(last) compressed
+	 * block's index
+	 */
+	unsigned int blkidx:8;
+	/*
+	 * when .first_page is one, @ofs indicates:
+	 * offset in raw page, which is start offset of compressed blocks
+	 * when .first_page is zero, @ofs indicates:
+	 * distance from first page in compressed block
+	 */
+	unsigned int ofs:12;
+	unsigned int padding:8;
+} __packed;
 
-#define	COMPRESS_LEVEL_OFFSET	8
+typedef u32 decompress_index_t;
 
+#define DI_VALID_OFFS		0
+#define DI_COMPRESSED_OFFS	1
+#define DI_FIRST_PAGE_OFFS	2
+#define DI_CROSS_BLOCK_OFFS	3
+#define DI_BLKID_OFFS		4
+#define DI_OFS_OFFS		12
+
+static decompress_index_t inline
+serialize_decompress_index(struct decompress_index *di)
+{
+	decompress_index_t val = 0;
+
+	val |= di->is_valid << DI_VALID_OFFS;
+	val |= di->is_compress << DI_COMPRESSED_OFFS;
+	val |= di->first_page << DI_FIRST_PAGE_OFFS;
+	val |= di->cross_block << DI_CROSS_BLOCK_OFFS;
+	val |= di->blkidx << DI_BLKID_OFFS;
+	val |= di->ofs << DI_OFS_OFFS;
+
+	return val;
+}
+
+static void inline deserialize_decompress_index(decompress_index_t val,
+						struct decompress_index *di)
+{
+	di->is_valid = (val & (1 << DI_VALID_OFFS)) >> DI_VALID_OFFS;
+	di->is_compress = (val & (1 << DI_COMPRESSED_OFFS)) >> DI_COMPRESSED_OFFS;
+	di->first_page = (val & (1 << DI_FIRST_PAGE_OFFS)) >> DI_FIRST_PAGE_OFFS;
+	di->cross_block = (val & (1 << DI_CROSS_BLOCK_OFFS)) >> DI_CROSS_BLOCK_OFFS;
+	di->blkidx = (val & (0xff << DI_BLKID_OFFS)) >> DI_BLKID_OFFS;
+	di->ofs = (val & (0xfff << DI_OFS_OFFS)) >> DI_OFS_OFFS;
+}
+
+#define MAX_BLKS_PER_CLUSTER		(1 << 3) // MAX_COMPRESS_LOG_SIZE
+
+#define CLEAR_IFLAG_IF_SET(inode, flag)                                        \
+	if (F2FS_I(inode)->i_flags & flag) {                                   \
+		F2FS_I(inode)->i_flags &= ~flag;                               \
+		f2fs_mark_inode_dirty_sync(inode, true);                       \
+	}
+#endif
+#define F2FS_ZSTD_DEFAULT_CLEVEL	1
 /* compress context */
 struct compress_ctx {
 	struct inode *inode;		/* inode the context belong to */
@@ -1465,6 +1725,11 @@ struct compress_ctx {
 	struct compress_data *cbuf;	/* virtual mapped address on cpages */
 	size_t rlen;			/* valid data length in rbuf */
 	size_t clen;			/* valid data length in cbuf */
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	struct decompress_index di[MAX_BLKS_PER_CLUSTER];
+	unsigned int rofs;		/* offset in rbuf */
+	unsigned int cofs;		/* offset in cbuf */
+#endif
 	void *private;			/* payload buffer for specified compression algorithm */
 	void *private2;			/* extra payload buffer */
 };
@@ -1485,16 +1750,19 @@ struct decompress_io_ctx {
 	pgoff_t cluster_idx;		/* cluster index number */
 	unsigned int cluster_size;	/* page count in cluster */
 	unsigned int log_cluster_size;	/* log of cluster size */
-	struct page **rpages;		/* pages store raw data in cluster */
+	struct page *rpages[MAX_BLKS_PER_CLUSTER];		/* pages store raw data in cluster */
 	unsigned int nr_rpages;		/* total page number in rpages */
-	struct page **cpages;		/* pages store compressed data in cluster */
+	struct page *cpages[MAX_BLKS_PER_CLUSTER];		/* pages store compressed data in cluster */
 	unsigned int nr_cpages;		/* total page number in cpages */
-	struct page **tpages;		/* temp pages to pad holes in cluster */
+	struct page *tpages[MAX_BLKS_PER_CLUSTER];		/* temp pages to pad holes in cluster */
 	void *rbuf;			/* virtual mapped address on rpages */
 	struct compress_data *cbuf;	/* virtual mapped address on cpages */
 	size_t rlen;			/* valid data length in rbuf */
 	size_t clen;			/* valid data length in cbuf */
-
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	unsigned int rofs;
+	unsigned int cofs;
+#endif
 	/*
 	 * The number of compressed pages remaining to be read in this cluster.
 	 * This is initially nr_cpages.  It is decremented by 1 each time a page
@@ -1526,6 +1794,12 @@ struct decompress_io_ctx {
 	void *private2;			/* extra payload buffer */
 	struct work_struct verity_work;	/* work to verify the decompressed pages */
 	struct work_struct free_work;	/* work for late free this structure itself */
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	int current_blk;
+	struct decompress_index di[MAX_BLKS_PER_CLUSTER];
+	char inplace_io[MAX_BLKS_PER_CLUSTER];
+#endif
 };
 
 #define NULL_CLUSTER			((unsigned int)(~0))
@@ -1794,6 +2068,17 @@ struct f2fs_sb_info {
 	spinlock_t iostat_lat_lock;
 	struct iostat_lat_info *iostat_io_lat;
 #endif
+
+	unsigned int oplus_feats;
+
+#ifdef CONFIG_F2FS_APPBOOST
+	int appboost;
+	unsigned int appboost_max_blocks;
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+	int seq_zone;
+#endif
 };
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
@@ -2264,7 +2549,7 @@ static inline void f2fs_i_blocks_write(struct inode *, block_t, bool, bool);
 static inline int inc_valid_block_count(struct f2fs_sb_info *sbi,
 				 struct inode *inode, blkcnt_t *count, bool partial)
 {
-	long long diff = 0, release = 0;
+	blkcnt_t diff = 0, release = 0;
 	block_t avail_user_block_count;
 	int ret;
 
@@ -2284,27 +2569,26 @@ static inline int inc_valid_block_count(struct f2fs_sb_info *sbi,
 	percpu_counter_add(&sbi->alloc_valid_block_count, (*count));
 
 	spin_lock(&sbi->stat_lock);
-
+	sbi->total_valid_block_count += (block_t)(*count);
 	avail_user_block_count = get_available_block_count(sbi, inode, true);
-	diff = (long long)sbi->total_valid_block_count + *count -
-						avail_user_block_count;
-	if (unlikely(diff > 0)) {
+
+	if (unlikely(sbi->total_valid_block_count > avail_user_block_count)) {
 		if (!partial) {
 			spin_unlock(&sbi->stat_lock);
-			release = *count;
 			goto enospc;
 		}
+
+		diff = sbi->total_valid_block_count - avail_user_block_count;
 		if (diff > *count)
 			diff = *count;
 		*count -= diff;
 		release = diff;
+		sbi->total_valid_block_count -= diff;
 		if (!*count) {
 			spin_unlock(&sbi->stat_lock);
 			goto enospc;
 		}
 	}
-	sbi->total_valid_block_count += (block_t)(*count);
-
 	spin_unlock(&sbi->stat_lock);
 
 	if (unlikely(release)) {
@@ -2420,14 +2704,8 @@ static inline void dec_valid_block_count(struct f2fs_sb_info *sbi,
 	blkcnt_t sectors = count << F2FS_LOG_SECTORS_PER_BLOCK;
 
 	spin_lock(&sbi->stat_lock);
-	if (unlikely(sbi->total_valid_block_count < count)) {
-		f2fs_warn(sbi, "Inconsistent total_valid_block_count:%u, ino:%lu, count:%u",
-			  sbi->total_valid_block_count, inode->i_ino, count);
-		sbi->total_valid_block_count = 0;
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-	} else {
-		sbi->total_valid_block_count -= count;
-	}
+	f2fs_bug_on(sbi, sbi->total_valid_block_count < (block_t) count);
+	sbi->total_valid_block_count -= (block_t)count;
 	if (sbi->reserved_blocks &&
 		sbi->current_reserved_blocks < sbi->reserved_blocks)
 		sbi->current_reserved_blocks = min(sbi->reserved_blocks,
@@ -2985,6 +3263,25 @@ static inline void f2fs_change_bit(unsigned int nr, char *addr)
 #define F2FS_PROJINHERIT_FL		0x20000000 /* Create with parents projid */
 #define F2FS_CASEFOLD_FL		0x40000000 /* Casefolded file */
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+#define F2FS_DEDUPED_FL			0x00000001
+#define F2FS_INNER_FL			0x00000002
+#define F2FS_UPDATED_FL			0x00000004
+#define F2FS_REVOKE_FL			0x00000008
+#define F2FS_DOING_DEDUP_FL		0x00000010
+
+#define F2FS_META_UN_MODIFY_FL		0x00000100
+#define F2FS_DATA_UN_MODIFY_FL		0x00000200
+#ifdef CONFIG_F2FS_APPBOOST
+#define F2FS_MERGED_FILE_FL		0x00000400
+#endif
+#define F2FS_SNAPSHOT_PREPARED_FL	0x00000800
+#define F2FS_SNAPSHOTED_FL		0x00001000
+#ifdef CONFIG_F2FS_SEQZONE
+#define F2FS_SEQZONE_FL			0x00002000 /* Seqzone file */
+#endif
+#endif
+
 /* Flags that should be inherited by new inodes from their parent. */
 #define F2FS_FL_INHERITED (F2FS_SYNC_FL | F2FS_NODUMP_FL | F2FS_NOATIME_FL | \
 			   F2FS_DIRSYNC_FL | F2FS_PROJINHERIT_FL | \
@@ -3021,6 +3318,22 @@ static inline void __mark_inode_dirty_flag(struct inode *inode,
 	case FI_DATA_EXIST:
 	case FI_PIN_FILE:
 	case FI_COMPRESS_RELEASED:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	case FI_DEDUPED:
+	case FI_INNER_INODE:
+	case FI_REVOKE_DEDUP:
+	case FI_DOING_DEDUP:
+	case FI_META_UN_MODIFY:
+	case FI_DATA_UN_MODIFY:
+#ifdef CONFIG_F2FS_APPBOOST
+	case FI_MERGED_FILE:
+#endif
+	case FI_SNAPSHOT_PREPARED:
+	case FI_SNAPSHOTED:
+#ifdef CONFIG_F2FS_SEQZONE
+	case FI_SEQZONE:
+#endif
+#endif
 		f2fs_mark_inode_dirty_sync(inode, true);
 	}
 }
@@ -3186,6 +3499,13 @@ static inline int f2fs_compressed_file(struct inode *inode)
 		is_inode_flag_set(inode, FI_COMPRESSED_FILE);
 }
 
+#ifdef CONFIG_F2FS_SEQZONE
+static inline int f2fs_seqzone_file(struct inode *inode)
+{
+	return S_ISREG(inode->i_mode) &&
+		is_inode_flag_set(inode, FI_SEQZONE);
+}
+#endif
 static inline bool f2fs_need_compress_data(struct inode *inode)
 {
 	int compress_mode = F2FS_OPTION(F2FS_I_SB(inode)).compress_mode;
@@ -3202,23 +3522,89 @@ static inline bool f2fs_need_compress_data(struct inode *inode)
 	return false;
 }
 
+static inline int f2fs_compress_layout(struct inode *inode)
+{
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	__u16 flag = F2FS_I(inode)->i_compress_flag;
+
+	f2fs_bug_on(F2FS_I_SB(inode), !S_ISREG(inode->i_mode));
+	return (flag & COMPRESS_LAYOUT_MASK) >> COMPRESS_LAYOUT;
+#else
+	return COMPRESS_FIXED_INPUT;
+#endif
+}
+
 static inline unsigned int addrs_per_inode(struct inode *inode)
 {
 	unsigned int addrs = CUR_ADDRS_PER_INODE(inode) -
 				get_inline_xattr_addrs(inode);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		return addrs / 2;
+#endif
 	if (!f2fs_compressed_file(inode))
 		return addrs;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compress_layout(inode) == COMPRESS_FIXED_INPUT)
+		return ALIGN_DOWN(addrs, F2FS_I(inode)->i_cluster_size);
+	return ALIGN_DOWN(addrs, F2FS_I(inode)->i_cluster_size * 2) / 2;
+#else
 	return ALIGN_DOWN(addrs, F2FS_I(inode)->i_cluster_size);
+#endif
 }
 
 static inline unsigned int addrs_per_block(struct inode *inode)
 {
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		return DEF_ADDRS_PER_BLOCK / 2;
+#endif
 	if (!f2fs_compressed_file(inode))
 		return DEF_ADDRS_PER_BLOCK;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compress_layout(inode) == COMPRESS_FIXED_INPUT)
+		return ALIGN_DOWN(DEF_ADDRS_PER_BLOCK, F2FS_I(inode)->i_cluster_size);
+	return ALIGN_DOWN(DEF_ADDRS_PER_BLOCK, F2FS_I(inode)->i_cluster_size * 2) / 2;
+#else
 	return ALIGN_DOWN(DEF_ADDRS_PER_BLOCK, F2FS_I(inode)->i_cluster_size);
+#endif
+}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+static inline void *decompress_index_addr(struct inode *inode, struct page *page)
+{
+	struct f2fs_node *rn = F2FS_NODE(page);
+	__le32 *addr_array = blkaddr_in_node(rn);
+	int addrs = ADDRS_PER_PAGE(page, inode);
+	int base = 0;
+
+	if (IS_INODE(page) && f2fs_has_extra_attr(inode))
+		base = get_extra_isize(inode);
+
+	return addr_array + base + addrs;
+}
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+static inline int __set_seqzone_index(struct dnode_of_data *dn)
+{
+	struct f2fs_node *rn = F2FS_NODE(dn->node_page);
+	__le32 *addr_array;
+	int base = 0;
+	int addrs = ADDRS_PER_PAGE(dn->node_page, dn->inode);
+
+	if (IS_INODE(dn->node_page) && f2fs_has_extra_attr(dn->inode))
+		base = get_extra_isize(dn->inode);
+
+	addr_array = blkaddr_in_node(rn);
+	if (f2fs_seqzone_file(dn->inode))
+		addr_array[base + dn->ofs_in_node + addrs] = cpu_to_le32(dn->seqzone_index);
+
+	return 0;
 }
+#endif
 
+extern void __set_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr);
 static inline void *inline_xattr_addr(struct inode *inode, struct page *page)
 {
 	struct f2fs_inode *ri = F2FS_INODE(page);
@@ -3282,6 +3668,155 @@ static inline int f2fs_has_inline_dentry(struct inode *inode)
 	return is_inode_flag_set(inode, FI_INLINE_DENTRY);
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+static inline void get_dedup_flags_info(struct inode *inode, struct f2fs_inode *ri)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	u32 i_dedup_flags = le32_to_cpu(ri->i_dedup_flags);
+	if (i_dedup_flags & F2FS_DEDUPED_FL)
+		set_bit(FI_DEDUPED, fi->flags);
+	if (i_dedup_flags & F2FS_INNER_FL)
+		set_bit(FI_INNER_INODE, fi->flags);
+	if (i_dedup_flags & F2FS_UPDATED_FL)
+		set_bit(FI_UPDATED, fi->flags);
+	if (i_dedup_flags & F2FS_REVOKE_FL)
+		set_bit(FI_REVOKE_DEDUP, fi->flags);
+	if (i_dedup_flags & F2FS_DOING_DEDUP_FL)
+		set_bit(FI_DOING_DEDUP, fi->flags);
+	if (i_dedup_flags & F2FS_META_UN_MODIFY_FL)
+		set_bit(FI_META_UN_MODIFY, fi->flags);
+	if (i_dedup_flags & F2FS_DATA_UN_MODIFY_FL)
+		set_bit(FI_DATA_UN_MODIFY, fi->flags);
+#ifdef CONFIG_F2FS_APPBOOST
+	if (i_dedup_flags & F2FS_MERGED_FILE_FL)
+		set_bit(FI_MERGED_FILE, fi->flags);
+#endif
+	if (i_dedup_flags & F2FS_SNAPSHOT_PREPARED_FL)
+		set_bit(FI_SNAPSHOT_PREPARED, fi->flags);
+	if (i_dedup_flags & F2FS_SNAPSHOTED_FL)
+		set_bit(FI_SNAPSHOTED, fi->flags);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (i_dedup_flags & F2FS_SEQZONE_FL)
+		set_bit(FI_SEQZONE, fi->flags);
+#endif
+}
+
+static inline void set_raw_dedup_flags(struct inode *inode, struct f2fs_inode *ri)
+{
+	u32 i_dedup_flags = 0;
+
+	if (is_inode_flag_set(inode, FI_DEDUPED))
+		i_dedup_flags |= F2FS_DEDUPED_FL;
+	if (is_inode_flag_set(inode, FI_INNER_INODE))
+		i_dedup_flags |= F2FS_INNER_FL;
+	if (is_inode_flag_set(inode, FI_UPDATED))
+		i_dedup_flags |= F2FS_UPDATED_FL;
+	if (is_inode_flag_set(inode, FI_REVOKE_DEDUP))
+		i_dedup_flags |= F2FS_REVOKE_FL;
+	if (is_inode_flag_set(inode, FI_DOING_DEDUP))
+		i_dedup_flags |= F2FS_DOING_DEDUP_FL;
+	if (is_inode_flag_set(inode, FI_META_UN_MODIFY))
+		i_dedup_flags |= F2FS_META_UN_MODIFY_FL;
+	if (is_inode_flag_set(inode, FI_DATA_UN_MODIFY))
+		i_dedup_flags |= F2FS_DATA_UN_MODIFY_FL;
+#ifdef CONFIG_F2FS_APPBOOST
+	if (is_inode_flag_set(inode, FI_MERGED_FILE))
+		i_dedup_flags |= F2FS_MERGED_FILE_FL;
+#endif
+	if (is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		i_dedup_flags |= F2FS_SNAPSHOT_PREPARED_FL;
+	if (is_inode_flag_set(inode, FI_SNAPSHOTED))
+		i_dedup_flags |= F2FS_SNAPSHOTED_FL;
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (is_inode_flag_set(inode, FI_SEQZONE))
+		i_dedup_flags |= F2FS_SEQZONE_FL;
+#endif
+	ri->i_dedup_flags = cpu_to_le32(i_dedup_flags);
+}
+
+static inline int f2fs_is_deduped_inode(struct inode *inode)
+{
+	return is_inode_flag_set(inode, FI_DEDUPED);
+}
+
+static inline int f2fs_is_inner_inode(struct inode *inode)
+{
+	if (!f2fs_is_deduped_inode(inode))
+		return false;
+	return is_inode_flag_set(inode, FI_INNER_INODE);
+}
+
+static inline int f2fs_is_outer_inode(struct inode *inode)
+{
+	if (!f2fs_is_deduped_inode(inode))
+		return false;
+	return !f2fs_is_inner_inode(inode);
+}
+
+static inline struct inode *get_inner_inode(struct inode *inode)
+{
+	struct inode *inner = NULL;
+
+	f2fs_down_read(&F2FS_I(inode)->i_sem);
+	if (f2fs_is_outer_inode(inode))
+		inner = igrab(F2FS_I(inode)->inner_inode);
+	f2fs_up_read(&F2FS_I(inode)->i_sem);
+	return inner;
+}
+
+static inline void put_inner_inode(struct inode *inode)
+{
+	if (inode) {
+		f2fs_bug_on(F2FS_I_SB(inode), !f2fs_is_inner_inode(inode));
+		iput(inode);
+	}
+}
+
+static inline void mark_file_modified(struct inode *inode)
+{
+	if (!is_inode_flag_set(inode, FI_DATA_UN_MODIFY))
+		return;
+	clear_bit(FI_DATA_UN_MODIFY, F2FS_I(inode)->flags);
+	f2fs_mark_inode_dirty_sync(inode, true);
+}
+
+static inline void inode_dec_read_io(struct inode *inode)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+
+	if (atomic_read(&fi->inflight_read_io) == 0) {
+		f2fs_err(F2FS_I_SB(inode),
+			"F2FS-fs: inode [%lu] inflight_read_io refcount may leak",
+			inode->i_ino);
+		wake_up_all(&fi->dedup_wq);
+		return;
+	}
+
+	if (atomic_dec_and_test(&fi->inflight_read_io) &&
+			wq_has_sleeper(&fi->dedup_wq))
+		wake_up_all(&fi->dedup_wq);
+}
+
+static inline void inode_inc_read_io(struct inode *inode)
+{
+	atomic_inc(&F2FS_I(inode)->inflight_read_io);
+}
+
+#ifdef CONFIG_F2FS_CHECK_FS
+#define f2fs_dedup_info(sbi, fmt, ...) \
+	f2fs_printk(sbi, KERN_INFO fmt, ##__VA_ARGS__)
+#else
+#define f2fs_dedup_info(sbi, fmt, ...) do {} while(0)
+#endif
+
+#else
+static inline int f2fs_is_deduped_inode(struct inode *inode)
+{
+	return 0;
+}
+#endif
+
 static inline int is_file(struct inode *inode, int type)
 {
 	return F2FS_I(inode)->i_advise & type;
@@ -3421,7 +3956,11 @@ static inline __le32 *get_dnode_addr(struct inode *inode,
 
 #define F2FS_TOTAL_EXTRA_ATTR_SIZE			\
 	(offsetof(struct f2fs_inode, i_extra_end) -	\
-	offsetof(struct f2fs_inode, i_extra_isize))	\
+	offsetof(struct f2fs_inode, i_extra_isize))
+#define F2FS_LEGACY_EXTRA_ATTR_SIZE			\
+	(offsetof(struct f2fs_inode, i_compress_flag) -	\
+	offsetof(struct f2fs_inode, i_extra_isize) +	\
+	sizeof(__le16))
 
 #define F2FS_OLD_ATTRIBUTE_SIZE	(offsetof(struct f2fs_inode, i_addr))
 #define F2FS_FITS_IN_INODE(f2fs_inode, extra_isize, field)		\
@@ -3438,15 +3977,17 @@ bool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
 static inline void verify_blkaddr(struct f2fs_sb_info *sbi,
 					block_t blkaddr, int type)
 {
-	if (!f2fs_is_valid_blkaddr(sbi, blkaddr, type))
+	if (!f2fs_is_valid_blkaddr(sbi, blkaddr, type)) {
 		f2fs_err(sbi, "invalid blkaddr: %u, type: %d, run fsck to fix.",
 			 blkaddr, type);
+		f2fs_bug_on(sbi, 1);
+	}
 }
 
 static inline bool __is_valid_data_blkaddr(block_t blkaddr)
 {
 	if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR ||
-			blkaddr == COMPRESS_ADDR)
+			blkaddr == COMPRESS_ADDR || blkaddr == DEDUP_ADDR)
 		return false;
 	return true;
 }
@@ -3473,6 +4014,21 @@ long f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
 long f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
 int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid);
 int f2fs_pin_file_control(struct inode *inode, bool inc);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+int f2fs_reserve_compress_blocks(struct inode *inode, unsigned int *ret_rsvd_blks);
+int f2fs_decompress_inode(struct inode *inode);
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+int f2fs_filemap_write_and_wait_range(struct inode *inode);
+int f2fs_revoke_deduped_inode(struct inode *dedup, const char *revoke_source);
+void f2fs_drop_deduped_link(struct inode *inode);
+bool f2fs_inode_support_dedup(struct f2fs_sb_info *sbi,
+			struct inode *inode);
+int f2fs_set_inode_addr(struct inode* inode, block_t addr);
+int create_page_info_slab(void);
+void destroy_page_info_slab(void);
+#endif
 
 /*
  * inode.c
@@ -3480,6 +4036,8 @@ int f2fs_pin_file_control(struct inode *inode, bool inc);
 void f2fs_set_inode_flags(struct inode *inode);
 bool f2fs_inode_chksum_verify(struct f2fs_sb_info *sbi, struct page *page);
 void f2fs_inode_chksum_set(struct f2fs_sb_info *sbi, struct page *page);
+int f2fs_inode_chksum_get(struct f2fs_sb_info *sbi, struct inode *inode,
+			  u32 *chksum);
 struct inode *f2fs_iget(struct super_block *sb, unsigned long ino);
 struct inode *f2fs_iget_retry(struct super_block *sb, unsigned long ino);
 int f2fs_try_to_free_nats(struct f2fs_sb_info *sbi, int nr_shrink);
@@ -3497,6 +4055,7 @@ int f2fs_update_extension_list(struct f2fs_sb_info *sbi, const char *name,
 struct dentry *f2fs_get_parent(struct dentry *child);
 int f2fs_get_tmpfile(struct user_namespace *mnt_userns, struct inode *dir,
 		     struct inode **new_inode);
+void f2fs_update_atime(struct inode *inode, bool oneshot);
 
 /*
  * dir.c
@@ -3509,8 +4068,7 @@ int f2fs_prepare_lookup(struct inode *dir, struct dentry *dentry,
 			struct f2fs_filename *fname);
 void f2fs_free_filename(struct f2fs_filename *fname);
 struct f2fs_dir_entry *f2fs_find_target_dentry(const struct f2fs_dentry_ptr *d,
-			const struct f2fs_filename *fname, int *max_slots,
-			bool use_hash);
+			const struct f2fs_filename *fname, int *max_slots);
 int f2fs_fill_dentries(struct dir_context *ctx, struct f2fs_dentry_ptr *d,
 			unsigned int start_pos, struct fscrypt_str *fstr);
 void f2fs_do_make_empty_dir(struct inode *inode, struct inode *parent,
@@ -3643,8 +4201,7 @@ int f2fs_issue_flush(struct f2fs_sb_info *sbi, nid_t ino);
 int f2fs_create_flush_cmd_control(struct f2fs_sb_info *sbi);
 int f2fs_flush_device_cache(struct f2fs_sb_info *sbi);
 void f2fs_destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free);
-void f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr,
-						unsigned int len);
+void f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr);
 bool f2fs_is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr);
 int f2fs_start_discard_thread(struct f2fs_sb_info *sbi);
 void f2fs_drop_discard_cmd(struct f2fs_sb_info *sbi);
@@ -3690,7 +4247,7 @@ void f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,
 int f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,
 			block_t old_blkaddr, block_t *new_blkaddr,
 			struct f2fs_summary *sum, int type,
-			struct f2fs_io_info *fio);
+			struct f2fs_io_info *fio, bool use_seqzone);
 void f2fs_update_device_state(struct f2fs_sb_info *sbi, nid_t ino,
 					block_t blkaddr, unsigned int blkcnt);
 void f2fs_wait_on_page_writeback(struct page *page,
@@ -3737,8 +4294,6 @@ struct page *f2fs_get_meta_page_retry(struct f2fs_sb_info *sbi, pgoff_t index);
 struct page *f2fs_get_tmp_page(struct f2fs_sb_info *sbi, pgoff_t index);
 bool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
 					block_t blkaddr, int type);
-bool f2fs_is_valid_blkaddr_raw(struct f2fs_sb_info *sbi,
-					block_t blkaddr, int type);
 int f2fs_ra_meta_pages(struct f2fs_sb_info *sbi, block_t start, int nrpages,
 			int type, bool sync);
 void f2fs_ra_meta_pages_cond(struct f2fs_sb_info *sbi, pgoff_t index,
@@ -3753,6 +4308,9 @@ void f2fs_set_dirty_device(struct f2fs_sb_info *sbi, nid_t ino,
 					unsigned int devidx, int type);
 bool f2fs_is_dirty_device(struct f2fs_sb_info *sbi, nid_t ino,
 					unsigned int devidx, int type);
+#ifdef CONFIG_F2FS_FS_DEDUP
+int f2fs_truncate_dedup_inode(struct inode *inode, unsigned int flag);
+#endif
 int f2fs_acquire_orphan_inode(struct f2fs_sb_info *sbi);
 void f2fs_release_orphan_inode(struct f2fs_sb_info *sbi);
 void f2fs_add_orphan_inode(struct inode *inode);
@@ -3779,7 +4337,6 @@ void f2fs_init_ckpt_req_control(struct f2fs_sb_info *sbi);
  */
 int __init f2fs_init_bioset(void);
 void f2fs_destroy_bioset(void);
-bool f2fs_is_cp_guaranteed(struct page *page);
 int f2fs_init_bio_entry_cache(void);
 void f2fs_destroy_bio_entry_cache(void);
 void f2fs_submit_read_bio(struct f2fs_sb_info *sbi, struct bio *bio,
@@ -3834,6 +4391,13 @@ void f2fs_destroy_post_read_processing(void);
 int f2fs_init_post_read_wq(struct f2fs_sb_info *sbi);
 void f2fs_destroy_post_read_wq(struct f2fs_sb_info *sbi);
 extern const struct iomap_ops f2fs_iomap_ops;
+#ifdef CONFIG_F2FS_SEQZONE
+void f2fs_seqzone_fio_check(struct f2fs_io_info *fio);
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+extern const char *seqzone_switch;
+#endif
 
 /*
  * gc.c
@@ -3858,7 +4422,9 @@ int f2fs_recover_fsync_data(struct f2fs_sb_info *sbi, bool check_only);
 bool f2fs_space_for_roll_forward(struct f2fs_sb_info *sbi);
 int __init f2fs_create_recovery_cache(void);
 void f2fs_destroy_recovery_cache(void);
-
+#ifdef CONFIG_F2FS_APPBOOST
+void f2fs_boostfile_free(struct inode *inode);
+#endif
 /*
  * debug.c
  */
@@ -4129,8 +4695,7 @@ int f2fs_write_inline_data(struct inode *inode, struct page *page);
 int f2fs_recover_inline_data(struct inode *inode, struct page *npage);
 struct f2fs_dir_entry *f2fs_find_in_inline_dir(struct inode *dir,
 					const struct f2fs_filename *fname,
-					struct page **res_page,
-					bool use_hash);
+					struct page **res_page);
 int f2fs_make_empty_inline_dir(struct inode *inode, struct inode *parent,
 			struct page *ipage);
 int f2fs_add_inline_entry(struct inode *dir, const struct f2fs_filename *fname,
@@ -4194,6 +4759,7 @@ unsigned int f2fs_shrink_age_extent_tree(struct f2fs_sb_info *sbi,
  */
 #define MIN_RA_MUL	2
 #define MAX_RA_MUL	256
+#define COMPR_RA_MUL	32
 
 int __init f2fs_init_sysfs(void);
 void f2fs_exit_sysfs(void);
@@ -4297,8 +4863,7 @@ void f2fs_destroy_page_array_cache(struct f2fs_sb_info *sbi);
 int __init f2fs_init_compress_cache(void);
 void f2fs_destroy_compress_cache(void);
 struct address_space *COMPRESS_MAPPING(struct f2fs_sb_info *sbi);
-void f2fs_invalidate_compress_pages_range(struct f2fs_sb_info *sbi,
-					block_t blkaddr, unsigned int len);
+void f2fs_invalidate_compress_page(struct f2fs_sb_info *sbi, block_t blkaddr);
 void f2fs_cache_compressed_page(struct f2fs_sb_info *sbi, struct page *page,
 						nid_t ino, block_t blkaddr);
 bool f2fs_load_compressed_page(struct f2fs_sb_info *sbi, struct page *page,
@@ -4353,8 +4918,8 @@ static inline int f2fs_init_page_array_cache(struct f2fs_sb_info *sbi) { return
 static inline void f2fs_destroy_page_array_cache(struct f2fs_sb_info *sbi) { }
 static inline int __init f2fs_init_compress_cache(void) { return 0; }
 static inline void f2fs_destroy_compress_cache(void) { }
-static inline void f2fs_invalidate_compress_pages_range(struct f2fs_sb_info *sbi,
-				block_t blkaddr, unsigned int len) { }
+static inline void f2fs_invalidate_compress_page(struct f2fs_sb_info *sbi,
+				block_t blkaddr) { }
 static inline void f2fs_cache_compressed_page(struct f2fs_sb_info *sbi,
 				struct page *page, nid_t ino, block_t blkaddr) { }
 static inline bool f2fs_load_compressed_page(struct f2fs_sb_info *sbi,
@@ -4374,11 +4939,19 @@ static inline void f2fs_update_read_extent_tree_range_compressed(
 				unsigned int llen, unsigned int c_len) { }
 #endif
 
+extern bool may_compress;
+extern bool may_set_compr_fl;
+
 static inline int set_compress_context(struct inode *inode)
 {
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		return -EBUSY;
+#endif
+	if (!may_compress)
+		return -EOPNOTSUPP;
 	F2FS_I(inode)->i_compress_algorithm =
 			F2FS_OPTION(sbi).compress_algorithm;
 	F2FS_I(inode)->i_log_cluster_size =
@@ -4386,6 +4959,11 @@ static inline int set_compress_context(struct inode *inode)
 	F2FS_I(inode)->i_compress_flag =
 			F2FS_OPTION(sbi).compress_chksum ?
 				BIT(COMPRESS_CHKSUM) : 0;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (F2FS_OPTION(sbi).compress_layout == COMPRESS_FIXED_OUTPUT)
+		F2FS_I(inode)->i_compress_flag |= COMPRESS_FIXED_OUTPUT <<
+				COMPRESS_LAYOUT;
+#endif
 	F2FS_I(inode)->i_cluster_size =
 			BIT(F2FS_I(inode)->i_log_cluster_size);
 	if ((F2FS_I(inode)->i_compress_algorithm == COMPRESS_LZ4 ||
@@ -4447,8 +5025,26 @@ F2FS_FEATURE_FUNCS(lost_found, LOST_FOUND);
 F2FS_FEATURE_FUNCS(verity, VERITY);
 F2FS_FEATURE_FUNCS(sb_chksum, SB_CHKSUM);
 F2FS_FEATURE_FUNCS(casefold, CASEFOLD);
+#ifndef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
 F2FS_FEATURE_FUNCS(compression, COMPRESSION);
+#else
+static inline bool f2fs_sb_has_compression(struct f2fs_sb_info *sbi)
+{
+	return F2FS_HAS_FEATURE(sbi, F2FS_FEATURE_COMPRESSION) &&
+		(sbi->oplus_feats & OPLUS_FEAT_COMPR);
+}
+#endif
 F2FS_FEATURE_FUNCS(readonly, RO);
+#ifdef CONFIG_F2FS_SEQZONE
+F2FS_FEATURE_FUNCS(seqzone, SEQZONE);
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+static inline bool f2fs_sb_has_dedup(struct f2fs_sb_info *sbi)
+{
+	return F2FS_HAS_FEATURE(sbi, F2FS_FEATURE_DEDUP) &&
+		(sbi->oplus_feats & OPLUS_FEAT_DEDUP);
+}
+#endif
 
 #ifdef CONFIG_BLK_DEV_ZONED
 static inline bool f2fs_blkz_is_seq(struct f2fs_sb_info *sbi, int devi,
@@ -4564,7 +5160,7 @@ static inline bool f2fs_need_verity(const struct inode *inode, pgoff_t idx)
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
 extern int f2fs_build_fault_attr(struct f2fs_sb_info *sbi, unsigned long rate,
-							unsigned long type);
+							unsigned long long type);
 #else
 static inline int f2fs_build_fault_attr(struct f2fs_sb_info *sbi,
 					unsigned long rate, unsigned long type)
@@ -4620,75 +5216,17 @@ static inline bool f2fs_is_readonly(struct f2fs_sb_info *sbi)
 static inline void f2fs_truncate_meta_inode_pages(struct f2fs_sb_info *sbi,
 					block_t blkaddr, unsigned int cnt)
 {
-	bool need_submit = false;
-	int i = 0;
-
-	do {
-		struct page *page;
-
-		page = find_get_page(META_MAPPING(sbi), blkaddr + i);
-		if (page) {
-			if (PageWriteback(page))
-				need_submit = true;
-			f2fs_put_page(page, 0);
-		}
-	} while (++i < cnt && !need_submit);
-
-	if (need_submit)
-		f2fs_submit_merged_write_cond(sbi, sbi->meta_inode,
-							NULL, 0, DATA);
-
+	f2fs_submit_merged_write_cond(sbi, sbi->meta_inode, NULL, 0, DATA);
 	truncate_inode_pages_range(META_MAPPING(sbi),
 			F2FS_BLK_TO_BYTES((loff_t)blkaddr),
 			F2FS_BLK_END_BYTES((loff_t)(blkaddr + cnt - 1)));
 }
 
 static inline void f2fs_invalidate_internal_cache(struct f2fs_sb_info *sbi,
-						block_t blkaddr, unsigned int len)
-{
-	f2fs_truncate_meta_inode_pages(sbi, blkaddr, len);
-	f2fs_invalidate_compress_pages_range(sbi, blkaddr, len);
-}
-
-enum f2fs_lookup_mode {
-	LOOKUP_PERF,
-	LOOKUP_COMPAT,
-	LOOKUP_AUTO,
-};
-
-/*
- * For bit-packing in f2fs_mount_info->alloc_mode
- */
-#define ALLOC_MODE_BITS     1
-#define LOOKUP_MODE_BITS    2
-
-#define ALLOC_MODE_SHIFT    0
-#define LOOKUP_MODE_SHIFT   (ALLOC_MODE_SHIFT + ALLOC_MODE_BITS)
-
-#define ALLOC_MODE_MASK     (((1 << ALLOC_MODE_BITS) - 1) << ALLOC_MODE_SHIFT)
-#define LOOKUP_MODE_MASK    (((1 << LOOKUP_MODE_BITS) - 1) << LOOKUP_MODE_SHIFT)
-
-static inline int f2fs_get_alloc_mode(struct f2fs_sb_info *sbi)
-{
-	return (F2FS_OPTION(sbi).alloc_mode & ALLOC_MODE_MASK) >> ALLOC_MODE_SHIFT;
-}
-
-static inline void f2fs_set_alloc_mode(struct f2fs_sb_info *sbi, int mode)
-{
-	F2FS_OPTION(sbi).alloc_mode &= ~ALLOC_MODE_MASK;
-	F2FS_OPTION(sbi).alloc_mode |= (mode << ALLOC_MODE_SHIFT);
-}
-
-static inline enum f2fs_lookup_mode f2fs_get_lookup_mode(struct f2fs_sb_info *sbi)
-{
-	return (F2FS_OPTION(sbi).alloc_mode & LOOKUP_MODE_MASK) >> LOOKUP_MODE_SHIFT;
-}
-
-static inline void f2fs_set_lookup_mode(struct f2fs_sb_info *sbi,
-						enum f2fs_lookup_mode mode)
+								block_t blkaddr)
 {
-	F2FS_OPTION(sbi).alloc_mode &= ~LOOKUP_MODE_MASK;
-	F2FS_OPTION(sbi).alloc_mode |= (mode << LOOKUP_MODE_SHIFT);
+	f2fs_truncate_meta_inode_pages(sbi, blkaddr, 1);
+	f2fs_invalidate_compress_page(sbi, blkaddr);
 }
 
 #define EFSBADCRC	EBADMSG		/* Bad CRC detected */
diff --git a/fs/f2fs/file.c b/fs/f2fs/file.c
index d9728163a..f91130342 100644
--- a/fs/f2fs/file.c
+++ b/fs/f2fs/file.c
@@ -25,6 +25,7 @@
 #include <linux/fileattr.h>
 #include <linux/fadvise.h>
 #include <linux/iomap.h>
+#include <linux/random.h>
 
 #include "f2fs.h"
 #include "node.h"
@@ -36,2283 +37,5244 @@
 #include <trace/events/f2fs.h>
 #include <uapi/linux/f2fs.h>
 
-static vm_fault_t f2fs_filemap_fault(struct vm_fault *vmf)
-{
-	struct inode *inode = file_inode(vmf->vma->vm_file);
-	vm_fault_t ret;
+#ifdef CONFIG_F2FS_APPBOOST
+#include <linux/version.h>
+#include <linux/delay.h>
+#include "../crypto/fscrypt_private.h"
+#endif
 
-	ret = filemap_fault(vmf);
-	if (ret & VM_FAULT_LOCKED)
-		f2fs_update_iostat(F2FS_I_SB(inode), inode,
-					APP_MAPPED_READ_IO, F2FS_BLKSIZE);
 
-	trace_f2fs_filemap_fault(inode, vmf->pgoff, (unsigned long)ret);
+#ifdef CONFIG_F2FS_FS_DEDUP
+#define DEDUP_COMPARE_PAGES	10
+
+#define DEDUP_META_UN_MODIFY_FL		0x1
+#define DEDUP_DATA_UN_MODIFY_FL		0x2
+#define DEDUP_SET_MODIFY_CHECK		0x4
+#define DEDUP_GET_MODIFY_CHECK		0x8
+#define DEDUP_CLEAR_MODIFY_CHECK	0x10
+#define DEDUP_CLONE_META		0x20
+#define DEDUP_CLONE_DATA		0x40
+#define DEDUP_SYNC_DATA			0x80
+#define DEDUP_FOR_SNAPSHOT		0x100
+#define DEDUP_LOOP_MOD			10000
+#define DEDUP_MIN_SIZE			65536
+#define OUTER_INODE			1
+#define INNER_INODE			2
+#define NORMAL_INODE			3
+
+struct page_list {
+	struct list_head list;
+	struct page *page;
+};
+static struct kmem_cache *page_info_slab;
+
+bool may_compress = false;
+bool may_set_compr_fl = false;
+
+#define LOG_PAGE_INTO_LIST(head, page)	do {			\
+	struct page_list *tmp;					\
+	tmp = f2fs_kmem_cache_alloc_nofail(page_info_slab, GFP_NOFS);	\
+	if (tmp) {						\
+		tmp->page = page;				\
+		INIT_LIST_HEAD(&tmp->list);			\
+		list_add_tail(&tmp->list, &head);		\
+	}							\
+} while (0)
+
+#define FREE_FIRST_PAGE_IN_LIST(head)	do {			\
+	struct page_list *tmp;					\
+	tmp = list_first_entry(&head, struct page_list, list);	\
+	f2fs_put_page(tmp->page, 0);				\
+	list_del(&tmp->list);					\
+	kmem_cache_free(page_info_slab, tmp);			\
+} while (0)
+
+int create_page_info_slab(void)
+{
+	page_info_slab = f2fs_kmem_cache_create("f2fs_page_info_entry",
+				sizeof(struct page_list));
+	if (!page_info_slab)
+		return -ENOMEM;
 
-	return ret;
+	return 0;
 }
 
-static vm_fault_t f2fs_vm_page_mkwrite(struct vm_fault *vmf)
+void destroy_page_info_slab(void)
 {
-	struct page *page = vmf->page;
-	struct inode *inode = file_inode(vmf->vma->vm_file);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct dnode_of_data dn;
-	bool need_alloc = !f2fs_is_pinned_file(inode);
-	int err = 0;
+	if (!page_info_slab)
+		return;
 
-	if (unlikely(IS_IMMUTABLE(inode)))
-		return VM_FAULT_SIGBUS;
+	kmem_cache_destroy(page_info_slab);
+}
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
-		return VM_FAULT_SIGBUS;
+/*
+ * need lock_op and acquire_orphan by caller
+ */
+void f2fs_drop_deduped_link(struct inode *inner)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inner);
 
-	if (unlikely(f2fs_cp_error(sbi))) {
-		err = -EIO;
-		goto err;
-	}
+	f2fs_down_write(&F2FS_I(inner)->i_sem);
+	f2fs_i_links_write(inner, false);
+	f2fs_up_write(&F2FS_I(inner)->i_sem);
 
-	if (!f2fs_is_checkpoint_ready(sbi)) {
-		err = -ENOSPC;
-		goto err;
-	}
+	if (inner->i_nlink == 0)
+		f2fs_add_orphan_inode(inner);
+	else
+		f2fs_release_orphan_inode(sbi);
+}
 
-	err = f2fs_convert_inline_inode(inode);
-	if (err)
-		goto err;
+int f2fs_set_inode_addr(struct inode* inode, block_t addr)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	struct page *node_page;
+	struct f2fs_inode *ri;
+	int count = 1;
+	bool need_update;
+	block_t blkaddr;
+	int i, end_offset;
 
-#ifdef CONFIG_F2FS_FS_COMPRESSION
-	if (f2fs_compressed_file(inode)) {
-		int ret = f2fs_is_compressed_cluster(inode, page->index);
+	if (time_to_inject(sbi, FAULT_DEDUP_FILL_INODE))
+		return -EIO;
 
-		if (ret < 0) {
-			err = ret;
-			goto err;
-		} else if (ret) {
-			need_alloc = false;
-		}
+repeat:
+	node_page = f2fs_get_node_page(sbi, inode->i_ino);
+	if (PTR_ERR(node_page) == -ENOMEM) {
+		if (!(count++ % DEDUP_LOOP_MOD))
+			f2fs_err(sbi,
+				"%s: try to get node page %d", __func__, count);
+
+		cond_resched();
+		goto repeat;
+	} else if (IS_ERR(node_page)) {
+		f2fs_err(sbi, "%s: get node page fail", __func__);
+		return PTR_ERR(node_page);
 	}
-#endif
-	/* should do out of any locked page */
-	if (need_alloc)
-		f2fs_balance_fs(sbi, true);
 
-	sb_start_pagefault(inode->i_sb);
+	f2fs_wait_on_page_writeback(node_page, NODE, true, true);
+	ri = F2FS_INODE(node_page);
 
-	f2fs_bug_on(sbi, f2fs_has_inline_data(inode));
+	end_offset = ADDRS_PER_PAGE(node_page, inode);
+	set_new_dnode(&dn, inode, NULL, node_page, inode->i_ino);
+	dn.ofs_in_node = 0;
 
-	file_update_time(vmf->vma->vm_file);
-	filemap_invalidate_lock_shared(inode->i_mapping);
-	lock_page(page);
-	if (unlikely(page->mapping != inode->i_mapping ||
-			page_offset(page) > i_size_read(inode) ||
-			!PageUptodate(page))) {
-		unlock_page(page);
-		err = -EFAULT;
-		goto out_sem;
-	}
+	for (; dn.ofs_in_node < end_offset; dn.ofs_in_node++) {
+		blkaddr = data_blkaddr(inode, node_page, dn.ofs_in_node);
 
-	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	if (need_alloc) {
-		/* block allocation */
-		err = f2fs_get_block_locked(&dn, page->index);
-	} else {
-		err = f2fs_get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
-		f2fs_put_dnode(&dn);
-		if (f2fs_is_pinned_file(inode) &&
-		    !__is_valid_data_blkaddr(dn.data_blkaddr))
-			err = -EIO;
+		if (__is_valid_data_blkaddr(blkaddr) &&
+			f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE))
+			f2fs_err(sbi, "%s: inode[%lu] leak data addr[%d:%u]",
+				__func__, inode->i_ino, dn.ofs_in_node, blkaddr);
+		else {
+			__set_data_blkaddr(&dn, addr);
+			need_update = true;
+		}
 	}
 
-	if (err) {
-		unlock_page(page);
-		goto out_sem;
+	for (i = 0; i < DEF_NIDS_PER_INODE; i++) {
+		if (ri->i_nid[i])
+			f2fs_err(sbi, "%s: inode[%lu] leak node addr[%d:%u]",
+				__func__, inode->i_ino, i, ri->i_nid[i]);
+		else {
+			ri->i_nid[i] = cpu_to_le32(0);
+			need_update = true;
+		}
 	}
 
-	f2fs_wait_on_page_writeback(page, DATA, false, true);
+	if (need_update)
+		set_page_dirty(node_page);
+	f2fs_put_page(node_page, 1);
 
-	/* wait for GCed page writeback via META_MAPPING */
-	f2fs_wait_on_block_writeback(inode, dn.data_blkaddr);
+	return 0;
+}
 
-	/*
-	 * check to see if the page is mapped already (no holes)
-	 */
-	if (PageMappedToDisk(page))
-		goto out_sem;
+static int __revoke_deduped_inode_begin(struct inode *dedup)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
+	int err;
 
-	/* page is wholly or partially inside EOF */
-	if (((loff_t)(page->index + 1) << PAGE_SHIFT) >
-						i_size_read(inode)) {
-		loff_t offset;
+	f2fs_lock_op(sbi);
 
-		offset = i_size_read(inode) & ~PAGE_MASK;
-		zero_user_segment(page, offset, PAGE_SIZE);
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		err = -ENOSPC;
+	else
+		err = f2fs_acquire_orphan_inode(sbi);	/* for layer inode */
+	if (err) {
+		f2fs_unlock_op(sbi);
+		f2fs_err(sbi, "revoke inode[%lu] begin fail, ret:%d",
+			dedup->i_ino, err);
+		return err;
 	}
-	set_page_dirty(page);
-	if (!PageUptodate(page))
-		SetPageUptodate(page);
 
-	f2fs_update_iostat(sbi, inode, APP_MAPPED_IO, F2FS_BLKSIZE);
-	f2fs_update_time(sbi, REQ_TIME);
+	f2fs_add_orphan_inode(dedup);
 
-	trace_f2fs_vm_page_mkwrite(page, DATA);
-out_sem:
-	filemap_invalidate_unlock_shared(inode->i_mapping);
+	set_inode_flag(dedup, FI_REVOKE_DEDUP);
 
-	sb_end_pagefault(inode->i_sb);
-err:
-	return block_page_mkwrite_return(err);
+	f2fs_unlock_op(sbi);
+
+	return 0;
 }
 
-static const struct vm_operations_struct f2fs_file_vm_ops = {
-	.fault		= f2fs_filemap_fault,
-	.map_pages	= filemap_map_pages,
-	.page_mkwrite	= f2fs_vm_page_mkwrite,
-};
+/*
+ * For kernel version < 5.4.0, we depend on inode lock in direct read IO
+ */
+static void dedup_wait_dio(struct inode *inode)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
 
-static int get_parent_ino(struct inode *inode, nid_t *pino)
+	f2fs_down_write(&fi->i_gc_rwsem[READ]);
+	inode_dio_wait(inode);
+	f2fs_up_write(&fi->i_gc_rwsem[READ]);
+}
+static void prepare_free_inner_inode(struct inode *inode, struct inode *inner)
 {
-	struct dentry *dentry;
+	struct f2fs_inode_info *fi = F2FS_I(inner);
+
+	fi->i_flags &= ~F2FS_IMMUTABLE_FL;
+	f2fs_set_inode_flags(inner);
+	f2fs_mark_inode_dirty_sync(inner, true);
 
 	/*
-	 * Make sure to get the non-deleted alias.  The alias associated with
-	 * the open file descriptor being fsync()'ed may be deleted already.
+	 * Before free inner inode, we should wait all reader of
+	 * the inner complete to avoid UAF or read unexpected data.
 	 */
-	dentry = d_find_alias(inode);
-	if (!dentry)
-		return 0;
+	wait_event(fi->dedup_wq,
+			atomic_read(&fi->inflight_read_io) == 0);
 
-	*pino = parent_ino(dentry);
-	dput(dentry);
-	return 1;
+	dedup_wait_dio(inode);
 }
 
-static inline enum cp_reason_type need_do_checkpoint(struct inode *inode)
+static int __revoke_deduped_inode_end(struct inode *dedup)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	enum cp_reason_type cp_reason = CP_NO_NEEDED;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
+	struct f2fs_inode_info *fi = F2FS_I(dedup);
+	struct inode *inner = NULL;
+	int err;
 
-	if (!S_ISREG(inode->i_mode))
-		cp_reason = CP_NON_REGULAR;
-	else if (f2fs_compressed_file(inode))
-		cp_reason = CP_COMPRESSED;
-	else if (inode->i_nlink != 1)
-		cp_reason = CP_HARDLINK;
-	else if (is_sbi_flag_set(sbi, SBI_NEED_CP))
-		cp_reason = CP_SB_NEED_CP;
-	else if (file_wrong_pino(inode))
-		cp_reason = CP_WRONG_PINO;
-	else if (!f2fs_space_for_roll_forward(sbi))
-		cp_reason = CP_NO_SPC_ROLL;
-	else if (!f2fs_is_checkpointed_node(sbi, F2FS_I(inode)->i_pino))
-		cp_reason = CP_NODE_NEED_CP;
-	else if (test_opt(sbi, FASTBOOT))
-		cp_reason = CP_FASTBOOT_MODE;
-	else if (F2FS_OPTION(sbi).active_logs == 2)
-		cp_reason = CP_SPEC_LOG_NUM;
-	else if (F2FS_OPTION(sbi).fsync_mode == FSYNC_MODE_STRICT &&
-		f2fs_need_dentry_mark(sbi, inode->i_ino) &&
-		f2fs_exist_written_data(sbi, F2FS_I(inode)->i_pino,
-							TRANS_DIR_INO))
-		cp_reason = CP_RECOVER_DIR;
-	else if (f2fs_exist_written_data(sbi, F2FS_I(inode)->i_pino,
-							XATTR_DIR_INO))
-		cp_reason = CP_XATTR_DIR;
+	f2fs_lock_op(sbi);
 
-	return cp_reason;
-}
+	f2fs_remove_orphan_inode(sbi, dedup->i_ino);
 
-static bool need_inode_page_update(struct f2fs_sb_info *sbi, nid_t ino)
-{
-	struct page *i = find_get_page(NODE_MAPPING(sbi), ino);
-	bool ret = false;
-	/* But we need to avoid that there are some inode updates */
-	if ((i && PageDirty(i)) || f2fs_need_inode_block_update(sbi, ino))
-		ret = true;
-	f2fs_put_page(i, 0);
-	return ret;
-}
+	f2fs_down_write(&fi->i_sem);
+	clear_inode_flag(dedup, FI_REVOKE_DEDUP);
+	clear_inode_flag(dedup, FI_DEDUPED);
+	clear_inode_flag(dedup, FI_META_UN_MODIFY);
+	clear_inode_flag(dedup, FI_DATA_UN_MODIFY);
 
-static void try_to_fix_pino(struct inode *inode)
-{
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	nid_t pino;
+	/*
+	 * other reader flow:
+	 * 1) lock inode
+	 * 2) judge whether inner_inode is NULL
+	 * 3) if no, then __iget inner inode
+	 */
+	inner = fi->inner_inode;
+	fi->inner_inode = NULL;
+	fi->dedup_cp_ver = cur_cp_version(F2FS_CKPT(sbi));
+	f2fs_up_write(&fi->i_sem);
 
-	f2fs_down_write(&fi->i_sem);
-	if (file_wrong_pino(inode) && inode->i_nlink == 1 &&
-			get_parent_ino(inode, &pino)) {
-		f2fs_i_pino_write(inode, pino);
-		file_got_pino(inode);
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		err = -ENOSPC;
+	else
+		err = f2fs_acquire_orphan_inode(sbi);	/* for inner inode */
+	if (err) {
+		set_sbi_flag(sbi, SBI_NEED_FSCK);	/* delete inner inode */
+		f2fs_warn(sbi,
+			"%s: orphan failed (ino=%lx), run fsck to fix.",
+			__func__, inner->i_ino);
+	} else {
+		f2fs_drop_deduped_link(inner);
 	}
-	f2fs_up_write(&fi->i_sem);
+	f2fs_unlock_op(sbi);
+
+	trace_f2fs_dedup_revoke_inode(dedup, inner);
+
+	if (inner->i_nlink == 0)
+		prepare_free_inner_inode(dedup, inner);
+
+	iput(inner);
+	return err;
 }
 
-static int f2fs_do_sync_file(struct file *file, loff_t start, loff_t end,
-						int datasync, bool atomic)
+bool f2fs_is_hole_blkaddr(struct inode *inode, pgoff_t pgofs)
 {
-	struct inode *inode = file->f_mapping->host;
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	nid_t ino = inode->i_ino;
-	int ret = 0;
-	enum cp_reason_type cp_reason = 0;
-	struct writeback_control wbc = {
-		.sync_mode = WB_SYNC_ALL,
-		.nr_to_write = LONG_MAX,
-		.for_reclaim = 0,
-	};
-	unsigned int seq_id = 0;
+	struct dnode_of_data dn;
+	block_t blkaddr;
+	int err = 0;
 
-	if (unlikely(f2fs_readonly(inode->i_sb)))
-		return 0;
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_DEDUP_HOLE))
+		return true;
 
-	trace_f2fs_sync_file_enter(inode);
+	if (f2fs_has_inline_data(inode) ||
+		f2fs_has_inline_dentry(inode))
+		return false;
 
-	if (S_ISDIR(inode->i_mode))
-		goto go_write;
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = f2fs_get_dnode_of_data(&dn, pgofs, LOOKUP_NODE);
+	if (err && err != -ENOENT)
+		return false;
 
-	/* if fdatasync is triggered, let's do in-place-update */
-	if (datasync || get_dirty_pages(inode) <= SM_I(sbi)->min_fsync_blocks)
-		set_inode_flag(inode, FI_NEED_IPU);
-	ret = file_write_and_wait_range(file, start, end);
-	clear_inode_flag(inode, FI_NEED_IPU);
+	/* direct node does not exists */
+	if (err == -ENOENT)
+		return true;
 
-	if (ret || is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {
-		trace_f2fs_sync_file_exit(inode, cp_reason, datasync, ret);
-		return ret;
-	}
+	blkaddr = f2fs_data_blkaddr(&dn);
+	f2fs_put_dnode(&dn);
 
-	/* if the inode is dirty, let's recover all the time */
-	if (!f2fs_skip_inode_update(inode, datasync)) {
-		f2fs_write_inode(inode, NULL);
-		goto go_write;
-	}
+	if (__is_valid_data_blkaddr(blkaddr) &&
+		!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),
+			blkaddr, DATA_GENERIC))
+		return false;
 
-	/*
-	 * if there is no written data, don't waste time to write recovery info.
-	 */
-	if (!is_inode_flag_set(inode, FI_APPEND_WRITE) &&
-			!f2fs_exist_written_data(sbi, ino, APPEND_INO)) {
+	if (blkaddr != NULL_ADDR)
+		return false;
 
-		/* it may call write_inode just prior to fsync */
-		if (need_inode_page_update(sbi, ino))
-			goto go_write;
+	return true;
+}
 
-		if (is_inode_flag_set(inode, FI_UPDATE_WRITE) ||
-				f2fs_exist_written_data(sbi, ino, UPDATE_INO))
-			goto flush_out;
-		goto out;
-	} else {
-		/*
-		 * for OPU case, during fsync(), node can be persisted before
-		 * data when lower device doesn't support write barrier, result
-		 * in data corruption after SPO.
-		 * So for strict fsync mode, force to use atomic write semantics
-		 * to keep write order in between data/node and last node to
-		 * avoid potential data corruption.
-		 */
-		if (F2FS_OPTION(sbi).fsync_mode ==
-				FSYNC_MODE_STRICT && !atomic)
-			atomic = true;
-	}
-go_write:
-	/*
-	 * Both of fdatasync() and fsync() are able to be recovered from
-	 * sudden-power-off.
-	 */
-	f2fs_down_read(&F2FS_I(inode)->i_sem);
-	cp_reason = need_do_checkpoint(inode);
-	f2fs_up_read(&F2FS_I(inode)->i_sem);
+static int revoke_deduped_blocks(struct inode *dedup, pgoff_t page_idx, int len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
+	struct address_space *mapping = dedup->i_mapping;
+	pgoff_t redirty_idx = page_idx;
+	int i, page_len = 0, ret = 0;
+	struct dnode_of_data dn;
+	filler_t *filler = NULL;
+	struct page *page;
+	LIST_HEAD(pages);
 
-	if (cp_reason) {
-		/* all the dirty node pages should be flushed for POR */
-		ret = f2fs_sync_fs(inode->i_sb, 1);
+	DEFINE_READAHEAD(ractl, NULL, NULL, mapping, page_idx);
+	page_cache_ra_unbounded(&ractl, len, 0);
 
-		/*
-		 * We've secured consistency through sync_fs. Following pino
-		 * will be used only for fsynced inodes after checkpoint.
-		 */
-		try_to_fix_pino(inode);
-		clear_inode_flag(inode, FI_APPEND_WRITE);
-		clear_inode_flag(inode, FI_UPDATE_WRITE);
-		goto out;
+	/* readahead pages in file */
+	for (i = 0; i < len; i++, page_idx++) {
+		if (time_to_inject(sbi, FAULT_DEDUP_REVOKE)) {
+			ret = -EIO;
+			goto out;
+		}
+		page = read_cache_page(mapping, page_idx, filler, NULL);
+		if (IS_ERR(page)) {
+			ret = PTR_ERR(page);
+			goto out;
+		}
+		page_len++;
+		LOG_PAGE_INTO_LIST(pages, page);
 	}
-sync_nodes:
-	atomic_inc(&sbi->wb_sync_req[NODE]);
-	ret = f2fs_fsync_node_pages(sbi, inode, &wbc, atomic, &seq_id);
-	atomic_dec(&sbi->wb_sync_req[NODE]);
-	if (ret)
-		goto out;
 
-	/* if cp_error was enabled, we should avoid infinite loop */
-	if (unlikely(f2fs_cp_error(sbi))) {
-		ret = -EIO;
-		goto out;
-	}
+	/* rewrite pages above */
+	for (i = 0; i < page_len; i++, redirty_idx++) {
+		if (time_to_inject(sbi, FAULT_DEDUP_REVOKE)) {
+			ret = -ENOMEM;
+			break;
+		}
+		page = find_lock_page(mapping, redirty_idx);
+		if (!page) {
+			ret = -ENOMEM;
+			break;
+		}
 
-	if (f2fs_need_inode_block_update(sbi, ino)) {
-		f2fs_mark_inode_dirty_sync(inode, true);
-		f2fs_write_inode(inode, NULL);
-		goto sync_nodes;
-	}
+		if (!f2fs_is_hole_blkaddr(F2FS_I(dedup)->inner_inode, redirty_idx)) {
+			set_new_dnode(&dn, dedup, NULL, NULL, 0);
+			if (time_to_inject(sbi, FAULT_DEDUP_REVOKE))
+				ret = -ENOSPC;
+			else
+				ret = f2fs_get_block_locked(&dn, redirty_idx);
+			f2fs_put_dnode(&dn);
+			f2fs_bug_on(sbi, !PageUptodate(page));
+			if (!ret)
+				set_page_dirty(page);
+		}
 
-	/*
-	 * If it's atomic_write, it's just fine to keep write ordering. So
-	 * here we don't need to wait for node write completion, since we use
-	 * node chain which serializes node blocks. If one of node writes are
-	 * reordered, we can see simply broken chain, resulting in stopping
-	 * roll-forward recovery. It means we'll recover all or none node blocks
-	 * given fsync mark.
-	 */
-	if (!atomic) {
-		ret = f2fs_wait_on_node_pages_writeback(sbi, seq_id);
+		f2fs_put_page(page, 1);
 		if (ret)
-			goto out;
+			break;
 	}
 
-	/* once recovery info is written, don't need to tack this */
-	f2fs_remove_ino_entry(sbi, ino, APPEND_INO);
-	clear_inode_flag(inode, FI_APPEND_WRITE);
-flush_out:
-	if ((!atomic && F2FS_OPTION(sbi).fsync_mode != FSYNC_MODE_NOBARRIER) ||
-	    (atomic && !test_opt(sbi, NOBARRIER) && f2fs_sb_has_blkzoned(sbi)))
-		ret = f2fs_issue_flush(sbi, inode->i_ino);
-	if (!ret) {
-		f2fs_remove_ino_entry(sbi, ino, UPDATE_INO);
-		clear_inode_flag(inode, FI_UPDATE_WRITE);
-		f2fs_remove_ino_entry(sbi, ino, FLUSH_INO);
-	}
-	f2fs_update_time(sbi, REQ_TIME);
 out:
-	trace_f2fs_sync_file_exit(inode, cp_reason, datasync, ret);
+	while (!list_empty(&pages))
+		FREE_FIRST_PAGE_IN_LIST(pages);
+
 	return ret;
 }
 
-int f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
+static int __revoke_deduped_data(struct inode *dedup)
 {
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(file_inode(file)))))
-		return -EIO;
-	return f2fs_do_sync_file(file, start, end, datasync, false);
-}
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
+	pgoff_t page_idx = 0, last_idx;
+	int blk_per_seg = sbi->blocks_per_seg;
+	int count;
+	int ret1 = 0;
+	int ret2 = 0;
 
-static bool __found_offset(struct address_space *mapping,
-		struct dnode_of_data *dn, pgoff_t index, int whence)
-{
-	block_t blkaddr = f2fs_data_blkaddr(dn);
-	struct inode *inode = mapping->host;
-	bool compressed_cluster = false;
+	f2fs_set_inode_addr(dedup, NULL_ADDR);
+	last_idx = DIV_ROUND_UP(i_size_read(dedup), PAGE_SIZE);
 
-	if (f2fs_compressed_file(inode)) {
-		block_t first_blkaddr = data_blkaddr(dn->inode, dn->node_page,
-		    ALIGN_DOWN(dn->ofs_in_node, F2FS_I(inode)->i_cluster_size));
+	count = last_idx - page_idx;
+	while (count) {
+		int len = min(blk_per_seg, count);
+		ret1 = revoke_deduped_blocks(dedup, page_idx, len);
+		if (ret1 < 0)
+			break;
 
-		compressed_cluster = first_blkaddr == COMPRESS_ADDR;
-	}
+		filemap_fdatawrite(dedup->i_mapping);
 
-	switch (whence) {
-	case SEEK_DATA:
-		if (__is_valid_data_blkaddr(blkaddr))
-			return true;
-		if (blkaddr == NEW_ADDR &&
-		    xa_get_mark(&mapping->i_pages, index, PAGECACHE_TAG_DIRTY))
-			return true;
-		if (compressed_cluster)
-			return true;
-		break;
-	case SEEK_HOLE:
-		if (compressed_cluster)
-			return false;
-		if (blkaddr == NULL_ADDR)
-			return true;
-		break;
+		count -= len;
+		page_idx += len;
 	}
-	return false;
+
+	ret2 = f2fs_filemap_write_and_wait_range(dedup);
+	if (ret1 || ret2)
+		f2fs_warn(sbi, "%s: The deduped inode[%lu] revoked fail(errno=%d,%d).",
+				__func__, dedup->i_ino, ret1, ret2);
+
+	return ret1 ? : ret2;
 }
 
-static loff_t f2fs_seek_block(struct file *file, loff_t offset, int whence)
+static void _revoke_error_handle(struct inode *inode)
 {
-	struct inode *inode = file->f_mapping->host;
-	loff_t maxbytes = inode->i_sb->s_maxbytes;
-	struct dnode_of_data dn;
-	pgoff_t pgofs, end_offset;
-	loff_t data_ofs = offset;
-	loff_t isize;
-	int err = 0;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	inode_lock(inode);
+	f2fs_lock_op(sbi);
+	f2fs_truncate_dedup_inode(inode, FI_REVOKE_DEDUP);
+	f2fs_remove_orphan_inode(sbi, inode->i_ino);
+	F2FS_I(inode)->dedup_cp_ver = cur_cp_version(F2FS_CKPT(sbi));
+	f2fs_unlock_op(sbi);
+	trace_f2fs_dedup_revoke_fail(inode, F2FS_I(inode)->inner_inode);
+}
 
-	isize = i_size_read(inode);
-	if (offset >= isize)
-		goto fail;
+/*
+ * need inode_lock by caller
+ */
+int f2fs_revoke_deduped_inode(struct inode *dedup, const char *revoke_source)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
+	int err = 0;
+	struct inode *inner_inode = NULL;
+	nid_t inner_ino = 0;
 
-	/* handle inline data case */
-	if (f2fs_has_inline_data(inode)) {
-		if (whence == SEEK_HOLE) {
-			data_ofs = isize;
-			goto found;
-		} else if (whence == SEEK_DATA) {
-			data_ofs = offset;
-			goto found;
-		}
-	}
+	if (unlikely(f2fs_cp_error(sbi)))
+		return -EIO;
 
-	pgofs = (pgoff_t)(offset >> PAGE_SHIFT);
+	if (!f2fs_is_outer_inode(dedup))
+		return -EINVAL;
 
-	for (; data_ofs < isize; data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		err = f2fs_get_dnode_of_data(&dn, pgofs, LOOKUP_NODE);
-		if (err && err != -ENOENT) {
-			goto fail;
-		} else if (err == -ENOENT) {
-			/* direct node does not exists */
-			if (whence == SEEK_DATA) {
-				pgofs = f2fs_get_next_page_offset(&dn, pgofs);
-				continue;
-			} else {
-				goto found;
-			}
-		}
+	if (is_inode_flag_set(dedup, FI_SNAPSHOTED))
+		return -EOPNOTSUPP;
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+	if (is_inode_flag_set(dedup, FI_SNAPSHOT_PREPARED))
+		return 0;
 
-		/* find data/hole in dnode block */
-		for (; dn.ofs_in_node < end_offset;
-				dn.ofs_in_node++, pgofs++,
-				data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
-			block_t blkaddr;
+	err = f2fs_dquot_initialize(dedup);
+	if (err)
+		return err;
 
-			blkaddr = f2fs_data_blkaddr(&dn);
+	f2fs_balance_fs(sbi, true);
 
-			if (__is_valid_data_blkaddr(blkaddr) &&
-				!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),
-					blkaddr, DATA_GENERIC_ENHANCE)) {
-				f2fs_put_dnode(&dn);
-				goto fail;
-			}
+	inner_inode = F2FS_I(dedup)->inner_inode;
+	if (inner_inode)
+		inner_ino = inner_inode->i_ino;
 
-			if (__found_offset(file->f_mapping, &dn,
-							pgofs, whence)) {
-				f2fs_put_dnode(&dn);
-				goto found;
-			}
-		}
-		f2fs_put_dnode(&dn);
+	err = __revoke_deduped_inode_begin(dedup);
+	if (err)
+		goto ret;
+
+	err = __revoke_deduped_data(dedup);
+	if (err) {
+		_revoke_error_handle(dedup);
+		goto ret;
 	}
 
-	if (whence == SEEK_DATA)
-		goto fail;
-found:
-	if (whence == SEEK_HOLE && data_ofs > isize)
-		data_ofs = isize;
-	inode_unlock(inode);
-	return vfs_setpos(file, data_ofs, maxbytes);
-fail:
-	inode_unlock(inode);
-	return -ENXIO;
+	err = __revoke_deduped_inode_end(dedup);
+
+ret:
+	return err;
 }
 
-static loff_t f2fs_llseek(struct file *file, loff_t offset, int whence)
+static void f2fs_set_modify_check(struct inode *inode,
+		struct f2fs_modify_check_info *info)
 {
-	struct inode *inode = file->f_mapping->host;
-	loff_t maxbytes = inode->i_sb->s_maxbytes;
-
-	if (f2fs_compressed_file(inode))
-		maxbytes = max_file_blocks(inode) << F2FS_BLKSIZE_BITS;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	switch (whence) {
-	case SEEK_SET:
-	case SEEK_CUR:
-	case SEEK_END:
-		return generic_file_llseek_size(file, offset, whence,
-						maxbytes, i_size_read(inode));
-	case SEEK_DATA:
-	case SEEK_HOLE:
-		if (offset < 0)
-			return -ENXIO;
-		return f2fs_seek_block(file, offset, whence);
+	if (info->flag & DEDUP_META_UN_MODIFY_FL) {
+		if (is_inode_flag_set(inode, FI_META_UN_MODIFY))
+			f2fs_err(sbi,
+				"inode[%lu] had set meta unmodified flag",
+				inode->i_ino);
+		else
+			set_inode_flag(inode, FI_META_UN_MODIFY);
 	}
 
-	return -EINVAL;
+	if (info->flag & DEDUP_DATA_UN_MODIFY_FL) {
+		if (is_inode_flag_set(inode, FI_DATA_UN_MODIFY))
+			f2fs_err(sbi,
+				"inode[%lu] had set data unmodified flag",
+				inode->i_ino);
+		else
+			set_inode_flag(inode, FI_DATA_UN_MODIFY);
+	}
 }
 
-static int f2fs_file_mmap(struct file *file, struct vm_area_struct *vma)
+static void f2fs_get_modify_check(struct inode *inode,
+		struct f2fs_modify_check_info *info)
 {
-	struct inode *inode = file_inode(file);
-
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
-		return -EIO;
+	memset(&(info->flag), 0, sizeof(info->flag));
 
-	if (!f2fs_is_compress_backend_ready(inode))
-		return -EOPNOTSUPP;
-
-	file_accessed(file);
-	vma->vm_ops = &f2fs_file_vm_ops;
-
-	f2fs_down_read(&F2FS_I(inode)->i_sem);
-	set_inode_flag(inode, FI_MMAP_FILE);
-	f2fs_up_read(&F2FS_I(inode)->i_sem);
+	if (is_inode_flag_set(inode, FI_META_UN_MODIFY))
+		info->flag = info->flag | DEDUP_META_UN_MODIFY_FL;
 
-	return 0;
+	if (is_inode_flag_set(inode, FI_DATA_UN_MODIFY))
+		info->flag = info->flag | DEDUP_DATA_UN_MODIFY_FL;
 }
 
-static int finish_preallocate_blocks(struct inode *inode)
+static void f2fs_clear_modify_check(struct inode *inode,
+		struct f2fs_modify_check_info *info)
 {
-	int ret;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	inode_lock(inode);
-	if (is_inode_flag_set(inode, FI_OPENED_FILE)) {
-		inode_unlock(inode);
-		return 0;
+	if (info->flag & DEDUP_META_UN_MODIFY_FL) {
+		if (!is_inode_flag_set(inode, FI_META_UN_MODIFY)) {
+			f2fs_err(sbi,
+				"inode[%lu] had clear unmodified meta flag",
+				inode->i_ino);
+		}
+
+		clear_inode_flag(inode, FI_META_UN_MODIFY);
 	}
 
-	if (!file_should_truncate(inode)) {
-		set_inode_flag(inode, FI_OPENED_FILE);
-		inode_unlock(inode);
-		return 0;
+	if (info->flag & DEDUP_DATA_UN_MODIFY_FL) {
+		if (!is_inode_flag_set(inode, FI_DATA_UN_MODIFY)) {
+			f2fs_err(sbi,
+				"inode[%lu] had clear unmodified data flag",
+				inode->i_ino);
+		}
+
+		clear_inode_flag(inode, FI_DATA_UN_MODIFY);
 	}
 
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(inode->i_mapping);
+	f2fs_mark_inode_dirty_sync(inode, true);
+}
 
-	truncate_setsize(inode, i_size_read(inode));
-	ret = f2fs_truncate(inode);
+bool f2fs_inode_support_dedup(struct f2fs_sb_info *sbi,
+		struct inode *inode)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_inode *ri;
 
-	filemap_invalidate_unlock(inode->i_mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	if (!f2fs_sb_has_dedup(sbi))
+		return false;
 
-	if (!ret)
-		set_inode_flag(inode, FI_OPENED_FILE);
+	if (!f2fs_has_extra_attr(inode))
+		return false;
 
-	inode_unlock(inode);
-	if (ret)
-		return ret;
+	if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_inner_ino))
+		return false;
 
-	file_dont_truncate(inode);
-	return 0;
+	if (f2fs_compressed_file(inode))
+		return false;
+
+	return true;
 }
 
-static int f2fs_file_open(struct inode *inode, struct file *filp)
+static int f2fs_inode_param_check(struct f2fs_sb_info *sbi,
+		struct inode *inode, int type)
 {
-	int err = fscrypt_file_open(inode, filp);
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
 
-	if (err)
-		return err;
+	if (type == OUTER_INODE &&
+		!is_inode_flag_set(inode, FI_SNAPSHOTED) &&
+		!is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED) &&
+					inode->i_size < DEDUP_MIN_SIZE) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] size < %d bytes.",
+			inode->i_ino, DEDUP_MIN_SIZE);
+		return -EINVAL;
+	}
 
-	if (!f2fs_is_compress_backend_ready(inode))
-		return -EOPNOTSUPP;
+	if (type == OUTER_INODE &&
+		!is_inode_flag_set(inode, FI_DATA_UN_MODIFY)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] has been modified.",
+			inode->i_ino);
+		return -EINVAL;
+	}
 
-	err = fsverity_file_open(inode, filp);
-	if (err)
-		return err;
+	if (IS_VERITY(inode)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] enable verity.",
+			inode->i_ino);
+		return -EACCES;
+	}
 
-	filp->f_mode |= FMODE_NOWAIT;
+	if (f2fs_is_atomic_file(inode)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] is atomic file.",
+			inode->i_ino);
+		return -EACCES;
+	}
 
-	err = dquot_file_open(inode, filp);
-	if (err)
-		return err;
+	if (f2fs_is_pinned_file(inode)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] is pinned file.",
+			inode->i_ino);
+		return -EACCES;
+	}
 
-	return finish_preallocate_blocks(inode);
+	if (type != INNER_INODE && IS_IMMUTABLE(inode)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] is immutable.",
+			inode->i_ino);
+		return -EACCES;
+	}
+	return 0;
 }
 
-void f2fs_truncate_data_blocks_range(struct dnode_of_data *dn, int count)
+static int f2fs_dedup_param_check(struct f2fs_sb_info *sbi,
+		struct inode *inode1, int type1,
+		struct inode *inode2, int type2)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
-	int nr_free = 0, ofs = dn->ofs_in_node, len = count;
-	__le32 *addr;
-	bool compressed_cluster = false;
-	int cluster_index = 0, valid_blocks = 0;
-	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
-	bool released = !atomic_read(&F2FS_I(dn->inode)->i_compr_blocks);
-	block_t blkstart;
-	int blklen = 0;
+	int ret;
 
-	addr = get_dnode_addr(dn->inode, dn->node_page) + ofs;
-	blkstart = le32_to_cpu(*addr);
+	if (time_to_inject(sbi, FAULT_DEDUP_PARAM_CHECK))
+		return -EINVAL;
 
-	/* Assumption: truncation starts with cluster */
-	for (; count > 0; count--, addr++, dn->ofs_in_node++, cluster_index++) {
-		block_t blkaddr = le32_to_cpu(*addr);
+	if (inode1->i_sb != inode2->i_sb || inode1 == inode2) {
+		f2fs_err(sbi, "%s: input inode[%lu] and [%lu] are illegal.",
+			__func__, inode1->i_ino, inode2->i_ino);
+		return -EINVAL;
+	}
 
-		if (f2fs_compressed_file(dn->inode) &&
-					!(cluster_index & (cluster_size - 1))) {
-			if (compressed_cluster)
-				f2fs_i_compr_blocks_update(dn->inode,
-							valid_blocks, false);
-			compressed_cluster = (blkaddr == COMPRESS_ADDR);
-			valid_blocks = 0;
-		}
+	if (type1 == OUTER_INODE && type2 == OUTER_INODE &&
+		!is_inode_flag_set(inode2, FI_SNAPSHOTED) &&
+		inode1->i_size != inode2->i_size) {
+		f2fs_err(sbi,
+			"dedup file size not match inode1[%lu] %lld, inode2[%lu] %lld",
+			inode1->i_ino, inode1->i_size,
+			inode2->i_ino, inode2->i_size);
+		return -EINVAL;
+	}
 
-		if (blkaddr == NULL_ADDR)
-			goto next;
+	ret = f2fs_inode_param_check(sbi, inode1, type1);
+	if (ret)
+		return ret;
 
-		f2fs_set_data_blkaddr(dn, NULL_ADDR);
+	ret = f2fs_inode_param_check(sbi, inode2, type2);
+	if (ret)
+		return ret;
 
-		if (__is_valid_data_blkaddr(blkaddr)) {
-			if (time_to_inject(sbi, FAULT_BLKADDR_CONSISTENCE))
-				goto next;
-			if (!f2fs_is_valid_blkaddr_raw(sbi, blkaddr,
-						DATA_GENERIC_ENHANCE)) {
-				f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-				goto next;
-			}
-			if (compressed_cluster)
-				valid_blocks++;
-		}
+	return 0;
+}
 
-		if (blkstart + blklen == blkaddr) {
-			blklen++;
-		} else {
-			f2fs_invalidate_blocks(sbi, blkstart, blklen);
-			blkstart = blkaddr;
-			blklen = 1;
-		}
+static int f2fs_ioc_modify_check(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct f2fs_modify_check_info info;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	int ret;
 
-		if (!released || blkaddr != COMPRESS_ADDR)
-			nr_free++;
+	if (unlikely(f2fs_cp_error(sbi)))
+		return -EIO;
 
-		continue;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-next:
-		if (blklen)
-			f2fs_invalidate_blocks(sbi, blkstart, blklen);
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-		blkstart = le32_to_cpu(*(addr + 1));
-		blklen = 0;
-	}
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
 
-	if (blklen)
-		f2fs_invalidate_blocks(sbi, blkstart, blklen);
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
 
-	if (compressed_cluster)
-		f2fs_i_compr_blocks_update(dn->inode, valid_blocks, false);
+	if (f2fs_has_inline_data(inode))
+		return -EINVAL;
 
-	if (nr_free) {
-		pgoff_t fofs;
-		/*
-		 * once we invalidate valid blkaddr in range [ofs, ofs + count],
-		 * we will invalidate all blkaddr in the whole range.
-		 */
-		fofs = f2fs_start_bidx_of_node(ofs_of_node(dn->node_page),
-							dn->inode) + ofs;
-		f2fs_update_read_extent_cache_range(dn, fofs, 0, len);
-		f2fs_update_age_extent_cache_range(dn, fofs, len);
-		dec_valid_block_count(sbi, dn->inode, nr_free);
-	}
-	dn->ofs_in_node = ofs;
+	if (copy_from_user(&info,
+		(struct f2fs_modify_check_info __user *)arg, sizeof(info)))
+		return -EFAULT;
 
-	f2fs_update_time(sbi, REQ_TIME);
-	trace_f2fs_truncate_data_blocks_range(dn->inode, dn->nid,
-					 dn->ofs_in_node, nr_free);
-}
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
 
-void f2fs_truncate_data_blocks(struct dnode_of_data *dn)
-{
-	f2fs_truncate_data_blocks_range(dn, ADDRS_PER_BLOCK(dn->inode));
-}
+	inode_lock(inode);
+	if (info.mode & DEDUP_SET_MODIFY_CHECK) {
+		struct address_space *mapping = inode->i_mapping;
+		bool dirty = false;
+		int nrpages = 0;
 
-static int truncate_partial_data_page(struct inode *inode, u64 from,
-								bool cache_only)
-{
-	loff_t offset = from & (PAGE_SIZE - 1);
-	pgoff_t index = from >> PAGE_SHIFT;
-	struct address_space *mapping = inode->i_mapping;
-	struct page *page;
+		if (mapping_mapped(mapping)) {
+			f2fs_err(sbi, "inode[%lu] has mapped vma", inode->i_ino);
+			ret = -EBUSY;
+			goto out;
+		}
 
-	if (!offset && !cache_only)
-		return 0;
+		ret = f2fs_inode_param_check(sbi, inode, NORMAL_INODE);
+		if (ret)
+			goto out;
 
-	if (cache_only) {
-		page = find_lock_page(mapping, index);
-		if (page && PageUptodate(page))
-			goto truncate_out;
-		f2fs_put_page(page, 1);
-		return 0;
+		if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) ||
+				mapping_tagged(mapping, PAGECACHE_TAG_WRITEBACK)) {
+			dirty = true;
+			nrpages = get_dirty_pages(inode);
+		}
+
+		if (dirty && (info.flag & DEDUP_SYNC_DATA)) {
+			ret = f2fs_filemap_write_and_wait_range(inode);
+			if (ret) {
+				f2fs_err(sbi, "inode[%lu] write data fail(%d)\n",
+						inode->i_ino, ret);
+				goto out;
+			}
+		} else if (dirty) {
+			f2fs_err(sbi, "inode[%lu] have dirty page[%d]\n",
+					inode->i_ino, nrpages);
+			ret = -EINVAL;
+			goto out;
+		}
+
+		f2fs_set_modify_check(inode, &info);
+	} else if (info.mode & DEDUP_GET_MODIFY_CHECK) {
+		f2fs_get_modify_check(inode, &info);
+	} else if (info.mode & DEDUP_CLEAR_MODIFY_CHECK) {
+		f2fs_clear_modify_check(inode, &info);
+	} else {
+		ret = -EINVAL;
 	}
 
-	page = f2fs_get_lock_data_page(inode, index, true);
-	if (IS_ERR(page))
-		return PTR_ERR(page) == -ENOENT ? 0 : PTR_ERR(page);
-truncate_out:
-	f2fs_wait_on_page_writeback(page, DATA, true, true);
-	zero_user(page, offset, PAGE_SIZE - offset);
+out:
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
 
-	/* An encrypted inode should have a key and truncate the last page. */
-	f2fs_bug_on(F2FS_I_SB(inode), cache_only && IS_ENCRYPTED(inode));
-	if (!cache_only)
-		set_page_dirty(page);
-	f2fs_put_page(page, 1);
-	return 0;
+	if (copy_to_user((struct f2fs_modify_check_info __user *)arg,
+		&info, sizeof(info)))
+		ret = -EFAULT;
+
+	return ret;
 }
 
-int f2fs_do_truncate_blocks(struct inode *inode, u64 from, bool lock)
+static int f2fs_ioc_dedup_permission_check(struct file *filp, unsigned long arg)
 {
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct dnode_of_data dn;
-	pgoff_t free_from;
-	int count = 0, err = 0;
-	struct page *ipage;
-	bool truncate_page = false;
 
-	trace_f2fs_truncate_blocks_enter(inode, from);
+	if (unlikely(f2fs_cp_error(sbi)))
+		return -EIO;
 
-	free_from = (pgoff_t)F2FS_BLK_ALIGN(from);
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	if (free_from >= max_file_blocks(inode))
-		goto free_partial;
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-	if (lock)
-		f2fs_lock_op(sbi);
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
 
-	ipage = f2fs_get_node_page(sbi, inode->i_ino);
-	if (IS_ERR(ipage)) {
-		err = PTR_ERR(ipage);
-		goto out;
-	}
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
 
-	if (f2fs_has_inline_data(inode)) {
-		f2fs_truncate_inline_inode(inode, ipage, from);
-		f2fs_put_page(ipage, 1);
-		truncate_page = true;
-		goto out;
-	}
+	if (f2fs_has_inline_data(inode))
+		return -EINVAL;
 
-	set_new_dnode(&dn, inode, ipage, NULL, 0);
-	err = f2fs_get_dnode_of_data(&dn, free_from, LOOKUP_NODE_RA);
-	if (err) {
-		if (err == -ENOENT)
-			goto free_next;
-		goto out;
+	return f2fs_inode_param_check(sbi, inode, OUTER_INODE);
+}
+
+static int f2fs_copy_data(struct inode *dst_inode,
+		struct inode *src_inode, pgoff_t page_idx, int len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dst_inode);
+	struct address_space *src_mapping = src_inode->i_mapping;
+	struct address_space *dst_mapping = dst_inode->i_mapping;
+	filler_t *filler = NULL;
+	struct page *page, *newpage;
+	pgoff_t copy_idx = page_idx;
+	int i, page_len = 0, ret = 0;
+	struct dnode_of_data dn;
+	DEFINE_READAHEAD(ractl, NULL, NULL, src_mapping, page_idx);
+	LIST_HEAD(pages);
+
+	page_cache_ra_unbounded(&ractl, len, 0);
+
+	for (i = 0; i < len; i++, page_idx++) {
+		if (time_to_inject(sbi, FAULT_DEDUP_CLONE)) {
+			ret = -ENOMEM;
+			break;
+		}
+		page = read_cache_page(src_mapping, page_idx, filler, NULL);
+		if (IS_ERR(page)) {
+			ret = PTR_ERR(page);
+			goto out;
+		}
+		page_len++;
+		LOG_PAGE_INTO_LIST(pages, page);
 	}
 
-	count = ADDRS_PER_PAGE(dn.node_page, inode);
+	for (i = 0; i < page_len; i++, copy_idx++) {
+		if (time_to_inject(sbi, FAULT_DEDUP_CLONE)) {
+			ret = -ENOMEM;
+			break;
+		}
+		page = find_lock_page(src_mapping, copy_idx);
+		if (!page) {
+			ret = -ENOMEM;
+			break;
+		}
 
-	count -= dn.ofs_in_node;
-	f2fs_bug_on(sbi, count < 0);
+		if (f2fs_is_hole_blkaddr(src_inode, copy_idx)) {
+			f2fs_put_page(page, 1);
+			continue;
+		}
 
-	if (dn.ofs_in_node || IS_INODE(dn.node_page)) {
-		f2fs_truncate_data_blocks_range(&dn, count);
-		free_from += count;
+		set_new_dnode(&dn, dst_inode, NULL, NULL, 0);
+		if (time_to_inject(sbi, FAULT_DEDUP_CLONE))
+			ret = -ENOSPC;
+		else
+			ret = f2fs_get_block_locked(&dn, copy_idx);
+		f2fs_put_dnode(&dn);
+		if (ret) {
+			f2fs_put_page(page, 1);
+			break;
+		}
+
+		if (time_to_inject(sbi, FAULT_DEDUP_CLONE))
+			newpage = NULL;
+		else
+			newpage = f2fs_grab_cache_page(dst_mapping, copy_idx, true);
+		if (!newpage) {
+			ret = -ENOMEM;
+			f2fs_put_page(page, 1);
+			break;
+		}
+		memcpy_page(newpage, 0, page, 0, PAGE_SIZE);
+
+		set_page_dirty(newpage);
+		f2fs_put_page(newpage, 1);
+		f2fs_put_page(page, 1);
 	}
 
-	f2fs_put_dnode(&dn);
-free_next:
-	err = f2fs_truncate_inode_blocks(inode, free_from);
 out:
-	if (lock)
-		f2fs_unlock_op(sbi);
-free_partial:
-	/* lastly zero out the first data page */
-	if (!err)
-		err = truncate_partial_data_page(inode, from, truncate_page);
+	while (!list_empty(&pages))
+		FREE_FIRST_PAGE_IN_LIST(pages);
 
-	trace_f2fs_truncate_blocks_exit(inode, err);
-	return err;
+	return ret;
 }
 
-int f2fs_truncate_blocks(struct inode *inode, u64 from, bool lock)
+static int f2fs_clone_data(struct inode *dst_inode,
+		struct inode *src_inode)
 {
-	u64 free_from = from;
-	int err;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(src_inode);
+	pgoff_t page_idx = 0, last_idx;
+	int blk_per_seg = sbi->blocks_per_seg;
+	int count;
+	int ret = 0;
 
-#ifdef CONFIG_F2FS_FS_COMPRESSION
-	/*
-	 * for compressed file, only support cluster size
-	 * aligned truncation.
-	 */
-	if (f2fs_compressed_file(inode))
-		free_from = round_up(from,
-				F2FS_I(inode)->i_cluster_size << PAGE_SHIFT);
-#endif
+	f2fs_balance_fs(sbi, true);
+	last_idx = DIV_ROUND_UP(i_size_read(src_inode), PAGE_SIZE);
+	count = last_idx - page_idx;
 
-	err = f2fs_do_truncate_blocks(inode, free_from, lock);
-	if (err)
-		return err;
+	while (count) {
+		int len = min(blk_per_seg, count);
+		ret = f2fs_copy_data(dst_inode, src_inode, page_idx, len);
+		if (ret < 0)
+			break;
 
-#ifdef CONFIG_F2FS_FS_COMPRESSION
-	/*
-	 * For compressed file, after release compress blocks, don't allow write
-	 * direct, but we should allow write direct after truncate to zero.
-	 */
-	if (f2fs_compressed_file(inode) && !free_from
-			&& is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
-		clear_inode_flag(inode, FI_COMPRESS_RELEASED);
+		filemap_fdatawrite(dst_inode->i_mapping);
+		count -= len;
+		page_idx += len;
+	}
 
-	if (from != free_from) {
-		err = f2fs_truncate_partial_cluster(inode, from, lock);
-		if (err)
-			return err;
+	if (!ret)
+		ret = f2fs_filemap_write_and_wait_range(dst_inode);
+
+	return ret;
+}
+
+static int __f2fs_ioc_clone_file(struct inode *dst_inode,
+		struct inode *src_inode, struct f2fs_clone_info *info)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(src_inode);
+	int ret;
+
+	ret = f2fs_convert_inline_inode(dst_inode);
+	if (ret) {
+		f2fs_err(sbi,
+			"inode[%lu] convert inline inode failed, ret:%d",
+			dst_inode->i_ino, ret);
+		return ret;
+	}
+
+	if (info->flags & DEDUP_CLONE_META) {
+		dst_inode->i_uid = src_inode->i_uid;
+		dst_inode->i_gid = src_inode->i_gid;
+		dst_inode->i_size = src_inode->i_size;
+	}
+
+	if (info->flags & DEDUP_CLONE_DATA) {
+		dst_inode->i_size = src_inode->i_size;
+		ret = f2fs_clone_data(dst_inode, src_inode);
+		if (ret) {
+			/* No need to truncate, beacuse tmpfile will be removed. */
+			f2fs_err(sbi,
+				"src inode[%lu] dst inode[%lu] ioc clone failed. ret=%d",
+				src_inode->i_ino, dst_inode->i_ino, ret);
+			return ret;
+		}
 	}
-#endif
+
+	set_inode_flag(dst_inode, FI_DATA_UN_MODIFY);
 
 	return 0;
 }
 
-int f2fs_truncate(struct inode *inode)
+static int f2fs_ioc_clone_file(struct file *filp, unsigned long arg)
 {
-	int err;
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct inode *src_inode;
+	struct f2fs_clone_info info;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct fd f;
+	int ret = 0;
 
 	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
 		return -EIO;
 
-	if (!(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
-				S_ISLNK(inode->i_mode)))
-		return 0;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	trace_f2fs_truncate(inode);
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-	if (time_to_inject(F2FS_I_SB(inode), FAULT_TRUNCATE))
-		return -EIO;
+	if (copy_from_user(&info, (struct f2fs_clone_info __user *)arg, sizeof(info)))
+		return -EFAULT;
 
-	err = f2fs_dquot_initialize(inode);
-	if (err)
-		return err;
+	f = fdget_pos(info.src_fd);
+	if (!f.file)
+		return -EBADF;
 
-	/* we should check inline_data size */
-	if (!f2fs_may_inline_data(inode)) {
-		err = f2fs_convert_inline_inode(inode);
-		if (err)
-			return err;
+	src_inode = file_inode(f.file);
+	if (inode->i_sb != src_inode->i_sb) {
+		f2fs_err(sbi, "%s: files should be in same FS ino:%lu, src_ino:%lu",
+				__func__, inode->i_ino, src_inode->i_ino);
+		ret = -EINVAL;
+		goto out;
 	}
 
-	err = f2fs_truncate_blocks(inode, i_size_read(inode), true);
-	if (err)
-		return err;
+	if (!f2fs_inode_support_dedup(sbi, src_inode)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		goto out;
+
+	inode_lock(inode);
+	ret = f2fs_dedup_param_check(sbi, src_inode, info.flags &
+		DEDUP_FOR_SNAPSHOT ? NORMAL_INODE : OUTER_INODE,
+			inode, INNER_INODE);
+	if (ret)
+		goto unlock;
+
+	ret = __f2fs_ioc_clone_file(inode, src_inode, &info);
+	if (ret)
+		goto unlock;
+
+	F2FS_I(inode)->i_flags |= F2FS_IMMUTABLE_FL;
+	f2fs_set_inode_flags(inode);
+	f2fs_mark_inode_dirty_sync(inode, true);
+
+unlock:
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
+out:
+	fdput_pos(f);
+	return ret;
+}
+
+static inline void _truncate_error_handle(struct inode *inode,
+		int ret)
+{
+	set_sbi_flag(F2FS_I_SB(inode), SBI_NEED_FSCK);
+	f2fs_err(F2FS_I_SB(inode),
+		"truncate data failed, inode:%lu ret:%d",
+		inode->i_ino, ret);
+}
+
+int f2fs_truncate_dedup_inode(struct inode *inode, unsigned int flag)
+{
+	int ret = 0;
+
+	if (!f2fs_is_outer_inode(inode)) {
+		f2fs_err(F2FS_I_SB(inode),
+			"inode:%lu is not dedup inode", inode->i_ino);
+		f2fs_bug_on(F2FS_I_SB(inode), 1);
+		return 0;
+	}
+
+	clear_inode_flag(inode, flag);
+
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_DEDUP_TRUNCATE)) {
+		ret = -EIO;
+		goto err;
+	}
+	ret = f2fs_truncate_blocks(inode, 0, false);
+	if (ret)
+		goto err;
+
+	ret = f2fs_set_inode_addr(inode, DEDUP_ADDR);
+	if (ret)
+		goto err;
 
-	inode->i_mtime = inode->i_ctime = current_time(inode);
-	f2fs_mark_inode_dirty_sync(inode, false);
 	return 0;
+err:
+	_truncate_error_handle(inode, ret);
+	return ret;
 }
 
-static bool f2fs_force_buffered_io(struct inode *inode, int rw)
+static int is_inode_match_dir_crypt_policy(struct dentry *dir,
+		struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	if (!fscrypt_dio_supported(inode))
-		return true;
-	if (fsverity_active(inode))
-		return true;
-	if (f2fs_compressed_file(inode))
-		return true;
-	/*
-	 * only force direct read to use buffered IO, for direct write,
-	 * it expects inline data conversion before committing IO.
-	 */
-	if (f2fs_has_inline_data(inode) && rw == READ)
-		return true;
+	if (time_to_inject(sbi, FAULT_DEDUP_CRYPT_POLICY))
+		return -EPERM;
 
-	/* disallow direct IO if any of devices has unaligned blksize */
-	if (f2fs_is_multi_device(sbi) && !sbi->aligned_blksize)
-		return true;
-	/*
-	 * for blkzoned device, fallback direct IO to buffered IO, so
-	 * all IOs can be serialized by log-structured write.
-	 */
-	if (f2fs_sb_has_blkzoned(sbi) && (rw == WRITE))
-		return true;
-	if (f2fs_lfs_mode(sbi) && rw == WRITE && F2FS_IO_ALIGNED(sbi))
-		return true;
-	if (is_sbi_flag_set(sbi, SBI_CP_DISABLED))
-		return true;
+	if (IS_ENCRYPTED(d_inode(dir)) &&
+		!fscrypt_has_permitted_context(d_inode(dir), inode)) {
+		f2fs_err(sbi, "inode[%lu] not match dir[%lu] fscrypt policy",
+			inode->i_ino, d_inode(dir)->i_ino);
+		return -EPERM;
+	}
 
-	return false;
+	return 0;
 }
 
-int f2fs_getattr(struct user_namespace *mnt_userns, const struct path *path,
-		 struct kstat *stat, u32 request_mask, unsigned int query_flags)
+static int deduped_files_match_fscrypt_policy(struct file *file1,
+		struct file *file2)
 {
-	struct inode *inode = d_inode(path->dentry);
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	struct f2fs_inode *ri = NULL;
-	unsigned int flags;
+	struct dentry *dir1 = dget_parent(file_dentry(file1));
+	struct dentry *dir2 = dget_parent(file_dentry(file2));
+	struct inode *inode1 = file_inode(file1);
+	struct inode *inode2 = file_inode(file2);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode1);
+	int err = 0;
 
-	if (f2fs_has_extra_attr(inode) &&
-			f2fs_sb_has_inode_crtime(F2FS_I_SB(inode)) &&
-			F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime)) {
-		stat->result_mask |= STATX_BTIME;
-		stat->btime.tv_sec = fi->i_crtime.tv_sec;
-		stat->btime.tv_nsec = fi->i_crtime.tv_nsec;
+	if (time_to_inject(sbi, FAULT_DEDUP_CRYPT_POLICY)) {
+		err = -EPERM;
+		goto out;
 	}
 
-	/*
-	 * Return the DIO alignment restrictions if requested.  We only return
-	 * this information when requested, since on encrypted files it might
-	 * take a fair bit of work to get if the file wasn't opened recently.
-	 *
-	 * f2fs sometimes supports DIO reads but not DIO writes.  STATX_DIOALIGN
-	 * cannot represent that, so in that case we report no DIO support.
-	 */
-	if ((request_mask & STATX_DIOALIGN) && S_ISREG(inode->i_mode)) {
-		unsigned int bsize = i_blocksize(inode);
+	if (IS_ENCRYPTED(d_inode(dir1)) &&
+		!fscrypt_has_permitted_context(d_inode(dir1), inode2)) {
+		f2fs_err(sbi, "dir[%lu] inode[%lu] and inode[%lu] fscrypt policy not match.",
+			d_inode(dir1)->i_ino, inode1->i_ino, inode2->i_ino);
+		err = -EPERM;
+		goto out;
+	}
 
-		stat->result_mask |= STATX_DIOALIGN;
-		if (!f2fs_force_buffered_io(inode, WRITE)) {
-			stat->dio_mem_align = bsize;
-			stat->dio_offset_align = bsize;
-		}
+	if (IS_ENCRYPTED(d_inode(dir2)) &&
+		!fscrypt_has_permitted_context(d_inode(dir2), inode1)) {
+		f2fs_err(sbi, "inode[%lu] and dir[%lu] inode[%lu] fscrypt policy not match.",
+			inode1->i_ino, d_inode(dir2)->i_ino, inode2->i_ino);
+		err = -EPERM;
 	}
 
-	flags = fi->i_flags;
-	if (flags & F2FS_COMPR_FL)
-		stat->attributes |= STATX_ATTR_COMPRESSED;
-	if (flags & F2FS_APPEND_FL)
-		stat->attributes |= STATX_ATTR_APPEND;
-	if (IS_ENCRYPTED(inode))
-		stat->attributes |= STATX_ATTR_ENCRYPTED;
-	if (flags & F2FS_IMMUTABLE_FL)
-		stat->attributes |= STATX_ATTR_IMMUTABLE;
-	if (flags & F2FS_NODUMP_FL)
-		stat->attributes |= STATX_ATTR_NODUMP;
-	if (IS_VERITY(inode))
-		stat->attributes |= STATX_ATTR_VERITY;
+out:
+	dput(dir2);
+	dput(dir1);
+	return err;
+}
 
-	stat->attributes_mask |= (STATX_ATTR_COMPRESSED |
-				  STATX_ATTR_APPEND |
-				  STATX_ATTR_ENCRYPTED |
-				  STATX_ATTR_IMMUTABLE |
-				  STATX_ATTR_NODUMP |
-				  STATX_ATTR_VERITY);
+static int f2fs_compare_page(struct page *src, struct page *dst)
+{
+	int ret;
+	char *src_kaddr = kmap_atomic(src);
+	char *dst_kaddr = kmap_atomic(dst);
 
-	generic_fillattr(mnt_userns, inode, stat);
+	flush_dcache_page(src);
+	flush_dcache_page(dst);
 
-	/* we need to show initial sectors used for inline_data/dentries */
-	if ((S_ISREG(inode->i_mode) && f2fs_has_inline_data(inode)) ||
-					f2fs_has_inline_dentry(inode))
-		stat->blocks += (stat->size + 511) >> 9;
+	ret = memcmp(dst_kaddr, src_kaddr, PAGE_SIZE);
+	kunmap_atomic(src_kaddr);
+	kunmap_atomic(dst_kaddr);
 
-	return 0;
+	return ret;
 }
 
-#ifdef CONFIG_F2FS_FS_POSIX_ACL
-static void __setattr_copy(struct user_namespace *mnt_userns,
-			   struct inode *inode, const struct iattr *attr)
+static inline void lock_two_pages(struct page *page1, struct page *page2)
 {
-	unsigned int ia_valid = attr->ia_valid;
-
-	i_uid_update(mnt_userns, attr, inode);
-	i_gid_update(mnt_userns, attr, inode);
-	if (ia_valid & ATTR_ATIME)
-		inode->i_atime = attr->ia_atime;
-	if (ia_valid & ATTR_MTIME)
-		inode->i_mtime = attr->ia_mtime;
-	if (ia_valid & ATTR_CTIME)
-		inode->i_ctime = attr->ia_ctime;
-	if (ia_valid & ATTR_MODE) {
-		umode_t mode = attr->ia_mode;
-		vfsgid_t vfsgid = i_gid_into_vfsgid(mnt_userns, inode);
-
-		if (!vfsgid_in_group_p(vfsgid) &&
-		    !capable_wrt_inode_uidgid(mnt_userns, inode, CAP_FSETID))
-			mode &= ~S_ISGID;
-		set_acl_inode(inode, mode);
-	}
+	lock_page(page1);
+	if (page1 != page2)
+		lock_page(page2);
 }
-#else
-#define __setattr_copy setattr_copy
-#endif
 
-int f2fs_setattr(struct user_namespace *mnt_userns, struct dentry *dentry,
-		 struct iattr *attr)
+static inline void unlock_two_pages(struct page *page1, struct page *page2)
 {
-	struct inode *inode = d_inode(dentry);
-	int err;
+	unlock_page(page1);
+	if (page1 != page2)
+		unlock_page(page2);
+}
 
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
-		return -EIO;
+static bool dedup_file_is_same(struct inode *src, struct inode *dst, int nr_pages)
+{
+	struct page *src_page, *dst_page;
+	pgoff_t index, last_idx;
+	int i, ret;
+	bool same = true;
 
-	if (unlikely(IS_IMMUTABLE(inode)))
-		return -EPERM;
+	if (time_to_inject(F2FS_I_SB(src), FAULT_DEDUP_SAME_FILE))
+		return false;
 
-	if (unlikely(IS_APPEND(inode) &&
-			(attr->ia_valid & (ATTR_MODE | ATTR_UID |
-				  ATTR_GID | ATTR_TIMES_SET))))
-		return -EPERM;
+	if (i_size_read(src) != i_size_read(dst))
+		return false;
 
-	if ((attr->ia_valid & ATTR_SIZE) &&
-		!f2fs_is_compress_backend_ready(inode))
-		return -EOPNOTSUPP;
+	last_idx = DIV_ROUND_UP(i_size_read(src), PAGE_SIZE);
 
-	err = setattr_prepare(mnt_userns, dentry, attr);
-	if (err)
-		return err;
+	for (i = 0; i < nr_pages; i++) {
+		index = get_random_u32() % last_idx;
 
-	err = fscrypt_prepare_setattr(dentry, attr);
-	if (err)
-		return err;
+		src_page = read_mapping_page(src->i_mapping, index, NULL);
+		if (IS_ERR(src_page)) {
+			ret = PTR_ERR(src_page);
+			same = false;
+			break;
+		}
 
-	err = fsverity_prepare_setattr(dentry, attr);
-	if (err)
-		return err;
+		dst_page = read_mapping_page(dst->i_mapping, index, NULL);
+		if (IS_ERR(dst_page)) {
+			ret = PTR_ERR(dst_page);
+			put_page(src_page);
+			same = false;
+			break;
+		}
 
-	if (is_quota_modification(mnt_userns, inode, attr)) {
-		err = f2fs_dquot_initialize(inode);
-		if (err)
-			return err;
-	}
-	if (i_uid_needs_update(mnt_userns, attr, inode) ||
-	    i_gid_needs_update(mnt_userns, attr, inode)) {
-		f2fs_lock_op(F2FS_I_SB(inode));
-		err = dquot_transfer(mnt_userns, inode, attr);
-		if (err) {
-			set_sbi_flag(F2FS_I_SB(inode),
-					SBI_QUOTA_NEED_REPAIR);
-			f2fs_unlock_op(F2FS_I_SB(inode));
-			return err;
+		lock_two_pages(src_page, dst_page);
+		if (!PageUptodate(src_page) || !PageUptodate(dst_page) ||
+				src_page->mapping != src->i_mapping ||
+				dst_page->mapping != dst->i_mapping) {
+			ret = -EINVAL;
+			same = false;
+			goto unlock;
+		}
+		ret = f2fs_compare_page(src_page, dst_page);
+		if (ret)
+			same = false;
+unlock:
+		unlock_two_pages(src_page, dst_page);
+		put_page(dst_page);
+		put_page(src_page);
+
+		if (!same) {
+			f2fs_err(F2FS_I_SB(src),
+					"src: %lu, dst: %lu page index: %lu is diff ret[%d]",
+					src->i_ino, dst->i_ino, index, ret);
+			break;
 		}
-		/*
-		 * update uid/gid under lock_op(), so that dquot and inode can
-		 * be updated atomically.
-		 */
-		i_uid_update(mnt_userns, attr, inode);
-		i_gid_update(mnt_userns, attr, inode);
-		f2fs_mark_inode_dirty_sync(inode, true);
-		f2fs_unlock_op(F2FS_I_SB(inode));
 	}
 
-	if (attr->ia_valid & ATTR_SIZE) {
-		loff_t old_size = i_size_read(inode);
+	return same;
+}
 
-		if (attr->ia_size > MAX_INLINE_DATA(inode)) {
-			/*
-			 * should convert inline inode before i_size_write to
-			 * keep smaller than inline_data size with inline flag.
-			 */
-			err = f2fs_convert_inline_inode(inode);
-			if (err)
-				return err;
-		}
+int f2fs_filemap_write_and_wait_range(struct inode *inode)
+{
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_DEDUP_WRITEBACK)) {
+		int ret;
 
-		/*
-		 * wait for inflight dio, blocks should be removed after
-		 * IO completion.
-		 */
-		if (attr->ia_size < old_size)
-			inode_dio_wait(inode);
+		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+						i_size_read(inode) / 2);
+		if (!ret)
+			ret = -EIO;
+		return ret;
+	}
 
-		f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-		filemap_invalidate_lock(inode->i_mapping);
+	return filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+}
 
-		truncate_setsize(inode, attr->ia_size);
+static int __f2fs_ioc_create_layered_inode(struct inode *inode,
+		struct inode *inner_inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	int ret;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
 
-		if (attr->ia_size <= old_size)
-			err = f2fs_truncate(inode);
-		/*
-		 * do not trim all blocks after i_size if target size is
-		 * larger than i_size.
-		 */
-		filemap_invalidate_unlock(inode->i_mapping);
-		f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-		if (err)
-			return err;
+	if (!dedup_file_is_same(inode, inner_inode, DEDUP_COMPARE_PAGES))
+		return -ESTALE;
 
-		spin_lock(&F2FS_I(inode)->i_size_lock);
-		inode->i_mtime = inode->i_ctime = current_time(inode);
-		F2FS_I(inode)->last_disk_size = i_size_read(inode);
-		spin_unlock(&F2FS_I(inode)->i_size_lock);
+	f2fs_lock_op(sbi);
+
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		ret = -ENOSPC;
+	else
+		ret = f2fs_acquire_orphan_inode(sbi);
+	if (ret) {
+		f2fs_err(sbi,
+			"create layer file acquire orphan fail, ino[%lu], inner[%lu]",
+			inode->i_ino, inner_inode->i_ino);
+		f2fs_unlock_op(sbi);
+		return ret;
 	}
+	f2fs_add_orphan_inode(inode);
 
-	__setattr_copy(mnt_userns, inode, attr);
+	f2fs_down_write(&F2FS_I(inner_inode)->i_sem);
+	igrab(inner_inode);
+	set_inode_flag(inner_inode, FI_INNER_INODE);
+	set_inode_flag(inner_inode, FI_DEDUPED);
+	if (is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		set_inode_flag(inner_inode, FI_SNAPSHOT_PREPARED);
+	f2fs_i_links_write(inner_inode, true);
+	f2fs_up_write(&F2FS_I(inner_inode)->i_sem);
 
-	if (attr->ia_valid & ATTR_MODE) {
-		err = posix_acl_chmod(mnt_userns, inode, f2fs_get_inode_mode(inode));
+	f2fs_down_write(&fi->i_sem);
+	fi->inner_inode = inner_inode;
+	set_inode_flag(inode, FI_DEDUPED);
+	//set_inode_flag(inode, FI_DOING_DEDUP);
+	f2fs_up_write(&fi->i_sem);
 
-		if (is_inode_flag_set(inode, FI_ACL_MODE)) {
-			if (!err)
-				inode->i_mode = F2FS_I(inode)->i_acl_mode;
-			clear_inode_flag(inode, FI_ACL_MODE);
-		}
-	}
+	f2fs_remove_orphan_inode(sbi, inner_inode->i_ino);
+	f2fs_unlock_op(sbi);
 
-	/* file size may changed here */
-	f2fs_mark_inode_dirty_sync(inode, true);
+	wait_event(fi->dedup_wq,
+			atomic_read(&fi->inflight_read_io) == 0);
+	dedup_wait_dio(inode);
 
-	/* inode change will produce dirty node pages flushed by checkpoint */
-	f2fs_balance_fs(F2FS_I_SB(inode), true);
+	f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+	/* GC may dirty pages before holding lock */
+	ret = f2fs_filemap_write_and_wait_range(inode);
+	if (ret)
+		goto out;
 
-	return err;
-}
+	f2fs_lock_op(sbi);
+	f2fs_remove_orphan_inode(sbi, inode->i_ino);
+	ret = f2fs_truncate_dedup_inode(inode, FI_DOING_DEDUP);
+	/*
+	 * Since system may do checkpoint after unlock cp,
+	 * we set cp_ver here to let fsync know dedup have finish.
+	 */
+	F2FS_I(inode)->dedup_cp_ver = cur_cp_version(F2FS_CKPT(sbi));
+	f2fs_unlock_op(sbi);
 
-const struct inode_operations f2fs_file_inode_operations = {
-	.getattr	= f2fs_getattr,
-	.setattr	= f2fs_setattr,
-	.get_acl	= f2fs_get_acl,
-	.set_acl	= f2fs_set_acl,
-	.listxattr	= f2fs_listxattr,
-	.fiemap		= f2fs_fiemap,
-	.fileattr_get	= f2fs_fileattr_get,
-	.fileattr_set	= f2fs_fileattr_set,
-};
+out:
+	f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+	f2fs_dedup_info(sbi, "inode[%lu] create layered success, inner[%lu] ret: %d",
+			inode->i_ino, inner_inode->i_ino, ret);
+	trace_f2fs_dedup_ioc_create_layered_inode(inode, inner_inode);
+	return ret;
+}
 
-static int fill_zero(struct inode *inode, pgoff_t index,
-					loff_t start, loff_t len)
+static int f2fs_ioc_create_layered_inode(struct file *filp, unsigned long arg)
 {
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct inode *inner_inode;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct page *page;
+	struct f2fs_dedup_src info;
+	struct dentry *dir;
+	struct fd f;
+	int ret;
 
-	if (!len)
-		return 0;
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
 
-	f2fs_balance_fs(sbi, true);
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	f2fs_lock_op(sbi);
-	page = f2fs_get_new_data_page(inode, NULL, index, false);
-	f2fs_unlock_op(sbi);
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-	if (IS_ERR(page))
-		return PTR_ERR(page);
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
 
-	f2fs_wait_on_page_writeback(page, DATA, true, true);
-	zero_user(page, start, len);
-	set_page_dirty(page);
-	f2fs_put_page(page, 1);
-	return 0;
-}
+	if (copy_from_user(&info, (struct f2fs_dedup_src __user *)arg, sizeof(info)))
+		return -EFAULT;
 
-int f2fs_truncate_hole(struct inode *inode, pgoff_t pg_start, pgoff_t pg_end)
-{
-	int err;
+	f = fdget_pos(info.inner_fd);
+	if (!f.file)
+		return -EBADF;
 
-	while (pg_start < pg_end) {
-		struct dnode_of_data dn;
-		pgoff_t end_offset, count;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		goto out;
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		err = f2fs_get_dnode_of_data(&dn, pg_start, LOOKUP_NODE);
-		if (err) {
-			if (err == -ENOENT) {
-				pg_start = f2fs_get_next_page_offset(&dn,
-								pg_start);
-				continue;
-			}
-			return err;
-		}
+	inode_lock(inode);
+	if (f2fs_is_deduped_inode(inode)) {
+		f2fs_err(sbi, "The inode[%lu] has been two layer file.",
+			inode->i_ino);
+		ret = -EINVAL;
+		goto unlock;
+	}
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-		count = min(end_offset - dn.ofs_in_node, pg_end - pg_start);
+	inner_inode = file_inode(f.file);
+	if (inode->i_sb != inner_inode->i_sb) {
+		f2fs_err(sbi, "%s files should be in same FS ino:%lu, inner_ino:%lu",
+				__func__, inode->i_ino, inner_inode->i_ino);
+		ret = -EINVAL;
+		goto unlock;
+	}
 
-		f2fs_bug_on(F2FS_I_SB(inode), count == 0 || count > end_offset);
+	if (!IS_IMMUTABLE(inner_inode)) {
+		f2fs_err(sbi, "create layer fail inner[%lu] is not immutable.",
+			inner_inode->i_ino);
+		ret = -EINVAL;
+		goto unlock;
+	}
 
-		f2fs_truncate_data_blocks_range(&dn, count);
-		f2fs_put_dnode(&dn);
+	ret = f2fs_dedup_param_check(sbi, inode, OUTER_INODE,
+			inner_inode, INNER_INODE);
+	if (ret)
+		goto unlock;
 
-		pg_start += count;
+	if (inode->i_nlink == 0) {
+		f2fs_err(sbi,
+			"The inode[%lu] has been removed.", inode->i_ino);
+		ret = -ENOENT;
+		goto unlock;
 	}
-	return 0;
+
+	dir = dget_parent(file_dentry(filp));
+	ret = is_inode_match_dir_crypt_policy(dir, inner_inode);
+	dput(dir);
+	if (ret)
+		goto unlock;
+
+	filemap_fdatawrite(inode->i_mapping);
+	ret = f2fs_filemap_write_and_wait_range(inode);
+	if (ret)
+		goto unlock;
+
+	ret = __f2fs_ioc_create_layered_inode(inode, inner_inode);
+
+unlock:
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
+out:
+	fdput_pos(f);
+	return ret;
 }
 
-static int f2fs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
+static int __f2fs_ioc_dedup_file(struct inode *base_inode,
+		struct inode *dedup_inode)
 {
-	pgoff_t pg_start, pg_end;
-	loff_t off_start, off_end;
-	int ret;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup_inode);
+	struct inode *inner = get_inner_inode(base_inode);
+	int ret = 0;
+	struct f2fs_inode_info *fi = F2FS_I(dedup_inode);
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
+	if (!inner)
+		return -EBADF;
+
+	if (is_inode_flag_set(dedup_inode, FI_SNAPSHOTED))
+		goto skip;
+	if (!dedup_file_is_same(base_inode, dedup_inode, DEDUP_COMPARE_PAGES)) {
+		put_inner_inode(inner);
+		return -ESTALE;
+	}
+skip:
+	f2fs_lock_op(sbi);
+
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		ret = -ENOSPC;
+	else
+		ret = f2fs_acquire_orphan_inode(sbi);
+	if (ret) {
+		f2fs_unlock_op(sbi);
+		f2fs_err(sbi,
+			"dedup file acquire orphan fail, ino[%lu], base ino[%lu]",
+			dedup_inode->i_ino, base_inode->i_ino);
+		put_inner_inode(inner);
 		return ret;
+	}
+	f2fs_add_orphan_inode(dedup_inode);
 
-	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
-	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
+	f2fs_down_write(&fi->i_sem);
+	fi->inner_inode = inner;
+	set_inode_flag(dedup_inode, FI_DEDUPED);
+	set_inode_flag(dedup_inode, FI_DOING_DEDUP);
+	f2fs_up_write(&fi->i_sem);
 
-	off_start = offset & (PAGE_SIZE - 1);
-	off_end = (offset + len) & (PAGE_SIZE - 1);
+	f2fs_down_write(&F2FS_I(inner)->i_sem);
+	f2fs_i_links_write(inner, true);
+	f2fs_up_write(&F2FS_I(inner)->i_sem);
+	f2fs_unlock_op(sbi);
 
-	if (pg_start == pg_end) {
-		ret = fill_zero(inode, pg_start, off_start,
-						off_end - off_start);
-		if (ret)
-			return ret;
+	wait_event(fi->dedup_wq,
+			atomic_read(&fi->inflight_read_io) == 0);
+	dedup_wait_dio(dedup_inode);
+
+	f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+	/* GC may dirty pages before holding lock */
+	ret = f2fs_filemap_write_and_wait_range(dedup_inode);
+	if (ret)
+		goto out;
+
+	f2fs_lock_op(sbi);
+	f2fs_remove_orphan_inode(sbi, dedup_inode->i_ino);
+	ret = f2fs_truncate_dedup_inode(dedup_inode, FI_DOING_DEDUP);
+	/*
+	 * Since system may do checkpoint after unlock cp,
+	 * we set cp_ver here to let fsync know dedup have finish.
+	 */
+	F2FS_I(dedup_inode)->dedup_cp_ver = cur_cp_version(F2FS_CKPT(sbi));
+	f2fs_unlock_op(sbi);
+out:
+	f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+	f2fs_dedup_info(sbi, "%s inode[%lu] dedup success, inner[%lu], ret[%d]",
+			__func__, dedup_inode->i_ino, inner->i_ino, ret);
+	trace_f2fs_dedup_ioc_dedup_inode(dedup_inode, inner);
+	return ret;
+}
+
+static int f2fs_ioc_dedup_file(struct file *filp, unsigned long arg)
+{
+	struct inode *dedup_inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct inode *base_inode, *inner_inode;
+	struct dentry *dir;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup_inode);
+	struct f2fs_dedup_dst info;
+	struct fd f;
+	int ret;
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(dedup_inode))))
+		return -EIO;
+
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
+
+	if (!inode_owner_or_capable(mnt_userns, dedup_inode))
+		return -EACCES;
+
+	if (!f2fs_inode_support_dedup(sbi, dedup_inode))
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&info, (struct f2fs_dedup_dst __user *)arg, sizeof(info)))
+		return -EFAULT;
+
+	f = fdget_pos(info.base_fd);
+	if (!f.file)
+		return -EBADF;
+
+	base_inode = file_inode(f.file);
+	if (dedup_inode->i_sb != base_inode->i_sb) {
+		f2fs_err(sbi, "%s: files should be in same FS ino:%lu, base_ino:%lu",
+				__func__, dedup_inode->i_ino, base_inode->i_ino);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (!f2fs_inode_support_dedup(sbi, base_inode)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
+
+	if (base_inode == dedup_inode) {
+		f2fs_err(sbi, "%s: input inode[%lu] and [%lu] are same.",
+			__func__, base_inode->i_ino, dedup_inode->i_ino);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	// if try to dedup, clear FI_SNAPSHOT_PREPARED of base inode, else set for snapshot
+	if (is_inode_flag_set(dedup_inode, FI_SNAPSHOTED)) {
+		set_inode_flag(base_inode, FI_SNAPSHOT_PREPARED);
 	} else {
-		if (off_start) {
-			ret = fill_zero(inode, pg_start++, off_start,
-						PAGE_SIZE - off_start);
-			if (ret)
-				return ret;
-		}
-		if (off_end) {
-			ret = fill_zero(inode, pg_end, 0, off_end);
-			if (ret)
-				return ret;
-		}
+		clear_inode_flag(base_inode, FI_SNAPSHOT_PREPARED);
+	}
 
-		if (pg_start < pg_end) {
-			loff_t blk_start, blk_end;
-			struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		goto out;
 
-			f2fs_balance_fs(sbi, true);
+	// dedup_inode lock already hold if create snapshot
+	if (!is_inode_flag_set(dedup_inode, FI_SNAPSHOTED))
+		inode_lock(dedup_inode);
+	if (!inode_trylock(base_inode)) {
+		f2fs_err(sbi, "inode[%lu] can't get lock", base_inode->i_ino);
+		ret = -EAGAIN;
+		goto unlock2;
+	}
 
-			blk_start = (loff_t)pg_start << PAGE_SHIFT;
-			blk_end = (loff_t)pg_end << PAGE_SHIFT;
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(base_inode) ||
+		f2fs_seqzone_file(dedup_inode)) {
+		ret = -EINVAL;
+		goto unlock1;
+	}
+#endif
+	if (f2fs_is_deduped_inode(dedup_inode)) {
+		f2fs_err(sbi, "dedup inode[%lu] has been two layer inode",
+			dedup_inode->i_ino);
+		ret = -EINVAL;
+		goto unlock1;
+	}
 
-			f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-			filemap_invalidate_lock(inode->i_mapping);
+	if (dedup_inode->i_nlink == 0) {
+		f2fs_err(sbi,
+			"dedup inode[%lu] has been removed.", dedup_inode->i_ino);
+		ret = -ENOENT;
+		goto unlock1;
+	}
 
-			truncate_pagecache_range(inode, blk_start, blk_end - 1);
+	if (!f2fs_is_outer_inode(base_inode)) {
+		f2fs_err(sbi, "base inode[%lu] is not outer inode",
+			base_inode->i_ino);
+		ret = -EINVAL;
+		goto unlock1;
+	}
 
-			f2fs_lock_op(sbi);
-			ret = f2fs_truncate_hole(inode, pg_start, pg_end);
-			f2fs_unlock_op(sbi);
+	ret = f2fs_dedup_param_check(sbi, base_inode, OUTER_INODE,
+			dedup_inode, OUTER_INODE);
+	if (ret)
+		goto unlock1;
 
-			filemap_invalidate_unlock(inode->i_mapping);
-			f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-		}
+	dir = dget_parent(file_dentry(filp));
+	inner_inode = get_inner_inode(base_inode);
+	if (likely(inner_inode)) {
+		ret = is_inode_match_dir_crypt_policy(dir, inner_inode);
+		put_inner_inode(inner_inode);
 	}
+	dput(dir);
+	if (ret)
+		goto unlock1;
 
+	ret = deduped_files_match_fscrypt_policy(filp, f.file);
+	if (ret)
+		goto unlock1;
+
+	filemap_fdatawrite(dedup_inode->i_mapping);
+	ret = f2fs_filemap_write_and_wait_range(dedup_inode);
+	if (ret)
+		goto unlock1;
+
+	ret = __f2fs_ioc_dedup_file(base_inode, dedup_inode);
+
+unlock1:
+	inode_unlock(base_inode);
+unlock2:
+	if (!is_inode_flag_set(dedup_inode, FI_SNAPSHOTED))
+		inode_unlock(dedup_inode);
+	mnt_drop_write_file(filp);
+out:
+	fdput_pos(f);
 	return ret;
 }
 
-static int __read_out_blkaddrs(struct inode *inode, block_t *blkaddr,
-				int *do_replace, pgoff_t off, pgoff_t len)
+static int f2fs_ioc_dedup_revoke(struct file *filp, unsigned long arg)
 {
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct dnode_of_data dn;
-	int ret, done, i;
+	int ret = 0;
 
-next_dnode:
-	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	ret = f2fs_get_dnode_of_data(&dn, off, LOOKUP_NODE_RA);
-	if (ret && ret != -ENOENT) {
-		return ret;
-	} else if (ret == -ENOENT) {
-		if (dn.max_level == 0)
-			return -ENOENT;
-		done = min((pgoff_t)ADDRS_PER_BLOCK(inode) -
-						dn.ofs_in_node, len);
-		blkaddr += done;
-		do_replace += done;
-		goto next;
-	}
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
 
-	done = min((pgoff_t)ADDRS_PER_PAGE(dn.node_page, inode) -
-							dn.ofs_in_node, len);
-	for (i = 0; i < done; i++, blkaddr++, do_replace++, dn.ofs_in_node++) {
-		*blkaddr = f2fs_data_blkaddr(&dn);
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-		if (__is_valid_data_blkaddr(*blkaddr) &&
-			!f2fs_is_valid_blkaddr(sbi, *blkaddr,
-					DATA_GENERIC_ENHANCE)) {
-			f2fs_put_dnode(&dn);
-			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-			return -EFSCORRUPTED;
-		}
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-		if (!f2fs_is_checkpointed_data(sbi, *blkaddr)) {
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
 
-			if (f2fs_lfs_mode(sbi)) {
-				f2fs_put_dnode(&dn);
-				return -EOPNOTSUPP;
-			}
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
 
-			/* do not invalidate this block address */
-			f2fs_update_data_blkaddr(&dn, NULL_ADDR);
-			*do_replace = 1;
-		}
-	}
-	f2fs_put_dnode(&dn);
-next:
-	len -= done;
-	off += done;
-	if (len)
-		goto next_dnode;
-	return 0;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	inode_lock(inode);
+	ret = f2fs_revoke_deduped_inode(inode, __func__);
+	inode_unlock(inode);
+
+	mnt_drop_write_file(filp);
+	return ret;
 }
 
-static int __roll_back_blkaddrs(struct inode *inode, block_t *blkaddr,
-				int *do_replace, pgoff_t off, int len)
+static int f2fs_ioc_get_dedupd_file_info(struct file *filp, unsigned long arg)
 {
+	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct dnode_of_data dn;
-	int ret, i;
+	struct f2fs_dedup_file_info info = {0};
+	struct inode *inner_inode;
+	int ret = 0;
 
-	for (i = 0; i < len; i++, do_replace++, blkaddr++) {
-		if (*do_replace == 0)
-			continue;
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		ret = f2fs_get_dnode_of_data(&dn, off + i, LOOKUP_NODE_RA);
-		if (ret) {
-			dec_valid_block_count(sbi, inode, 1);
-			f2fs_invalidate_blocks(sbi, *blkaddr, 1);
-		} else {
-			f2fs_update_data_blkaddr(&dn, *blkaddr);
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
+
+	inode_lock(inode);
+
+	if (!is_inode_flag_set(inode, FI_DEDUPED)) {
+		info.is_layered = false;
+		info.is_deduped = false;
+	} else {
+		info.is_layered = true;
+		inner_inode = F2FS_I(inode)->inner_inode;
+		if (inner_inode) {
+			f2fs_down_write(&F2FS_I(inner_inode)->i_sem);
+			if (inner_inode->i_nlink > 1)
+				info.is_deduped = true;
+
+			info.group = inner_inode->i_ino;
+			f2fs_up_write(&F2FS_I(inner_inode)->i_sem);
 		}
-		f2fs_put_dnode(&dn);
 	}
+
+	inode_unlock(inode);
+
+	if (copy_to_user((struct f2fs_dedup_file_info __user *)arg, &info, sizeof(info)))
+		ret = -EFAULT;
+
+	return ret;
+}
+
+/* used for dedup big data statistics */
+static int f2fs_ioc_get_dedup_sysinfo(struct file *filp, unsigned long arg)
+{
 	return 0;
 }
 
-static int __clone_blkaddrs(struct inode *src_inode, struct inode *dst_inode,
-			block_t *blkaddr, int *do_replace,
-			pgoff_t src, pgoff_t dst, pgoff_t len, bool full)
+static int f2fs_ioc_create_snapshot(struct file *filp, unsigned long arg)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(src_inode);
-	pgoff_t i = 0;
-	int ret;
+	struct inode *inode = file_inode(filp);
+	int ret = 0;
+	struct inode *base_inode;
+	struct f2fs_dedup_dst info;
+	struct fd f;
 
-	while (i < len) {
-		if (blkaddr[i] == NULL_ADDR && !full) {
-			i++;
-			continue;
-		}
+	if (copy_from_user(&info, (struct f2fs_dedup_dst __user *)arg, sizeof(info)))
+		return -EFAULT;
 
-		if (do_replace[i] || blkaddr[i] == NULL_ADDR) {
-			struct dnode_of_data dn;
-			struct node_info ni;
-			size_t new_size;
-			pgoff_t ilen;
+	inode_lock(inode);
 
-			set_new_dnode(&dn, dst_inode, NULL, NULL, 0);
-			ret = f2fs_get_dnode_of_data(&dn, dst + i, ALLOC_NODE);
-			if (ret)
-				return ret;
-
-			ret = f2fs_get_node_info(sbi, dn.nid, &ni, false);
-			if (ret) {
-				f2fs_put_dnode(&dn);
-				return ret;
-			}
-
-			ilen = min((pgoff_t)
-				ADDRS_PER_PAGE(dn.node_page, dst_inode) -
-						dn.ofs_in_node, len - i);
-			do {
-				dn.data_blkaddr = f2fs_data_blkaddr(&dn);
-				f2fs_truncate_data_blocks_range(&dn, 1);
-
-				if (do_replace[i]) {
-					f2fs_i_blocks_write(src_inode,
-							1, false, false);
-					f2fs_i_blocks_write(dst_inode,
-							1, true, false);
-					f2fs_replace_block(sbi, &dn, dn.data_blkaddr,
-					blkaddr[i], ni.version, true, false);
-
-					do_replace[i] = 0;
-				}
-				dn.ofs_in_node++;
-				i++;
-				new_size = (loff_t)(dst + i) << PAGE_SHIFT;
-				if (dst_inode->i_size < new_size)
-					f2fs_i_size_write(dst_inode, new_size);
-			} while (--ilen && (do_replace[i] || blkaddr[i] == NULL_ADDR));
-
-			f2fs_put_dnode(&dn);
-		} else {
-			struct page *psrc, *pdst;
-
-			psrc = f2fs_get_lock_data_page(src_inode,
-							src + i, true);
-			if (IS_ERR(psrc))
-				return PTR_ERR(psrc);
-			pdst = f2fs_get_new_data_page(dst_inode, NULL, dst + i,
-								true);
-			if (IS_ERR(pdst)) {
-				f2fs_put_page(psrc, 1);
-				return PTR_ERR(pdst);
-			}
-			memcpy_page(pdst, 0, psrc, 0, PAGE_SIZE);
-			set_page_dirty(pdst);
-			f2fs_put_page(pdst, 1);
-			f2fs_put_page(psrc, 1);
-
-			ret = f2fs_truncate_hole(src_inode,
-						src + i, src + i + 1);
-			if (ret)
-				return ret;
-			i++;
-		}
+	//do not create snapshot for a snapshot file
+	if (is_inode_flag_set(inode, FI_SNAPSHOTED)) {
+		ret = -EOPNOTSUPP;
+		goto unlock;
 	}
-	return 0;
-}
-
-static int __exchange_data_block(struct inode *src_inode,
-			struct inode *dst_inode, pgoff_t src, pgoff_t dst,
-			pgoff_t len, bool full)
-{
-	block_t *src_blkaddr;
-	int *do_replace;
-	pgoff_t olen;
-	int ret;
-
-	while (len) {
-		olen = min((pgoff_t)4 * ADDRS_PER_BLOCK(src_inode), len);
-
-		src_blkaddr = f2fs_kvzalloc(F2FS_I_SB(src_inode),
-					array_size(olen, sizeof(block_t)),
-					GFP_NOFS);
-		if (!src_blkaddr)
-			return -ENOMEM;
-
-		do_replace = f2fs_kvzalloc(F2FS_I_SB(src_inode),
-					array_size(olen, sizeof(int)),
-					GFP_NOFS);
-		if (!do_replace) {
-			kvfree(src_blkaddr);
-			return -ENOMEM;
+	set_inode_flag(inode, FI_SNAPSHOTED);
+	ret = f2fs_ioc_dedup_file(filp, arg);
+	if (!ret) {
+		f = fdget_pos(info.base_fd);
+		if (!f.file) {
+			ret = -EBADF;
+			goto unlock;
 		}
-
-		ret = __read_out_blkaddrs(src_inode, src_blkaddr,
-					do_replace, src, olen);
-		if (ret)
-			goto roll_back;
-
-		ret = __clone_blkaddrs(src_inode, dst_inode, src_blkaddr,
-					do_replace, src, dst, olen, full);
-		if (ret)
-			goto roll_back;
-
-		src += olen;
-		dst += olen;
-		len -= olen;
-
-		kvfree(src_blkaddr);
-		kvfree(do_replace);
+		base_inode = file_inode(f.file);
+		inode_lock(base_inode);
+		clear_inode_flag(base_inode, FI_SNAPSHOT_PREPARED);
+		//clone ctime/mtime... from base to snapshot
+		inode->i_atime.tv_sec = base_inode->i_atime.tv_sec;
+		inode->i_ctime.tv_sec = base_inode->i_ctime.tv_sec;
+		inode->i_mtime.tv_sec = base_inode->i_mtime.tv_sec;
+		inode->i_atime.tv_nsec = base_inode->i_atime.tv_nsec;
+		inode->i_ctime.tv_nsec = base_inode->i_ctime.tv_nsec;
+		inode->i_mtime.tv_nsec = base_inode->i_mtime.tv_nsec;
+		inode_unlock(base_inode);
+		fdput_pos(f);
+	} else {
+		clear_inode_flag(inode, FI_SNAPSHOTED);
 	}
-	return 0;
 
-roll_back:
-	__roll_back_blkaddrs(src_inode, src_blkaddr, do_replace, src, olen);
-	kvfree(src_blkaddr);
-	kvfree(do_replace);
+unlock:
+	inode_unlock(inode);
 	return ret;
 }
 
-static int f2fs_do_collapse(struct inode *inode, loff_t offset, loff_t len)
+static int f2fs_ioc_prepare_snapshot(struct file *filp, unsigned long arg)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	pgoff_t nrpages = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
-	pgoff_t start = offset >> PAGE_SHIFT;
-	pgoff_t end = (offset + len) >> PAGE_SHIFT;
-	int ret;
-
-	f2fs_balance_fs(sbi, true);
-
-	/* avoid gc operation during block exchange */
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(inode->i_mapping);
-
-	f2fs_lock_op(sbi);
-	f2fs_drop_extent_tree(inode);
-	truncate_pagecache(inode, offset);
-	ret = __exchange_data_block(inode, inode, end, start, nrpages - end, true);
-	f2fs_unlock_op(sbi);
+	struct inode *inode = file_inode(filp);
+	int ret = 0;
 
-	filemap_invalidate_unlock(inode->i_mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	set_inode_flag(inode, FI_SNAPSHOT_PREPARED);
+	ret = f2fs_ioc_create_layered_inode(filp, arg);
+	if (ret)
+		clear_inode_flag(inode, FI_SNAPSHOT_PREPARED);
 	return ret;
 }
 
-static int f2fs_collapse_range(struct inode *inode, loff_t offset, loff_t len)
+#endif
+static vm_fault_t f2fs_filemap_fault(struct vm_fault *vmf)
 {
-	loff_t new_size;
-	int ret;
-
-	if (offset + len >= i_size_read(inode))
-		return -EINVAL;
-
-	/* collapse range should be aligned to block size of f2fs. */
-	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
-		return -EINVAL;
-
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		return ret;
-
-	/* write out all dirty pages from offset */
-	ret = filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
-	if (ret)
-		return ret;
+	struct inode *inode = file_inode(vmf->vma->vm_file);
+	vm_fault_t ret;
 
-	ret = f2fs_do_collapse(inode, offset, len);
-	if (ret)
-		return ret;
+	ret = filemap_fault(vmf);
+	if (!ret)
+		f2fs_update_iostat(F2FS_I_SB(inode), inode,
+					APP_MAPPED_READ_IO, F2FS_BLKSIZE);
 
-	/* write out all moved pages, if possible */
-	filemap_invalidate_lock(inode->i_mapping);
-	filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
-	truncate_pagecache(inode, offset);
+	trace_f2fs_filemap_fault(inode, vmf->pgoff, (unsigned long)ret);
 
-	new_size = i_size_read(inode) - len;
-	ret = f2fs_truncate_blocks(inode, new_size, true);
-	filemap_invalidate_unlock(inode->i_mapping);
-	if (!ret)
-		f2fs_i_size_write(inode, new_size);
 	return ret;
 }
 
-static int f2fs_do_zero_range(struct dnode_of_data *dn, pgoff_t start,
-								pgoff_t end)
+static vm_fault_t f2fs_vm_page_mkwrite(struct vm_fault *vmf)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
-	pgoff_t index = start;
-	unsigned int ofs_in_node = dn->ofs_in_node;
-	blkcnt_t count = 0;
-	int ret;
+	struct page *page = vmf->page;
+	struct inode *inode = file_inode(vmf->vma->vm_file);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	bool need_alloc = true;
+	int err = 0;
 
-	for (; index < end; index++, dn->ofs_in_node++) {
-		if (f2fs_data_blkaddr(dn) == NULL_ADDR)
-			count++;
+	if (unlikely(IS_IMMUTABLE(inode)))
+		return VM_FAULT_SIGBUS;
+
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		inode_lock(inode);
+		err = f2fs_reserve_compress_blocks(inode, NULL);
+		inode_unlock(inode);
+		if (err < 0)
+			goto err;
+#else
+		return VM_FAULT_SIGBUS;
+#endif
 	}
 
-	dn->ofs_in_node = ofs_in_node;
-	ret = f2fs_reserve_new_blocks(dn, count);
-	if (ret)
-		return ret;
+	if (unlikely(f2fs_cp_error(sbi))) {
+		err = -EIO;
+		goto err;
+	}
 
-	dn->ofs_in_node = ofs_in_node;
-	for (index = start; index < end; index++, dn->ofs_in_node++) {
-		dn->data_blkaddr = f2fs_data_blkaddr(dn);
-		/*
-		 * f2fs_reserve_new_blocks will not guarantee entire block
-		 * allocation.
-		 */
-		if (dn->data_blkaddr == NULL_ADDR) {
-			ret = -ENOSPC;
-			break;
-		}
+	if (!f2fs_is_checkpoint_ready(sbi)) {
+		err = -ENOSPC;
+		goto err;
+	}
 
-		if (dn->data_blkaddr == NEW_ADDR)
-			continue;
+	err = f2fs_convert_inline_inode(inode);
+	if (err)
+		goto err;
 
-		if (!f2fs_is_valid_blkaddr(sbi, dn->data_blkaddr,
-					DATA_GENERIC_ENHANCE)) {
-			ret = -EFSCORRUPTED;
-			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-			break;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	inode_lock(inode);
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		err = f2fs_revoke_deduped_inode(inode, __func__);
+		if (err) {
+			inode_unlock(inode);
+			goto err;
 		}
-
-		f2fs_invalidate_blocks(sbi, dn->data_blkaddr, 1);
-		f2fs_set_data_blkaddr(dn, NEW_ADDR);
 	}
+	inode_unlock(inode);
+#endif
 
-	f2fs_update_read_extent_cache_range(dn, start, 0, index - start);
-	f2fs_update_age_extent_cache_range(dn, start, index - start);
-
-	return ret;
-}
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	if (f2fs_compressed_file(inode)) {
+		int ret;
+		CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+		ret = f2fs_is_compressed_cluster(inode, page->index);
 
-static int f2fs_zero_range(struct inode *inode, loff_t offset, loff_t len,
-								int mode)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct address_space *mapping = inode->i_mapping;
-	pgoff_t index, pg_start, pg_end;
-	loff_t new_size = i_size_read(inode);
-	loff_t off_start, off_end;
-	int ret = 0;
+		if (ret < 0) {
+			err = ret;
+			goto err;
+		} else if (ret) {
+			need_alloc = false;
+		}
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		inode_lock(inode);
+		clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+		inode_unlock(inode);
+#endif
+	}
+#endif
+	/* should do out of any locked page */
+	if (need_alloc)
+		f2fs_balance_fs(sbi, true);
 
-	ret = inode_newsize_ok(inode, (len + offset));
-	if (ret)
-		return ret;
+	sb_start_pagefault(inode->i_sb);
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		return ret;
+	f2fs_bug_on(sbi, f2fs_has_inline_data(inode));
 
-	ret = filemap_write_and_wait_range(mapping, offset, offset + len - 1);
-	if (ret)
-		return ret;
+	file_update_time(vmf->vma->vm_file);
+	filemap_invalidate_lock_shared(inode->i_mapping);
+	lock_page(page);
+	if (unlikely(page->mapping != inode->i_mapping ||
+			page_offset(page) > i_size_read(inode) ||
+			!PageUptodate(page))) {
+		unlock_page(page);
+		err = -EFAULT;
+		goto out_sem;
+	}
 
-	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
-	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
+	if (need_alloc) {
+		/* block allocation */
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		err = f2fs_get_block_locked(&dn, page->index);
+	}
 
-	off_start = offset & (PAGE_SIZE - 1);
-	off_end = (offset + len) & (PAGE_SIZE - 1);
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	if (!need_alloc) {
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		err = f2fs_get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
+		f2fs_put_dnode(&dn);
+	}
+#endif
+	if (err) {
+		unlock_page(page);
+		goto out_sem;
+	}
 
-	if (pg_start == pg_end) {
-		ret = fill_zero(inode, pg_start, off_start,
-						off_end - off_start);
-		if (ret)
-			return ret;
+	f2fs_wait_on_page_writeback(page, DATA, false, true);
 
-		new_size = max_t(loff_t, new_size, offset + len);
-	} else {
-		if (off_start) {
-			ret = fill_zero(inode, pg_start++, off_start,
-						PAGE_SIZE - off_start);
-			if (ret)
-				return ret;
+	/* wait for GCed page writeback via META_MAPPING */
+	f2fs_wait_on_block_writeback(inode, dn.data_blkaddr);
 
-			new_size = max_t(loff_t, new_size,
-					(loff_t)pg_start << PAGE_SHIFT);
-		}
+	/*
+	 * check to see if the page is mapped already (no holes)
+	 */
+	if (PageMappedToDisk(page))
+		goto out_sem;
 
-		for (index = pg_start; index < pg_end;) {
-			struct dnode_of_data dn;
-			unsigned int end_offset;
-			pgoff_t end;
+	/* page is wholly or partially inside EOF */
+	if (((loff_t)(page->index + 1) << PAGE_SHIFT) >
+						i_size_read(inode)) {
+		loff_t offset;
 
-			f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-			filemap_invalidate_lock(mapping);
+		offset = i_size_read(inode) & ~PAGE_MASK;
+		zero_user_segment(page, offset, PAGE_SIZE);
+	}
+	set_page_dirty(page);
+	if (!PageUptodate(page))
+		SetPageUptodate(page);
 
-			truncate_pagecache_range(inode,
-				(loff_t)index << PAGE_SHIFT,
-				((loff_t)pg_end << PAGE_SHIFT) - 1);
+	f2fs_update_iostat(sbi, inode, APP_MAPPED_IO, F2FS_BLKSIZE);
+	f2fs_update_time(sbi, REQ_TIME);
 
-			f2fs_lock_op(sbi);
+	trace_f2fs_vm_page_mkwrite(page, DATA);
+out_sem:
+	filemap_invalidate_unlock_shared(inode->i_mapping);
 
-			set_new_dnode(&dn, inode, NULL, NULL, 0);
-			ret = f2fs_get_dnode_of_data(&dn, index, ALLOC_NODE);
-			if (ret) {
-				f2fs_unlock_op(sbi);
-				filemap_invalidate_unlock(mapping);
-				f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-				goto out;
-			}
+	sb_end_pagefault(inode->i_sb);
+err:
+	return block_page_mkwrite_return(err);
+}
 
-			end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-			end = min(pg_end, end_offset - dn.ofs_in_node + index);
+static const struct vm_operations_struct f2fs_file_vm_ops = {
+	.fault		= f2fs_filemap_fault,
+	.map_pages	= filemap_map_pages,
+	.page_mkwrite	= f2fs_vm_page_mkwrite,
+};
 
-			ret = f2fs_do_zero_range(&dn, index, end);
-			f2fs_put_dnode(&dn);
+static int get_parent_ino(struct inode *inode, nid_t *pino)
+{
+	struct dentry *dentry;
 
-			f2fs_unlock_op(sbi);
-			filemap_invalidate_unlock(mapping);
-			f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	/*
+	 * Make sure to get the non-deleted alias.  The alias associated with
+	 * the open file descriptor being fsync()'ed may be deleted already.
+	 */
+	dentry = d_find_alias(inode);
+	if (!dentry)
+		return 0;
 
-			f2fs_balance_fs(sbi, dn.node_changed);
+	*pino = parent_ino(dentry);
+	dput(dentry);
+	return 1;
+}
 
-			if (ret)
-				goto out;
+static inline enum cp_reason_type need_do_checkpoint(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	enum cp_reason_type cp_reason = CP_NO_NEEDED;
 
-			index = end;
-			new_size = max_t(loff_t, new_size,
-					(loff_t)index << PAGE_SHIFT);
-		}
+	if (!S_ISREG(inode->i_mode))
+		cp_reason = CP_NON_REGULAR;
+	else if (f2fs_compressed_file(inode))
+		cp_reason = CP_COMPRESSED;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	/*
+	 * If inode have do dedup or revoke recently, we need to do
+	 * checkpoint to avoid roll forward recovery after fsync,
+	 * which may cause data inconsistency.
+	 */
+	else if (F2FS_I(inode)->dedup_cp_ver == cur_cp_version(F2FS_CKPT(sbi)))
+		cp_reason = CP_DEDUPED;
+#endif
+	else if (inode->i_nlink != 1)
+		cp_reason = CP_HARDLINK;
+	else if (is_sbi_flag_set(sbi, SBI_NEED_CP))
+		cp_reason = CP_SB_NEED_CP;
+	else if (file_wrong_pino(inode))
+		cp_reason = CP_WRONG_PINO;
+	else if (!f2fs_space_for_roll_forward(sbi))
+		cp_reason = CP_NO_SPC_ROLL;
+	else if (!f2fs_is_checkpointed_node(sbi, F2FS_I(inode)->i_pino))
+		cp_reason = CP_NODE_NEED_CP;
+	else if (test_opt(sbi, FASTBOOT))
+		cp_reason = CP_FASTBOOT_MODE;
+	else if (F2FS_OPTION(sbi).active_logs == 2)
+		cp_reason = CP_SPEC_LOG_NUM;
+	else if (F2FS_OPTION(sbi).fsync_mode == FSYNC_MODE_STRICT &&
+		f2fs_need_dentry_mark(sbi, inode->i_ino) &&
+		f2fs_exist_written_data(sbi, F2FS_I(inode)->i_pino,
+							TRANS_DIR_INO))
+		cp_reason = CP_RECOVER_DIR;
+	else if (f2fs_exist_written_data(sbi, F2FS_I(inode)->i_pino,
+							XATTR_DIR_INO))
+		cp_reason = CP_XATTR_DIR;
 
-		if (off_end) {
-			ret = fill_zero(inode, pg_end, 0, off_end);
-			if (ret)
-				goto out;
+	return cp_reason;
+}
 
-			new_size = max_t(loff_t, new_size, offset + len);
-		}
-	}
+static bool need_inode_page_update(struct f2fs_sb_info *sbi, nid_t ino)
+{
+	struct page *i = find_get_page(NODE_MAPPING(sbi), ino);
+	bool ret = false;
+	/* But we need to avoid that there are some inode updates */
+	if ((i && PageDirty(i)) || f2fs_need_inode_block_update(sbi, ino))
+		ret = true;
+	f2fs_put_page(i, 0);
+	return ret;
+}
 
-out:
-	if (new_size > i_size_read(inode)) {
-		if (mode & FALLOC_FL_KEEP_SIZE)
-			file_set_keep_isize(inode);
-		else
-			f2fs_i_size_write(inode, new_size);
+static void try_to_fix_pino(struct inode *inode)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	nid_t pino;
+
+	f2fs_down_write(&fi->i_sem);
+	if (file_wrong_pino(inode) && inode->i_nlink == 1 &&
+			get_parent_ino(inode, &pino)) {
+		f2fs_i_pino_write(inode, pino);
+		file_got_pino(inode);
 	}
-	return ret;
+	f2fs_up_write(&fi->i_sem);
 }
 
-static int f2fs_insert_range(struct inode *inode, loff_t offset, loff_t len)
+static int f2fs_do_sync_file(struct file *file, loff_t start, loff_t end,
+						int datasync, bool atomic)
 {
+	struct inode *inode = file->f_mapping->host;
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct address_space *mapping = inode->i_mapping;
-	pgoff_t nr, pg_start, pg_end, delta, idx;
-	loff_t new_size;
+	nid_t ino = inode->i_ino;
 	int ret = 0;
+	enum cp_reason_type cp_reason = 0;
+	struct writeback_control wbc = {
+		.sync_mode = WB_SYNC_ALL,
+		.nr_to_write = LONG_MAX,
+		.for_reclaim = 0,
+	};
+	unsigned int seq_id = 0;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL, *outer = NULL;
+
+	if(is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED)) {
+		inner = get_inner_inode(outer);
+		if (inner) {
+			outer = inode;
+			inode = inner;
+		}
+	}
+#endif
+	if (unlikely(f2fs_readonly(inode->i_sb)))
+		return 0;
 
-	new_size = i_size_read(inode) + len;
-	ret = inode_newsize_ok(inode, new_size);
-	if (ret)
-		return ret;
+	trace_f2fs_sync_file_enter(inode);
 
-	if (offset >= i_size_read(inode))
-		return -EINVAL;
+	if (S_ISDIR(inode->i_mode))
+		goto go_write;
 
-	/* insert range should be aligned to block size of f2fs. */
-	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
-		return -EINVAL;
+	/* if fdatasync is triggered, let's do in-place-update */
+#ifdef CONFIG_F2FS_SEQZONE
+	if (datasync || get_dirty_pages(inode) <= DEF_MIN_FSYNC_BLOCKS)
+#else
+	if (datasync || get_dirty_pages(inode) <= SM_I(sbi)->min_fsync_blocks)
+#endif
+		set_inode_flag(inode, FI_NEED_IPU);
+	ret = file_write_and_wait_range(file, start, end);
+	clear_inode_flag(inode, FI_NEED_IPU);
+
+	if (ret || is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {
+		trace_f2fs_sync_file_exit(inode, cp_reason, datasync, ret);
+		return ret;
+	}
+
+	/* if the inode is dirty, let's recover all the time */
+	if (!f2fs_skip_inode_update(inode, datasync)) {
+		f2fs_write_inode(inode, NULL);
+		goto go_write;
+	}
+
+	/*
+	 * if there is no written data, don't waste time to write recovery info.
+	 */
+	if (!is_inode_flag_set(inode, FI_APPEND_WRITE) &&
+			!f2fs_exist_written_data(sbi, ino, APPEND_INO)) {
+
+		/* it may call write_inode just prior to fsync */
+		if (need_inode_page_update(sbi, ino))
+			goto go_write;
+
+		if (is_inode_flag_set(inode, FI_UPDATE_WRITE) ||
+				f2fs_exist_written_data(sbi, ino, UPDATE_INO))
+			goto flush_out;
+		goto out;
+	} else {
+		/*
+		 * for OPU case, during fsync(), node can be persisted before
+		 * data when lower device doesn't support write barrier, result
+		 * in data corruption after SPO.
+		 * So for strict fsync mode, force to use atomic write semantics
+		 * to keep write order in between data/node and last node to
+		 * avoid potential data corruption.
+		 */
+		if (F2FS_OPTION(sbi).fsync_mode ==
+				FSYNC_MODE_STRICT && !atomic)
+			atomic = true;
+	}
+go_write:
+	/*
+	 * Both of fdatasync() and fsync() are able to be recovered from
+	 * sudden-power-off.
+	 */
+	f2fs_down_read(&F2FS_I(inode)->i_sem);
+	cp_reason = need_do_checkpoint(inode);
+	f2fs_up_read(&F2FS_I(inode)->i_sem);
+
+	if (cp_reason) {
+		/* all the dirty node pages should be flushed for POR */
+		ret = f2fs_sync_fs(inode->i_sb, 1);
+
+		/*
+		 * We've secured consistency through sync_fs. Following pino
+		 * will be used only for fsynced inodes after checkpoint.
+		 */
+		try_to_fix_pino(inode);
+		clear_inode_flag(inode, FI_APPEND_WRITE);
+		clear_inode_flag(inode, FI_UPDATE_WRITE);
+		goto out;
+	}
+sync_nodes:
+	atomic_inc(&sbi->wb_sync_req[NODE]);
+	ret = f2fs_fsync_node_pages(sbi, inode, &wbc, atomic, &seq_id);
+	atomic_dec(&sbi->wb_sync_req[NODE]);
+	if (ret)
+		goto out;
+
+	/* if cp_error was enabled, we should avoid infinite loop */
+	if (unlikely(f2fs_cp_error(sbi))) {
+		ret = -EIO;
+		goto out;
+	}
+
+	if (f2fs_need_inode_block_update(sbi, ino)) {
+		f2fs_mark_inode_dirty_sync(inode, true);
+		f2fs_write_inode(inode, NULL);
+		goto sync_nodes;
+	}
+
+	/*
+	 * If it's atomic_write, it's just fine to keep write ordering. So
+	 * here we don't need to wait for node write completion, since we use
+	 * node chain which serializes node blocks. If one of node writes are
+	 * reordered, we can see simply broken chain, resulting in stopping
+	 * roll-forward recovery. It means we'll recover all or none node blocks
+	 * given fsync mark.
+	 */
+	if (!atomic) {
+		ret = f2fs_wait_on_node_pages_writeback(sbi, seq_id);
+		if (ret)
+			goto out;
+	}
+
+	/* once recovery info is written, don't need to tack this */
+	f2fs_remove_ino_entry(sbi, ino, APPEND_INO);
+	clear_inode_flag(inode, FI_APPEND_WRITE);
+flush_out:
+	if ((!atomic && F2FS_OPTION(sbi).fsync_mode != FSYNC_MODE_NOBARRIER) ||
+	    (atomic && !test_opt(sbi, NOBARRIER) && f2fs_sb_has_blkzoned(sbi)))
+		ret = f2fs_issue_flush(sbi, inode->i_ino);
+	if (!ret) {
+		f2fs_remove_ino_entry(sbi, ino, UPDATE_INO);
+		clear_inode_flag(inode, FI_UPDATE_WRITE);
+		f2fs_remove_ino_entry(sbi, ino, FLUSH_INO);
+	}
+	f2fs_update_time(sbi, REQ_TIME);
+out:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner) {
+		inode = outer;
+		put_inner_inode(inner);
+	}
+#endif
+	trace_f2fs_sync_file_exit(inode, cp_reason, datasync, ret);
+	return ret;
+}
+
+int f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
+{
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(file_inode(file)))))
+		return -EIO;
+	return f2fs_do_sync_file(file, start, end, datasync, false);
+}
+
+static bool __found_offset(struct address_space *mapping,
+		struct dnode_of_data *dn, pgoff_t index, int whence)
+{
+	block_t blkaddr = f2fs_data_blkaddr(dn);
+	struct inode *inode = mapping->host;
+	bool compressed_cluster = false;
+
+	if (f2fs_compressed_file(inode)) {
+		block_t first_blkaddr = data_blkaddr(dn->inode, dn->node_page,
+		    ALIGN_DOWN(dn->ofs_in_node, F2FS_I(inode)->i_cluster_size));
+
+		compressed_cluster = first_blkaddr == COMPRESS_ADDR;
+	}
+
+	switch (whence) {
+	case SEEK_DATA:
+		if (__is_valid_data_blkaddr(blkaddr))
+			return true;
+		if (blkaddr == NEW_ADDR &&
+		    xa_get_mark(&mapping->i_pages, index, PAGECACHE_TAG_DIRTY))
+			return true;
+		if (compressed_cluster)
+			return true;
+		break;
+	case SEEK_HOLE:
+		if (compressed_cluster)
+			return false;
+		if (blkaddr == NULL_ADDR)
+			return true;
+		break;
+	}
+	return false;
+}
+
+static loff_t f2fs_seek_block(struct file *file, loff_t offset, int whence)
+{
+	struct inode *inode = file->f_mapping->host;
+	loff_t maxbytes = inode->i_sb->s_maxbytes;
+	struct dnode_of_data dn;
+	pgoff_t pgofs, end_offset;
+	loff_t data_ofs = offset;
+	loff_t isize;
+	int err = 0;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL, *exter = NULL;
+#endif
+
+	inode_lock(inode);
+
+	isize = i_size_read(inode);
+	if (offset >= isize)
+		goto fail;
+
+	/* handle inline data case */
+	if (f2fs_has_inline_data(inode)) {
+		if (whence == SEEK_HOLE) {
+			data_ofs = isize;
+			goto found;
+		} else if (whence == SEEK_DATA) {
+			data_ofs = offset;
+			goto found;
+		}
+	}
+
+	pgofs = (pgoff_t)(offset >> PAGE_SHIFT);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	inner = get_inner_inode(inode);
+	if (inner) {
+		exter = inode;
+		inode = inner;
+	}
+#endif
+	for (; data_ofs < isize; data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		err = f2fs_get_dnode_of_data(&dn, pgofs, LOOKUP_NODE);
+		if (err && err != -ENOENT) {
+			goto fail;
+		} else if (err == -ENOENT) {
+			/* direct node does not exists */
+			if (whence == SEEK_DATA) {
+				pgofs = f2fs_get_next_page_offset(&dn, pgofs);
+				continue;
+			} else {
+				goto found;
+			}
+		}
+
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+
+		/* find data/hole in dnode block */
+		for (; dn.ofs_in_node < end_offset;
+				dn.ofs_in_node++, pgofs++,
+				data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
+			block_t blkaddr;
+
+			blkaddr = f2fs_data_blkaddr(&dn);
+
+			if (__is_valid_data_blkaddr(blkaddr) &&
+				!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),
+					blkaddr, DATA_GENERIC_ENHANCE)) {
+				f2fs_put_dnode(&dn);
+				goto fail;
+			}
+
+			if (__found_offset(file->f_mapping, &dn,
+							pgofs, whence)) {
+				f2fs_put_dnode(&dn);
+				goto found;
+			}
+		}
+		f2fs_put_dnode(&dn);
+	}
+
+	if (whence == SEEK_DATA)
+		goto fail;
+found:
+	if (whence == SEEK_HOLE && data_ofs > isize)
+		data_ofs = isize;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner) {
+		inode = exter;
+		put_inner_inode(inner);
+	}
+#endif
+	inode_unlock(inode);
+	return vfs_setpos(file, data_ofs, maxbytes);
+fail:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner) {
+		inode = exter;
+		put_inner_inode(inner);
+	}
+#endif
+	inode_unlock(inode);
+	return -ENXIO;
+}
+
+static loff_t f2fs_llseek(struct file *file, loff_t offset, int whence)
+{
+	struct inode *inode = file->f_mapping->host;
+	loff_t maxbytes = inode->i_sb->s_maxbytes;
+
+	if (f2fs_compressed_file(inode))
+		maxbytes = max_file_blocks(inode) << F2FS_BLKSIZE_BITS;
+
+	switch (whence) {
+	case SEEK_SET:
+	case SEEK_CUR:
+	case SEEK_END:
+		return generic_file_llseek_size(file, offset, whence,
+						maxbytes, i_size_read(inode));
+	case SEEK_DATA:
+	case SEEK_HOLE:
+		if (offset < 0)
+			return -ENXIO;
+		return f2fs_seek_block(file, offset, whence);
+	}
+
+	return -EINVAL;
+}
+
+static int f2fs_file_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	struct inode *inode = file_inode(file);
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+
+	if (!f2fs_is_compress_backend_ready(inode))
+		return -EOPNOTSUPP;
+
+	file_accessed(file);
+	vma->vm_ops = &f2fs_file_vm_ops;
+
+	f2fs_down_read(&F2FS_I(inode)->i_sem);
+	set_inode_flag(inode, FI_MMAP_FILE);
+	f2fs_up_read(&F2FS_I(inode)->i_sem);
+
+	return 0;
+}
+
+static int finish_preallocate_blocks(struct inode *inode)
+{
+	int ret;
+
+	inode_lock(inode);
+	if (is_inode_flag_set(inode, FI_OPENED_FILE)) {
+		inode_unlock(inode);
+		return 0;
+	}
+
+	if (!file_should_truncate(inode)) {
+		set_inode_flag(inode, FI_OPENED_FILE);
+		inode_unlock(inode);
+		return 0;
+	}
+
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(inode->i_mapping);
+
+	truncate_setsize(inode, i_size_read(inode));
+	ret = f2fs_truncate(inode);
+
+	filemap_invalidate_unlock(inode->i_mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
+	if (!ret)
+		set_inode_flag(inode, FI_OPENED_FILE);
+
+	inode_unlock(inode);
+	if (ret)
+		return ret;
+
+	file_dont_truncate(inode);
+	return 0;
+}
+
+static int f2fs_release_file(struct inode *inode, struct file *filp);
+static int f2fs_file_open(struct inode *inode, struct file *filp)
+{
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL;
+#endif
+	int err;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_DEDUP_OPEN))
+		return -EIO;
+#endif
+
+	err = fscrypt_file_open(inode, filp);
+	if (err)
+		return err;
+
+	if (!f2fs_is_compress_backend_ready(inode))
+		return -EOPNOTSUPP;
+
+	err = fsverity_file_open(inode, filp);
+	if (err)
+		return err;
+
+	filp->f_mode |= FMODE_NOWAIT;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	err = dquot_file_open(inode, filp);
+	if (err)
+		return err;
+
+	if (f2fs_is_outer_inode(inode)) {
+		inner = get_inner_inode(inode);
+		if (inner) {
+			err = f2fs_file_open(inner, filp);
+		} else {
+			f2fs_release_file(inode, filp);
+			return -ENOENT;
+		}
+		put_inner_inode(inner);
+		return err;
+	}
+	return finish_preallocate_blocks(inode);
+#else
+	err = dquot_file_open(inode, filp);
+	if (err)
+		return err;
+
+	return finish_preallocate_blocks(inode);
+#endif
+}
+
+void f2fs_truncate_data_blocks_range(struct dnode_of_data *dn, int count)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	int nr_free = 0, ofs = dn->ofs_in_node, len = count;
+	__le32 *addr;
+	bool compressed_cluster = false;
+	int cluster_index = 0, valid_blocks = 0;
+	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
+	bool released = !atomic_read(&F2FS_I(dn->inode)->i_compr_blocks);
+
+	addr = get_dnode_addr(dn->inode, dn->node_page) + ofs;
+
+	/* Assumption: truncation starts with cluster */
+	for (; count > 0; count--, addr++, dn->ofs_in_node++, cluster_index++) {
+		block_t blkaddr = le32_to_cpu(*addr);
+
+		if (f2fs_compressed_file(dn->inode) &&
+					!(cluster_index & (cluster_size - 1))) {
+			if (compressed_cluster)
+				f2fs_i_compr_blocks_update(dn->inode,
+							valid_blocks, false);
+			compressed_cluster = (blkaddr == COMPRESS_ADDR);
+			valid_blocks = 0;
+		}
+
+		if (blkaddr == NULL_ADDR)
+			continue;
+
+#ifdef CONFIG_F2FS_FS_SEQZONE
+		if (f2fs_seqzone_file(dn->inode))
+			dn->seqzone_index = NULL_ADDR;
+#endif
+		f2fs_set_data_blkaddr(dn, NULL_ADDR);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (blkaddr == DEDUP_ADDR)
+			continue;
+#endif
+		if (__is_valid_data_blkaddr(blkaddr)) {
+			if (!f2fs_is_valid_blkaddr(sbi, blkaddr,
+					DATA_GENERIC_ENHANCE))
+				continue;
+			if (compressed_cluster)
+				valid_blocks++;
+		}
+
+		f2fs_invalidate_blocks(sbi, blkaddr);
+
+		if (!released || blkaddr != COMPRESS_ADDR)
+			nr_free++;
+	}
+
+	if (compressed_cluster)
+		f2fs_i_compr_blocks_update(dn->inode, valid_blocks, false);
+
+	if (nr_free) {
+		pgoff_t fofs;
+		/*
+		 * once we invalidate valid blkaddr in range [ofs, ofs + count],
+		 * we will invalidate all blkaddr in the whole range.
+		 */
+		fofs = f2fs_start_bidx_of_node(ofs_of_node(dn->node_page),
+							dn->inode) + ofs;
+		f2fs_update_read_extent_cache_range(dn, fofs, 0, len);
+		f2fs_update_age_extent_cache_range(dn, fofs, len);
+		dec_valid_block_count(sbi, dn->inode, nr_free);
+	}
+	dn->ofs_in_node = ofs;
+
+	f2fs_update_time(sbi, REQ_TIME);
+	trace_f2fs_truncate_data_blocks_range(dn->inode, dn->nid,
+					 dn->ofs_in_node, nr_free);
+}
+
+void f2fs_truncate_data_blocks(struct dnode_of_data *dn)
+{
+	f2fs_truncate_data_blocks_range(dn, ADDRS_PER_BLOCK(dn->inode));
+}
+
+static int truncate_partial_data_page(struct inode *inode, u64 from,
+								bool cache_only)
+{
+	loff_t offset = from & (PAGE_SIZE - 1);
+	pgoff_t index = from >> PAGE_SHIFT;
+	struct address_space *mapping = inode->i_mapping;
+	struct page *page;
+
+	if (!offset && !cache_only)
+		return 0;
+
+	if (cache_only) {
+		page = find_lock_page(mapping, index);
+		if (page && PageUptodate(page))
+			goto truncate_out;
+		f2fs_put_page(page, 1);
+		return 0;
+	}
+
+	page = f2fs_get_lock_data_page(inode, index, true);
+	if (IS_ERR(page))
+		return PTR_ERR(page) == -ENOENT ? 0 : PTR_ERR(page);
+truncate_out:
+	f2fs_wait_on_page_writeback(page, DATA, true, true);
+	zero_user(page, offset, PAGE_SIZE - offset);
+
+	/* An encrypted inode should have a key and truncate the last page. */
+	f2fs_bug_on(F2FS_I_SB(inode), cache_only && IS_ENCRYPTED(inode));
+	if (!cache_only)
+		set_page_dirty(page);
+	f2fs_put_page(page, 1);
+	return 0;
+}
+
+int f2fs_do_truncate_blocks(struct inode *inode, u64 from, bool lock)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	pgoff_t free_from;
+	int count = 0, err = 0;
+	struct page *ipage;
+	bool truncate_page = false;
+
+	trace_f2fs_truncate_blocks_enter(inode, from);
+
+	free_from = (pgoff_t)F2FS_BLK_ALIGN(from);
+
+	if (free_from >= max_file_blocks(inode))
+		goto free_partial;
+
+	if (lock)
+		f2fs_lock_op(sbi);
+
+	ipage = f2fs_get_node_page(sbi, inode->i_ino);
+	if (IS_ERR(ipage)) {
+		err = PTR_ERR(ipage);
+		goto out;
+	}
+
+	if (f2fs_has_inline_data(inode)) {
+		f2fs_truncate_inline_inode(inode, ipage, from);
+		f2fs_put_page(ipage, 1);
+		truncate_page = true;
+		goto out;
+	}
+
+	set_new_dnode(&dn, inode, ipage, NULL, 0);
+	err = f2fs_get_dnode_of_data(&dn, free_from, LOOKUP_NODE_RA);
+	if (err) {
+		if (err == -ENOENT)
+			goto free_next;
+		goto out;
+	}
+
+	count = ADDRS_PER_PAGE(dn.node_page, inode);
+
+	count -= dn.ofs_in_node;
+	f2fs_bug_on(sbi, count < 0);
+
+	if (dn.ofs_in_node || IS_INODE(dn.node_page)) {
+		f2fs_truncate_data_blocks_range(&dn, count);
+		free_from += count;
+	}
+
+	f2fs_put_dnode(&dn);
+free_next:
+	err = f2fs_truncate_inode_blocks(inode, free_from);
+out:
+	if (lock)
+		f2fs_unlock_op(sbi);
+free_partial:
+	/* lastly zero out the first data page */
+	if (!err)
+		err = truncate_partial_data_page(inode, from, truncate_page);
+
+	trace_f2fs_truncate_blocks_exit(inode, err);
+	return err;
+}
+
+int f2fs_truncate_blocks(struct inode *inode, u64 from, bool lock)
+{
+	u64 free_from = from;
+	int err;
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	/*
+	 * for compressed file, only support cluster size
+	 * aligned truncation.
+	 */
+	if (f2fs_compressed_file(inode))
+		free_from = round_up(from,
+				F2FS_I(inode)->i_cluster_size << PAGE_SHIFT);
+#endif
+
+	err = f2fs_do_truncate_blocks(inode, free_from, lock);
+	if (err)
+		return err;
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	/*
+	 * For compressed file, after release compress blocks, don't allow write
+	 * direct, but we should allow write direct after truncate to zero.
+	 */
+	if (f2fs_compressed_file(inode) && !free_from
+			&& is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
+		clear_inode_flag(inode, FI_COMPRESS_RELEASED);
+
+	if (from != free_from) {
+		err = f2fs_truncate_partial_cluster(inode, from, lock);
+		if (err)
+			return err;
+	}
+#endif
+
+	return 0;
+}
+
+int f2fs_truncate(struct inode *inode)
+{
+	int err;
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+
+	if (!(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
+				S_ISLNK(inode->i_mode)))
+		return 0;
+
+	trace_f2fs_truncate(inode);
+
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_TRUNCATE))
+		return -EIO;
+
+	err = f2fs_dquot_initialize(inode);
+	if (err)
+		return err;
+
+	/* we should check inline_data size */
+	if (!f2fs_may_inline_data(inode)) {
+		err = f2fs_convert_inline_inode(inode);
+		if (err)
+			return err;
+	}
+
+	err = f2fs_truncate_blocks(inode, i_size_read(inode), true);
+	if (err)
+		return err;
+
+	inode->i_mtime = inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, false);
+	return 0;
+}
+
+static bool f2fs_force_buffered_io(struct inode *inode, int rw)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	if (!fscrypt_dio_supported(inode))
+		return true;
+	if (fsverity_active(inode))
+		return true;
+	if (f2fs_compressed_file(inode))
+		return true;
+	if (f2fs_has_inline_data(inode))
+		return true;
+
+	/* disallow direct IO if any of devices has unaligned blksize */
+	if (f2fs_is_multi_device(sbi) && !sbi->aligned_blksize)
+		return true;
+	/*
+	 * for blkzoned device, fallback direct IO to buffered IO, so
+	 * all IOs can be serialized by log-structured write.
+	 */
+	if (f2fs_sb_has_blkzoned(sbi) && (rw == WRITE))
+		return true;
+	if (f2fs_lfs_mode(sbi) && rw == WRITE && F2FS_IO_ALIGNED(sbi))
+		return true;
+	if (is_sbi_flag_set(sbi, SBI_CP_DISABLED))
+		return true;
+
+	return false;
+}
+
+int f2fs_getattr(struct user_namespace *mnt_userns, const struct path *path,
+		 struct kstat *stat, u32 request_mask, unsigned int query_flags)
+{
+	struct inode *inode = d_inode(path->dentry);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_inode *ri = NULL;
+	unsigned int flags;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL;
+#endif
+
+	if (f2fs_has_extra_attr(inode) &&
+			f2fs_sb_has_inode_crtime(F2FS_I_SB(inode)) &&
+			F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime)) {
+		stat->result_mask |= STATX_BTIME;
+		stat->btime.tv_sec = fi->i_crtime.tv_sec;
+		stat->btime.tv_nsec = fi->i_crtime.tv_nsec;
+	}
+
+	/*
+	 * Return the DIO alignment restrictions if requested.  We only return
+	 * this information when requested, since on encrypted files it might
+	 * take a fair bit of work to get if the file wasn't opened recently.
+	 *
+	 * f2fs sometimes supports DIO reads but not DIO writes.  STATX_DIOALIGN
+	 * cannot represent that, so in that case we report no DIO support.
+	 */
+	if ((request_mask & STATX_DIOALIGN) && S_ISREG(inode->i_mode)) {
+		unsigned int bsize = i_blocksize(inode);
+
+		stat->result_mask |= STATX_DIOALIGN;
+		if (!f2fs_force_buffered_io(inode, WRITE)) {
+			stat->dio_mem_align = bsize;
+			stat->dio_offset_align = bsize;
+		}
+	}
+
+	flags = fi->i_flags;
+	if ((flags & F2FS_COMPR_FL) && may_compress)
+		stat->attributes |= STATX_ATTR_COMPRESSED;
+	if (flags & F2FS_APPEND_FL)
+		stat->attributes |= STATX_ATTR_APPEND;
+	if (IS_ENCRYPTED(inode))
+		stat->attributes |= STATX_ATTR_ENCRYPTED;
+	if (flags & F2FS_IMMUTABLE_FL)
+		stat->attributes |= STATX_ATTR_IMMUTABLE;
+	if (flags & F2FS_NODUMP_FL)
+		stat->attributes |= STATX_ATTR_NODUMP;
+	if (flags & F2FS_NOCOMP_FL)
+		stat->attributes |= STATX_ATTR_NOCOMPR;
+	if (IS_VERITY(inode))
+		stat->attributes |= STATX_ATTR_VERITY;
+
+	stat->attributes_mask |= (STATX_ATTR_COMPRESSED |
+				  STATX_ATTR_APPEND |
+				  STATX_ATTR_ENCRYPTED |
+				  STATX_ATTR_IMMUTABLE |
+				  STATX_ATTR_NODUMP |
+				  STATX_ATTR_NOCOMPR |
+				  STATX_ATTR_VERITY);
+
+	generic_fillattr(mnt_userns, inode, stat);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	inner = get_inner_inode(inode);
+	if (inner) {
+		f2fs_down_read(&F2FS_I(inner)->i_sem);
+		if (inner->i_nlink == 0)
+			f2fs_bug_on(F2FS_I_SB(inode), 1);
+		else
+			stat->blocks = inner->i_blocks / inner->i_nlink;
+		f2fs_up_read(&F2FS_I(inner)->i_sem);
+	}
+	put_inner_inode(inner);
+#endif
+
+	/* we need to show initial sectors used for inline_data/dentries */
+	if ((S_ISREG(inode->i_mode) && f2fs_has_inline_data(inode)) ||
+					f2fs_has_inline_dentry(inode))
+		stat->blocks += (stat->size + 511) >> 9;
+
+	return 0;
+}
+
+#ifdef CONFIG_F2FS_FS_POSIX_ACL
+static void __setattr_copy(struct user_namespace *mnt_userns,
+			   struct inode *inode, const struct iattr *attr)
+{
+	unsigned int ia_valid = attr->ia_valid;
+
+	i_uid_update(mnt_userns, attr, inode);
+	i_gid_update(mnt_userns, attr, inode);
+	if (ia_valid & ATTR_ATIME)
+		inode->i_atime = attr->ia_atime;
+	if (ia_valid & ATTR_MTIME)
+		inode->i_mtime = attr->ia_mtime;
+	if (ia_valid & ATTR_CTIME)
+		inode->i_ctime = attr->ia_ctime;
+	if (ia_valid & ATTR_MODE) {
+		umode_t mode = attr->ia_mode;
+		vfsgid_t vfsgid = i_gid_into_vfsgid(mnt_userns, inode);
+
+		if (!vfsgid_in_group_p(vfsgid) &&
+		    !capable_wrt_inode_uidgid(mnt_userns, inode, CAP_FSETID))
+			mode &= ~S_ISGID;
+		set_acl_inode(inode, mode);
+	}
+}
+#else
+#define __setattr_copy setattr_copy
+#endif
+
+int f2fs_setattr(struct user_namespace *mnt_userns, struct dentry *dentry,
+		 struct iattr *attr)
+{
+	struct inode *inode = d_inode(dentry);
+	int err;
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+
+	if (unlikely(IS_IMMUTABLE(inode)))
+		return -EPERM;
+
+	if (unlikely(IS_APPEND(inode) &&
+			(attr->ia_valid & (ATTR_MODE | ATTR_UID |
+				  ATTR_GID | ATTR_TIMES_SET))))
+		return -EPERM;
+
+	if ((attr->ia_valid & ATTR_SIZE) &&
+		!f2fs_is_compress_backend_ready(inode))
+		return -EOPNOTSUPP;
+
+	err = setattr_prepare(mnt_userns, dentry, attr);
+	if (err)
+		return err;
+
+	err = fscrypt_prepare_setattr(dentry, attr);
+	if (err)
+		return err;
+
+	err = fsverity_prepare_setattr(dentry, attr);
+	if (err)
+		return err;
+
+	if (is_quota_modification(mnt_userns, inode, attr)) {
+		err = f2fs_dquot_initialize(inode);
+		if (err)
+			return err;
+	}
+	if (i_uid_needs_update(mnt_userns, attr, inode) ||
+	    i_gid_needs_update(mnt_userns, attr, inode)) {
+		f2fs_lock_op(F2FS_I_SB(inode));
+		err = dquot_transfer(mnt_userns, inode, attr);
+		if (err) {
+			set_sbi_flag(F2FS_I_SB(inode),
+					SBI_QUOTA_NEED_REPAIR);
+			f2fs_unlock_op(F2FS_I_SB(inode));
+			return err;
+		}
+		/*
+		 * update uid/gid under lock_op(), so that dquot and inode can
+		 * be updated atomically.
+		 */
+		i_uid_update(mnt_userns, attr, inode);
+		i_gid_update(mnt_userns, attr, inode);
+		f2fs_mark_inode_dirty_sync(inode, true);
+		f2fs_unlock_op(F2FS_I_SB(inode));
+	}
+
+	if (attr->ia_valid & ATTR_SIZE) {
+		loff_t old_size = i_size_read(inode);
+
+		if (attr->ia_size > MAX_INLINE_DATA(inode)) {
+			/*
+			 * should convert inline inode before i_size_write to
+			 * keep smaller than inline_data size with inline flag.
+			 */
+			err = f2fs_convert_inline_inode(inode);
+			if (err)
+				return err;
+		}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+		if (attr->ia_size <= old_size && f2fs_compressed_file(inode) &&
+		    is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+			err = f2fs_reserve_compress_blocks(inode, NULL);
+			if (err < 0)
+				return err;
+		}
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+		/*
+		 * caller have hold inode lock
+		 */
+		if (is_inode_flag_set(inode, FI_SNAPSHOTED))
+			return -EOPNOTSUPP;
+		if (attr->ia_size <= old_size && f2fs_is_outer_inode(inode)) {
+			mark_file_modified(inode);
+			err = f2fs_revoke_deduped_inode(inode, __func__);
+			if (err)
+				return err;
+		}
+#endif
+
+		f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		filemap_invalidate_lock(inode->i_mapping);
+
+		truncate_setsize(inode, attr->ia_size);
+
+		if (attr->ia_size <= old_size)
+			err = f2fs_truncate(inode);
+		/*
+		 * do not trim all blocks after i_size if target size is
+		 * larger than i_size.
+		 */
+		filemap_invalidate_unlock(inode->i_mapping);
+		f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		if (err)
+			return err;
+
+		spin_lock(&F2FS_I(inode)->i_size_lock);
+		inode->i_mtime = inode->i_ctime = current_time(inode);
+		F2FS_I(inode)->last_disk_size = i_size_read(inode);
+		spin_unlock(&F2FS_I(inode)->i_size_lock);
+	}
+
+	__setattr_copy(mnt_userns, inode, attr);
+
+	if (attr->ia_valid & ATTR_MODE) {
+		err = posix_acl_chmod(mnt_userns, inode, f2fs_get_inode_mode(inode));
+
+		if (is_inode_flag_set(inode, FI_ACL_MODE)) {
+			if (!err)
+				inode->i_mode = F2FS_I(inode)->i_acl_mode;
+			clear_inode_flag(inode, FI_ACL_MODE);
+		}
+	}
+
+	/* file size may changed here */
+	f2fs_mark_inode_dirty_sync(inode, true);
+
+	/* inode change will produce dirty node pages flushed by checkpoint */
+	f2fs_balance_fs(F2FS_I_SB(inode), true);
+
+	return err;
+}
+
+const struct inode_operations f2fs_file_inode_operations = {
+	.getattr	= f2fs_getattr,
+	.setattr	= f2fs_setattr,
+	.get_acl	= f2fs_get_acl,
+	.set_acl	= f2fs_set_acl,
+	.listxattr	= f2fs_listxattr,
+	.fiemap		= f2fs_fiemap,
+	.fileattr_get	= f2fs_fileattr_get,
+	.fileattr_set	= f2fs_fileattr_set,
+};
+
+static int fill_zero(struct inode *inode, pgoff_t index,
+					loff_t start, loff_t len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct page *page;
+
+	if (!len)
+		return 0;
+
+	f2fs_balance_fs(sbi, true);
+
+	f2fs_lock_op(sbi);
+	page = f2fs_get_new_data_page(inode, NULL, index, false);
+	f2fs_unlock_op(sbi);
+
+	if (IS_ERR(page))
+		return PTR_ERR(page);
+
+	f2fs_wait_on_page_writeback(page, DATA, true, true);
+	zero_user(page, start, len);
+	set_page_dirty(page);
+	f2fs_put_page(page, 1);
+	return 0;
+}
+
+int f2fs_truncate_hole(struct inode *inode, pgoff_t pg_start, pgoff_t pg_end)
+{
+	int err;
+
+	while (pg_start < pg_end) {
+		struct dnode_of_data dn;
+		pgoff_t end_offset, count;
+
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		err = f2fs_get_dnode_of_data(&dn, pg_start, LOOKUP_NODE);
+		if (err) {
+			if (err == -ENOENT) {
+				pg_start = f2fs_get_next_page_offset(&dn,
+								pg_start);
+				continue;
+			}
+			return err;
+		}
+
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+		count = min(end_offset - dn.ofs_in_node, pg_end - pg_start);
+
+		f2fs_bug_on(F2FS_I_SB(inode), count == 0 || count > end_offset);
+
+		f2fs_truncate_data_blocks_range(&dn, count);
+		f2fs_put_dnode(&dn);
+
+		pg_start += count;
+	}
+	return 0;
+}
+
+static int f2fs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
+{
+	pgoff_t pg_start, pg_end;
+	loff_t off_start, off_end;
+	int ret;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		return ret;
+
+	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
+	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
+
+	off_start = offset & (PAGE_SIZE - 1);
+	off_end = (offset + len) & (PAGE_SIZE - 1);
+
+	if (pg_start == pg_end) {
+		ret = fill_zero(inode, pg_start, off_start,
+						off_end - off_start);
+		if (ret)
+			return ret;
+	} else {
+		if (off_start) {
+			ret = fill_zero(inode, pg_start++, off_start,
+						PAGE_SIZE - off_start);
+			if (ret)
+				return ret;
+		}
+		if (off_end) {
+			ret = fill_zero(inode, pg_end, 0, off_end);
+			if (ret)
+				return ret;
+		}
+
+		if (pg_start < pg_end) {
+			loff_t blk_start, blk_end;
+			struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+			f2fs_balance_fs(sbi, true);
+
+			blk_start = (loff_t)pg_start << PAGE_SHIFT;
+			blk_end = (loff_t)pg_end << PAGE_SHIFT;
+
+			f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+			filemap_invalidate_lock(inode->i_mapping);
+
+			truncate_pagecache_range(inode, blk_start, blk_end - 1);
+
+			f2fs_lock_op(sbi);
+			ret = f2fs_truncate_hole(inode, pg_start, pg_end);
+			f2fs_unlock_op(sbi);
+
+			filemap_invalidate_unlock(inode->i_mapping);
+			f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		}
+	}
+
+	return ret;
+}
+
+static int __read_out_blkaddrs(struct inode *inode, block_t *blkaddr,
+				int *do_replace, pgoff_t off, pgoff_t len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	int ret, done, i;
+
+next_dnode:
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	ret = f2fs_get_dnode_of_data(&dn, off, LOOKUP_NODE_RA);
+	if (ret && ret != -ENOENT) {
+		return ret;
+	} else if (ret == -ENOENT) {
+		if (dn.max_level == 0)
+			return -ENOENT;
+		done = min((pgoff_t)ADDRS_PER_BLOCK(inode) -
+						dn.ofs_in_node, len);
+		blkaddr += done;
+		do_replace += done;
+		goto next;
+	}
+
+	done = min((pgoff_t)ADDRS_PER_PAGE(dn.node_page, inode) -
+							dn.ofs_in_node, len);
+	for (i = 0; i < done; i++, blkaddr++, do_replace++, dn.ofs_in_node++) {
+		*blkaddr = f2fs_data_blkaddr(&dn);
+
+		if (__is_valid_data_blkaddr(*blkaddr) &&
+			!f2fs_is_valid_blkaddr(sbi, *blkaddr,
+					DATA_GENERIC_ENHANCE)) {
+			f2fs_put_dnode(&dn);
+			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
+			return -EFSCORRUPTED;
+		}
+
+		if (!f2fs_is_checkpointed_data(sbi, *blkaddr)) {
+
+			if (f2fs_lfs_mode(sbi)) {
+				f2fs_put_dnode(&dn);
+				return -EOPNOTSUPP;
+			}
+
+			/* do not invalidate this block address */
+			f2fs_update_data_blkaddr(&dn, NULL_ADDR);
+			*do_replace = 1;
+		}
+	}
+	f2fs_put_dnode(&dn);
+next:
+	len -= done;
+	off += done;
+	if (len)
+		goto next_dnode;
+	return 0;
+}
+
+static int __roll_back_blkaddrs(struct inode *inode, block_t *blkaddr,
+				int *do_replace, pgoff_t off, int len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	int ret, i;
+
+	for (i = 0; i < len; i++, do_replace++, blkaddr++) {
+		if (*do_replace == 0)
+			continue;
+
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		ret = f2fs_get_dnode_of_data(&dn, off + i, LOOKUP_NODE_RA);
+		if (ret) {
+			dec_valid_block_count(sbi, inode, 1);
+			f2fs_invalidate_blocks(sbi, *blkaddr);
+		} else {
+			f2fs_update_data_blkaddr(&dn, *blkaddr);
+		}
+		f2fs_put_dnode(&dn);
+	}
+	return 0;
+}
+
+static int __clone_blkaddrs(struct inode *src_inode, struct inode *dst_inode,
+			block_t *blkaddr, int *do_replace,
+			pgoff_t src, pgoff_t dst, pgoff_t len, bool full)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(src_inode);
+	pgoff_t i = 0;
+	int ret;
+
+	while (i < len) {
+		if (blkaddr[i] == NULL_ADDR && !full) {
+			i++;
+			continue;
+		}
+
+		if (do_replace[i] || blkaddr[i] == NULL_ADDR) {
+			struct dnode_of_data dn;
+			struct node_info ni;
+			size_t new_size;
+			pgoff_t ilen;
+
+			set_new_dnode(&dn, dst_inode, NULL, NULL, 0);
+			ret = f2fs_get_dnode_of_data(&dn, dst + i, ALLOC_NODE);
+			if (ret)
+				return ret;
+
+			ret = f2fs_get_node_info(sbi, dn.nid, &ni, false);
+			if (ret) {
+				f2fs_put_dnode(&dn);
+				return ret;
+			}
+
+			ilen = min((pgoff_t)
+				ADDRS_PER_PAGE(dn.node_page, dst_inode) -
+						dn.ofs_in_node, len - i);
+			do {
+				dn.data_blkaddr = f2fs_data_blkaddr(&dn);
+				f2fs_truncate_data_blocks_range(&dn, 1);
+
+				if (do_replace[i]) {
+					f2fs_i_blocks_write(src_inode,
+							1, false, false);
+					f2fs_i_blocks_write(dst_inode,
+							1, true, false);
+					f2fs_replace_block(sbi, &dn, dn.data_blkaddr,
+					blkaddr[i], ni.version, true, false);
+
+					do_replace[i] = 0;
+				}
+				dn.ofs_in_node++;
+				i++;
+				new_size = (loff_t)(dst + i) << PAGE_SHIFT;
+				if (dst_inode->i_size < new_size)
+					f2fs_i_size_write(dst_inode, new_size);
+			} while (--ilen && (do_replace[i] || blkaddr[i] == NULL_ADDR));
+
+			f2fs_put_dnode(&dn);
+		} else {
+			struct page *psrc, *pdst;
+
+			psrc = f2fs_get_lock_data_page(src_inode,
+							src + i, true);
+			if (IS_ERR(psrc))
+				return PTR_ERR(psrc);
+			pdst = f2fs_get_new_data_page(dst_inode, NULL, dst + i,
+								true);
+			if (IS_ERR(pdst)) {
+				f2fs_put_page(psrc, 1);
+				return PTR_ERR(pdst);
+			}
+			memcpy_page(pdst, 0, psrc, 0, PAGE_SIZE);
+			set_page_dirty(pdst);
+			f2fs_put_page(pdst, 1);
+			f2fs_put_page(psrc, 1);
+
+			ret = f2fs_truncate_hole(src_inode,
+						src + i, src + i + 1);
+			if (ret)
+				return ret;
+			i++;
+		}
+	}
+	return 0;
+}
+
+static int __exchange_data_block(struct inode *src_inode,
+			struct inode *dst_inode, pgoff_t src, pgoff_t dst,
+			pgoff_t len, bool full)
+{
+	block_t *src_blkaddr;
+	int *do_replace;
+	pgoff_t olen;
+	int ret;
+
+	while (len) {
+		olen = min((pgoff_t)4 * ADDRS_PER_BLOCK(src_inode), len);
+
+		src_blkaddr = f2fs_kvzalloc(F2FS_I_SB(src_inode),
+					array_size(olen, sizeof(block_t)),
+					GFP_NOFS);
+		if (!src_blkaddr)
+			return -ENOMEM;
+
+		do_replace = f2fs_kvzalloc(F2FS_I_SB(src_inode),
+					array_size(olen, sizeof(int)),
+					GFP_NOFS);
+		if (!do_replace) {
+			kvfree(src_blkaddr);
+			return -ENOMEM;
+		}
+
+		ret = __read_out_blkaddrs(src_inode, src_blkaddr,
+					do_replace, src, olen);
+		if (ret)
+			goto roll_back;
+
+		ret = __clone_blkaddrs(src_inode, dst_inode, src_blkaddr,
+					do_replace, src, dst, olen, full);
+		if (ret)
+			goto roll_back;
+
+		src += olen;
+		dst += olen;
+		len -= olen;
+
+		kvfree(src_blkaddr);
+		kvfree(do_replace);
+	}
+	return 0;
+
+roll_back:
+	__roll_back_blkaddrs(src_inode, src_blkaddr, do_replace, src, olen);
+	kvfree(src_blkaddr);
+	kvfree(do_replace);
+	return ret;
+}
+
+static int f2fs_do_collapse(struct inode *inode, loff_t offset, loff_t len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	pgoff_t nrpages = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	pgoff_t start = offset >> PAGE_SHIFT;
+	pgoff_t end = (offset + len) >> PAGE_SHIFT;
+	int ret;
+
+	f2fs_balance_fs(sbi, true);
+
+	/* avoid gc operation during block exchange */
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(inode->i_mapping);
+
+	f2fs_lock_op(sbi);
+	f2fs_drop_extent_tree(inode);
+	truncate_pagecache(inode, offset);
+	ret = __exchange_data_block(inode, inode, end, start, nrpages - end, true);
+	f2fs_unlock_op(sbi);
+
+	filemap_invalidate_unlock(inode->i_mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	return ret;
+}
+
+static int f2fs_collapse_range(struct inode *inode, loff_t offset, loff_t len)
+{
+	loff_t new_size;
+	int ret;
+
+	if (offset + len >= i_size_read(inode))
+		return -EINVAL;
+
+	/* collapse range should be aligned to block size of f2fs. */
+	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
+		return -EINVAL;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		return ret;
+
+	/* write out all dirty pages from offset */
+	ret = filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
+	if (ret)
+		return ret;
+
+	ret = f2fs_do_collapse(inode, offset, len);
+	if (ret)
+		return ret;
+
+	/* write out all moved pages, if possible */
+	filemap_invalidate_lock(inode->i_mapping);
+	filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
+	truncate_pagecache(inode, offset);
+
+	new_size = i_size_read(inode) - len;
+	ret = f2fs_truncate_blocks(inode, new_size, true);
+	filemap_invalidate_unlock(inode->i_mapping);
+	if (!ret)
+		f2fs_i_size_write(inode, new_size);
+	return ret;
+}
+
+static int f2fs_do_zero_range(struct dnode_of_data *dn, pgoff_t start,
+								pgoff_t end)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	pgoff_t index = start;
+	unsigned int ofs_in_node = dn->ofs_in_node;
+	blkcnt_t count = 0;
+	int ret;
+
+	for (; index < end; index++, dn->ofs_in_node++) {
+		if (f2fs_data_blkaddr(dn) == NULL_ADDR)
+			count++;
+	}
+
+	dn->ofs_in_node = ofs_in_node;
+	ret = f2fs_reserve_new_blocks(dn, count);
+	if (ret)
+		return ret;
+
+	dn->ofs_in_node = ofs_in_node;
+	for (index = start; index < end; index++, dn->ofs_in_node++) {
+		dn->data_blkaddr = f2fs_data_blkaddr(dn);
+		/*
+		 * f2fs_reserve_new_blocks will not guarantee entire block
+		 * allocation.
+		 */
+		if (dn->data_blkaddr == NULL_ADDR) {
+			ret = -ENOSPC;
+			break;
+		}
+
+		if (dn->data_blkaddr == NEW_ADDR)
+			continue;
+
+		if (!f2fs_is_valid_blkaddr(sbi, dn->data_blkaddr,
+					DATA_GENERIC_ENHANCE)) {
+			ret = -EFSCORRUPTED;
+			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
+			break;
+		}
+
+		f2fs_invalidate_blocks(sbi, dn->data_blkaddr);
+#ifdef CONFIG_F2FS_FS_SEQZONE
+		if (f2fs_seqzone_file(dn->inode))
+			dn->seqzone_index = NULL_ADDR;
+#endif
+		f2fs_set_data_blkaddr(dn, NEW_ADDR);
+	}
+
+	f2fs_update_read_extent_cache_range(dn, start, 0, index - start);
+	f2fs_update_age_extent_cache_range(dn, start, index - start);
+
+	return ret;
+}
+
+static int f2fs_zero_range(struct inode *inode, loff_t offset, loff_t len,
+								int mode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct address_space *mapping = inode->i_mapping;
+	pgoff_t index, pg_start, pg_end;
+	loff_t new_size = i_size_read(inode);
+	loff_t off_start, off_end;
+	int ret = 0;
+
+	ret = inode_newsize_ok(inode, (len + offset));
+	if (ret)
+		return ret;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		return ret;
+
+	ret = filemap_write_and_wait_range(mapping, offset, offset + len - 1);
+	if (ret)
+		return ret;
+
+	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
+	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
+
+	off_start = offset & (PAGE_SIZE - 1);
+	off_end = (offset + len) & (PAGE_SIZE - 1);
+
+	if (pg_start == pg_end) {
+		ret = fill_zero(inode, pg_start, off_start,
+						off_end - off_start);
+		if (ret)
+			return ret;
+
+		new_size = max_t(loff_t, new_size, offset + len);
+	} else {
+		if (off_start) {
+			ret = fill_zero(inode, pg_start++, off_start,
+						PAGE_SIZE - off_start);
+			if (ret)
+				return ret;
+
+			new_size = max_t(loff_t, new_size,
+					(loff_t)pg_start << PAGE_SHIFT);
+		}
+
+		for (index = pg_start; index < pg_end;) {
+			struct dnode_of_data dn;
+			unsigned int end_offset;
+			pgoff_t end;
+
+			f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+			filemap_invalidate_lock(mapping);
+
+			truncate_pagecache_range(inode,
+				(loff_t)index << PAGE_SHIFT,
+				((loff_t)pg_end << PAGE_SHIFT) - 1);
+
+			f2fs_lock_op(sbi);
+
+			set_new_dnode(&dn, inode, NULL, NULL, 0);
+			ret = f2fs_get_dnode_of_data(&dn, index, ALLOC_NODE);
+			if (ret) {
+				f2fs_unlock_op(sbi);
+				filemap_invalidate_unlock(mapping);
+				f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+				goto out;
+			}
+
+			end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+			end = min(pg_end, end_offset - dn.ofs_in_node + index);
+
+			ret = f2fs_do_zero_range(&dn, index, end);
+			f2fs_put_dnode(&dn);
+
+			f2fs_unlock_op(sbi);
+			filemap_invalidate_unlock(mapping);
+			f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
+			f2fs_balance_fs(sbi, dn.node_changed);
+
+			if (ret)
+				goto out;
+
+			index = end;
+			new_size = max_t(loff_t, new_size,
+					(loff_t)index << PAGE_SHIFT);
+		}
+
+		if (off_end) {
+			ret = fill_zero(inode, pg_end, 0, off_end);
+			if (ret)
+				goto out;
+
+			new_size = max_t(loff_t, new_size, offset + len);
+		}
+	}
+
+out:
+	if (new_size > i_size_read(inode)) {
+		if (mode & FALLOC_FL_KEEP_SIZE)
+			file_set_keep_isize(inode);
+		else
+			f2fs_i_size_write(inode, new_size);
+	}
+	return ret;
+}
+
+static int f2fs_insert_range(struct inode *inode, loff_t offset, loff_t len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct address_space *mapping = inode->i_mapping;
+	pgoff_t nr, pg_start, pg_end, delta, idx;
+	loff_t new_size;
+	int ret = 0;
+
+	new_size = i_size_read(inode) + len;
+	ret = inode_newsize_ok(inode, new_size);
+	if (ret)
+		return ret;
+
+	if (offset >= i_size_read(inode))
+		return -EINVAL;
+
+	/* insert range should be aligned to block size of f2fs. */
+	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
+		return -EINVAL;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		return ret;
+
+	f2fs_balance_fs(sbi, true);
+
+	filemap_invalidate_lock(mapping);
+	ret = f2fs_truncate_blocks(inode, i_size_read(inode), true);
+	filemap_invalidate_unlock(mapping);
+	if (ret)
+		return ret;
+
+	/* write out all dirty pages from offset */
+	ret = filemap_write_and_wait_range(mapping, offset, LLONG_MAX);
+	if (ret)
+		return ret;
+
+	pg_start = offset >> PAGE_SHIFT;
+	pg_end = (offset + len) >> PAGE_SHIFT;
+	delta = pg_end - pg_start;
+	idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+
+	/* avoid gc operation during block exchange */
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(mapping);
+	truncate_pagecache(inode, offset);
+
+	while (!ret && idx > pg_start) {
+		nr = idx - pg_start;
+		if (nr > delta)
+			nr = delta;
+		idx -= nr;
+
+		f2fs_lock_op(sbi);
+		f2fs_drop_extent_tree(inode);
+
+		ret = __exchange_data_block(inode, inode, idx,
+					idx + delta, nr, false);
+		f2fs_unlock_op(sbi);
+	}
+	filemap_invalidate_unlock(mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
+	/* write out all moved pages, if possible */
+	filemap_invalidate_lock(mapping);
+	filemap_write_and_wait_range(mapping, offset, LLONG_MAX);
+	truncate_pagecache(inode, offset);
+	filemap_invalidate_unlock(mapping);
+
+	if (!ret)
+		f2fs_i_size_write(inode, new_size);
+	return ret;
+}
+
+static int f2fs_expand_inode_data(struct inode *inode, loff_t offset,
+					loff_t len, int mode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_map_blocks map = { .m_next_pgofs = NULL,
+			.m_next_extent = NULL, .m_seg_type = NO_CHECK_TYPE,
+			.m_may_create = true };
+	struct f2fs_gc_control gc_control = { .victim_segno = NULL_SEGNO,
+			.init_gc_type = FG_GC,
+			.should_migrate_blocks = false,
+			.err_gc_skipped = true,
+			.nr_free_secs = 0 };
+	pgoff_t pg_start, pg_end;
+	loff_t new_size;
+	loff_t off_end;
+	block_t expanded = 0;
+	int err;
+
+	err = inode_newsize_ok(inode, (len + offset));
+	if (err)
+		return err;
+
+	err = f2fs_convert_inline_inode(inode);
+	if (err)
+		return err;
+
+	f2fs_balance_fs(sbi, true);
+
+	pg_start = ((unsigned long long)offset) >> PAGE_SHIFT;
+	pg_end = ((unsigned long long)offset + len) >> PAGE_SHIFT;
+	off_end = (offset + len) & (PAGE_SIZE - 1);
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		map.m_seqblk = pg_start;
+#endif
+	map.m_lblk = pg_start;
+	map.m_len = pg_end - pg_start;
+	if (off_end)
+		map.m_len++;
+
+	if (!map.m_len)
+		return 0;
+
+	if (f2fs_is_pinned_file(inode)) {
+		block_t sec_blks = CAP_BLKS_PER_SEC(sbi);
+		block_t sec_len = roundup(map.m_len, sec_blks);
+
+		map.m_len = sec_blks;
+next_alloc:
+		if (has_not_enough_free_secs(sbi, 0,
+			GET_SEC_FROM_SEG(sbi, overprovision_segments(sbi)))) {
+			f2fs_down_write(&sbi->gc_lock);
+			err = f2fs_gc(sbi, &gc_control);
+			if (err && err != -ENODATA)
+				goto out_err;
+		}
+
+		f2fs_down_write(&sbi->pin_sem);
+
+		f2fs_lock_op(sbi);
+		f2fs_allocate_new_section(sbi, CURSEG_COLD_DATA_PINNED, false);
+		f2fs_unlock_op(sbi);
+
+		map.m_seg_type = CURSEG_COLD_DATA_PINNED;
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRE_DIO);
+		file_dont_truncate(inode);
+
+		f2fs_up_write(&sbi->pin_sem);
+
+		expanded += map.m_len;
+		sec_len -= map.m_len;
+		map.m_lblk += map.m_len;
+		if (!err && sec_len)
+			goto next_alloc;
+
+		map.m_len = expanded;
+	} else {
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRE_AIO);
+		expanded = map.m_len;
+	}
+out_err:
+	if (err) {
+		pgoff_t last_off;
+
+		if (!expanded)
+			return err;
+
+		last_off = pg_start + expanded - 1;
+
+		/* update new size to the failed position */
+		new_size = (last_off == pg_end) ? offset + len :
+					(loff_t)(last_off + 1) << PAGE_SHIFT;
+	} else {
+		new_size = ((loff_t)pg_end << PAGE_SHIFT) + off_end;
+	}
+
+	if (new_size > i_size_read(inode)) {
+		if (mode & FALLOC_FL_KEEP_SIZE)
+			file_set_keep_isize(inode);
+		else
+			f2fs_i_size_write(inode, new_size);
+	}
+
+	return err;
+}
+
+static long f2fs_fallocate(struct file *file, int mode,
+				loff_t offset, loff_t len)
+{
+	struct inode *inode = file_inode(file);
+	long ret = 0;
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+	if (!f2fs_is_checkpoint_ready(F2FS_I_SB(inode)))
+		return -ENOSPC;
+	if (!f2fs_is_compress_backend_ready(inode))
+		return -EOPNOTSUPP;
+
+	/* f2fs only support ->fallocate for regular file */
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	if (IS_ENCRYPTED(inode) &&
+		(mode & (FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_INSERT_RANGE)))
+		return -EOPNOTSUPP;
+
+	/*
+	 * Pinned file should not support partial truncation since the block
+	 * can be used by applications.
+	 */
+	inode_lock(inode);
+	if ((f2fs_compressed_file(inode) || f2fs_is_pinned_file(inode)) &&
+		(mode & (FALLOC_FL_PUNCH_HOLE | FALLOC_FL_COLLAPSE_RANGE |
+			FALLOC_FL_ZERO_RANGE | FALLOC_FL_INSERT_RANGE))) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (f2fs_compressed_file(inode)) {
+			CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+			if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+				ret = f2fs_reserve_compress_blocks(inode, NULL);
+				if (ret < 0)
+					goto out;
+			}
+			ret = f2fs_decompress_inode(inode);
+			if (ret < 0)
+				goto out;
+		} else {
+			ret = -EOPNOTSUPP;
+			goto out;
+		}
+#else
+		ret = -EOPNOTSUPP;
+		goto out;
+#endif
+	}
+
+	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |
+			FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE |
+			FALLOC_FL_INSERT_RANGE)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
+
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode) && (mode &
+		(FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_INSERT_RANGE))) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
+#endif
+	ret = file_modified(file);
+	if (ret)
+		goto out;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode) &&
+			f2fs_revoke_deduped_inode(inode, __func__)) {
+		ret = -EIO;
+		goto out;
+	}
+#endif
+	if (mode & FALLOC_FL_PUNCH_HOLE) {
+		int i;
+		if (offset >= inode->i_size)
+			goto out;
+
+		f2fs_info(F2FS_I_SB(inode), "punch ino %lu isize %lld offset %lld len %lld\n",
+			inode->i_ino, i_size_read(inode), offset, len);
+		for (i = 0; i < BITS_TO_LONGS(FI_MAX); i++)
+			f2fs_info(F2FS_I_SB(inode), "flags[%d] %lx", i, F2FS_I(inode)->flags[i]);
+		ret = f2fs_punch_hole(inode, offset, len);
+	} else if (mode & FALLOC_FL_COLLAPSE_RANGE) {
+		ret = f2fs_collapse_range(inode, offset, len);
+	} else if (mode & FALLOC_FL_ZERO_RANGE) {
+		int i;
+		f2fs_info(F2FS_I_SB(inode), "zero ino %lu isize %lld offset %lld len %lld mode %d\n",
+			inode->i_ino, i_size_read(inode), offset, len, mode);
+		for (i = 0; i < BITS_TO_LONGS(FI_MAX); i++)
+			f2fs_info(F2FS_I_SB(inode), "flags[%d] %lx", i, F2FS_I(inode)->flags[i]);
+		ret = f2fs_zero_range(inode, offset, len, mode);
+	} else if (mode & FALLOC_FL_INSERT_RANGE) {
+		ret = f2fs_insert_range(inode, offset, len);
+	} else {
+		ret = f2fs_expand_inode_data(inode, offset, len, mode);
+	}
+
+#ifdef CONFIG_F2FS_APPBOOST
+	/* file change, update mtime */
+	inode->i_mtime = inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, false);
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+#else
+	if (!ret) {
+		inode->i_mtime = inode->i_ctime = current_time(inode);
+		f2fs_mark_inode_dirty_sync(inode, false);
+		f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	}
+#endif
+out:
+	inode_unlock(inode);
+
+	trace_f2fs_fallocate(inode, mode, offset, len, ret);
+	return ret;
+}
+
+static int f2fs_release_file(struct inode *inode, struct file *filp)
+{
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL;
+#endif
+	/*
+	 * f2fs_release_file is called at every close calls. So we should
+	 * not drop any inmemory pages by close called by other process.
+	 */
+	if (!(filp->f_mode & FMODE_WRITE) ||
+			atomic_read(&inode->i_writecount) != 1)
+		return 0;
+
+	inode_lock(inode);
+	f2fs_abort_atomic_write(inode, true);
+	inode_unlock(inode);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_outer_inode(inode)) {
+		inner = get_inner_inode(inode);
+		if (inner)
+			f2fs_release_file(inner, filp);
+		put_inner_inode(inner);
+	}
+#endif
+	return 0;
+}
+
+static int f2fs_file_flush(struct file *file, fl_owner_t id)
+{
+	struct inode *inode = file_inode(file);
+
+	/*
+	 * If the process doing a transaction is crashed, we should do
+	 * roll-back. Otherwise, other reader/write can see corrupted database
+	 * until all the writers close its file. Since this should be done
+	 * before dropping file lock, it needs to do in ->flush.
+	 */
+	if (F2FS_I(inode)->atomic_write_task == current &&
+				(current->flags & PF_EXITING)) {
+		inode_lock(inode);
+		f2fs_abort_atomic_write(inode, true);
+		inode_unlock(inode);
+	}
+
+	return 0;
+}
+
+static int f2fs_setflags_common(struct inode *inode, u32 iflags, u32 mask)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	u32 masked_flags = fi->i_flags & mask;
+
+	/* mask can be shrunk by flags_valid selector */
+	iflags &= mask;
+
+	/* Is it quota file? Do not allow user to mess with it */
+	if (IS_NOQUOTA(inode))
+		return -EPERM;
+
+	if ((iflags ^ masked_flags) & F2FS_CASEFOLD_FL) {
+		if (!f2fs_sb_has_casefold(F2FS_I_SB(inode)))
+			return -EOPNOTSUPP;
+		if (!f2fs_empty_dir(inode))
+			return -ENOTEMPTY;
+	}
+
+	if (iflags & (F2FS_COMPR_FL | F2FS_NOCOMP_FL)) {
+		if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
+			return -EOPNOTSUPP;
+		if ((iflags & F2FS_COMPR_FL) && (iflags & F2FS_NOCOMP_FL))
+			return -EINVAL;
+	}
+
+	if ((iflags ^ masked_flags) & F2FS_COMPR_FL) {
+		if (masked_flags & F2FS_COMPR_FL) {
+			if (!f2fs_disable_compressed_file(inode))
+				return -EINVAL;
+		} else {
+			/* try to convert inline_data to support compression */
+			int err = f2fs_convert_inline_inode(inode);
+			if (err)
+				return err;
+
+			f2fs_down_write(&F2FS_I(inode)->i_sem);
+			if (!f2fs_may_compress(inode) ||
+					(S_ISREG(inode->i_mode) &&
+					F2FS_HAS_BLOCKS(inode))) {
+				f2fs_up_write(&F2FS_I(inode)->i_sem);
+				return -EINVAL;
+			}
+#ifdef CONFIG_F2FS_SEQZONE
+			if (f2fs_seqzone_file(inode)) {
+				f2fs_up_write(&F2FS_I(inode)->i_sem);
+				return -EINVAL;
+			}
+#endif
+			if (!may_set_compr_fl) {
+				f2fs_up_write(&F2FS_I(inode)->i_sem);
+				return -EOPNOTSUPP;
+			}
+			err = set_compress_context(inode);
+			f2fs_up_write(&F2FS_I(inode)->i_sem);
+
+			if (err)
+				return err;
+		}
+	}
+
+	fi->i_flags = iflags | (fi->i_flags & ~mask);
+	f2fs_bug_on(F2FS_I_SB(inode), (fi->i_flags & F2FS_COMPR_FL) &&
+					(fi->i_flags & F2FS_NOCOMP_FL));
+
+	if (fi->i_flags & F2FS_PROJINHERIT_FL)
+		set_inode_flag(inode, FI_PROJ_INHERIT);
+	else
+		clear_inode_flag(inode, FI_PROJ_INHERIT);
+
+	inode->i_ctime = current_time(inode);
+	f2fs_set_inode_flags(inode);
+	f2fs_mark_inode_dirty_sync(inode, true);
+	return 0;
+}
+
+/* FS_IOC_[GS]ETFLAGS and FS_IOC_FS[GS]ETXATTR support */
+
+/*
+ * To make a new on-disk f2fs i_flag gettable via FS_IOC_GETFLAGS, add an entry
+ * for it to f2fs_fsflags_map[], and add its FS_*_FL equivalent to
+ * F2FS_GETTABLE_FS_FL.  To also make it settable via FS_IOC_SETFLAGS, also add
+ * its FS_*_FL equivalent to F2FS_SETTABLE_FS_FL.
+ *
+ * Translating flags to fsx_flags value used by FS_IOC_FSGETXATTR and
+ * FS_IOC_FSSETXATTR is done by the VFS.
+ */
+
+static const struct {
+	u32 iflag;
+	u32 fsflag;
+} f2fs_fsflags_map[] = {
+	{ F2FS_COMPR_FL,	FS_COMPR_FL },
+	{ F2FS_SYNC_FL,		FS_SYNC_FL },
+	{ F2FS_IMMUTABLE_FL,	FS_IMMUTABLE_FL },
+	{ F2FS_APPEND_FL,	FS_APPEND_FL },
+	{ F2FS_NODUMP_FL,	FS_NODUMP_FL },
+	{ F2FS_NOATIME_FL,	FS_NOATIME_FL },
+	{ F2FS_NOCOMP_FL,	FS_NOCOMP_FL },
+	{ F2FS_INDEX_FL,	FS_INDEX_FL },
+	{ F2FS_DIRSYNC_FL,	FS_DIRSYNC_FL },
+	{ F2FS_PROJINHERIT_FL,	FS_PROJINHERIT_FL },
+	{ F2FS_CASEFOLD_FL,	FS_CASEFOLD_FL },
+};
+
+#define F2FS_GETTABLE_FS_FL (		\
+		FS_COMPR_FL |		\
+		FS_SYNC_FL |		\
+		FS_IMMUTABLE_FL |	\
+		FS_APPEND_FL |		\
+		FS_NODUMP_FL |		\
+		FS_NOATIME_FL |		\
+		FS_NOCOMP_FL |		\
+		FS_INDEX_FL |		\
+		FS_DIRSYNC_FL |		\
+		FS_PROJINHERIT_FL |	\
+		FS_ENCRYPT_FL |		\
+		FS_INLINE_DATA_FL |	\
+		FS_NOCOW_FL |		\
+		FS_VERITY_FL |		\
+		FS_CASEFOLD_FL)
+
+#define F2FS_SETTABLE_FS_FL (		\
+		FS_COMPR_FL |		\
+		FS_SYNC_FL |		\
+		FS_IMMUTABLE_FL |	\
+		FS_APPEND_FL |		\
+		FS_NODUMP_FL |		\
+		FS_NOATIME_FL |		\
+		FS_NOCOMP_FL |		\
+		FS_DIRSYNC_FL |		\
+		FS_PROJINHERIT_FL |	\
+		FS_CASEFOLD_FL)
+
+/* Convert f2fs on-disk i_flags to FS_IOC_{GET,SET}FLAGS flags */
+static inline u32 f2fs_iflags_to_fsflags(u32 iflags)
+{
+	u32 fsflags = 0;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(f2fs_fsflags_map); i++)
+		if (iflags & f2fs_fsflags_map[i].iflag)
+			fsflags |= f2fs_fsflags_map[i].fsflag;
+
+	return fsflags;
+}
+
+/* Convert FS_IOC_{GET,SET}FLAGS flags to f2fs on-disk i_flags */
+static inline u32 f2fs_fsflags_to_iflags(u32 fsflags)
+{
+	u32 iflags = 0;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(f2fs_fsflags_map); i++)
+		if (fsflags & f2fs_fsflags_map[i].fsflag)
+			iflags |= f2fs_fsflags_map[i].iflag;
+
+	return iflags;
+}
+
+static int f2fs_ioc_getversion(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+
+	return put_user(inode->i_generation, (int __user *)arg);
+}
+
+static int f2fs_ioc_start_atomic_write(struct file *filp, bool truncate)
+{
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	loff_t isize;
+	int ret;
+
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
+
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
+
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	if (filp->f_flags & O_DIRECT)
+		return -EINVAL;
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	inode_lock(inode);
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compressed_file(inode)) {
+		CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+		if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+			ret = f2fs_reserve_compress_blocks(inode, NULL);
+			if (ret < 0)
+				goto out;
+		}
+		ret = f2fs_decompress_inode(inode);
+		if (ret < 0)
+			goto out;
+	}
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		ret = f2fs_revoke_deduped_inode(inode, __func__);
+		if (ret)
+			goto out;
+	}
+#endif
+
+	if (!f2fs_disable_compressed_file(inode)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (f2fs_is_atomic_file(inode))
+		goto out;
 
 	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		goto out;
+
+	f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+
+	/*
+	 * Should wait end_io to count F2FS_WB_CP_DATA correctly by
+	 * f2fs_is_atomic_file.
+	 */
+	if (get_dirty_pages(inode))
+		f2fs_warn(sbi, "Unexpected flush for atomic writes: ino=%lu, npages=%u",
+			  inode->i_ino, get_dirty_pages(inode));
+	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+	if (ret) {
+		f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+		goto out;
+	}
+
+	/* Check if the inode already has a COW inode */
+	if (fi->cow_inode == NULL) {
+		/* Create a COW inode for atomic write */
+		struct dentry *dentry = file_dentry(filp);
+		struct inode *dir = d_inode(dentry->d_parent);
+
+		ret = f2fs_get_tmpfile(mnt_userns, dir, &fi->cow_inode);
+		if (ret) {
+			f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+			goto out;
+		}
+
+		set_inode_flag(fi->cow_inode, FI_COW_FILE);
+		clear_inode_flag(fi->cow_inode, FI_INLINE_DATA);
+
+		/* Set the COW inode's atomic_inode to the atomic inode */
+		F2FS_I(fi->cow_inode)->atomic_inode = inode;
+	} else {
+		/* Reuse the already created COW inode */
+		f2fs_bug_on(sbi, get_dirty_pages(fi->cow_inode));
+
+		invalidate_mapping_pages(fi->cow_inode->i_mapping, 0, -1);
+
+		ret = f2fs_do_truncate_blocks(fi->cow_inode, 0, true);
+		if (ret) {
+			f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+			goto out;
+		}
+	}
+
+	f2fs_write_inode(inode, NULL);
+
+	stat_inc_atomic_inode(inode);
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi) &&
+		IS_ENCRYPTED(inode) && !f2fs_compressed_file(inode) &&
+		f2fs_seqzone_file(inode))
+		if (f2fs_inode_support_dedup(sbi, inode))
+			set_inode_flag(fi->cow_inode, FI_SEQZONE);
+#endif
+	set_inode_flag(inode, FI_ATOMIC_FILE);
+
+	isize = i_size_read(inode);
+	fi->original_i_size = isize;
+	if (truncate) {
+		set_inode_flag(inode, FI_ATOMIC_REPLACE);
+		truncate_inode_pages_final(inode->i_mapping);
+		f2fs_i_size_write(inode, 0);
+		isize = 0;
+	}
+	f2fs_i_size_write(fi->cow_inode, isize);
+
+	f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+
+	f2fs_update_time(sbi, REQ_TIME);
+	fi->atomic_write_task = current;
+	stat_update_max_atomic_write(inode);
+	fi->atomic_write_cnt = 0;
+out:
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
+	return ret;
+}
+
+static int f2fs_ioc_commit_atomic_write(struct file *filp)
+{
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	int ret;
+
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
+
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	f2fs_balance_fs(F2FS_I_SB(inode), true);
+
+	inode_lock(inode);
+
+	if (f2fs_is_atomic_file(inode)) {
+		ret = f2fs_commit_atomic_write(inode);
+		if (!ret)
+			ret = f2fs_do_sync_file(filp, 0, LLONG_MAX, 0, true);
+
+		f2fs_abort_atomic_write(inode, ret);
+	} else {
+		ret = f2fs_do_sync_file(filp, 0, LLONG_MAX, 1, false);
+	}
+
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
+	return ret;
+}
+
+static int f2fs_ioc_abort_atomic_write(struct file *filp)
+{
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	int ret;
+
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
+
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	inode_lock(inode);
+
+	f2fs_abort_atomic_write(inode, true);
+
+	inode_unlock(inode);
+
+	mnt_drop_write_file(filp);
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	return ret;
+}
+
+static int f2fs_ioc_shutdown(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct super_block *sb = sbi->sb;
+	__u32 in;
+	int ret = 0;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (get_user(in, (__u32 __user *)arg))
+		return -EFAULT;
+
+	if (in != F2FS_GOING_DOWN_FULLSYNC) {
+		ret = mnt_want_write_file(filp);
+		if (ret) {
+			if (ret == -EROFS) {
+				ret = 0;
+				f2fs_stop_checkpoint(sbi, false,
+						STOP_CP_REASON_SHUTDOWN);
+				set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+				trace_f2fs_shutdown(sbi, in, ret);
+			}
+			return ret;
+		}
+	}
+
+	switch (in) {
+	case F2FS_GOING_DOWN_FULLSYNC:
+		ret = freeze_bdev(sb->s_bdev);
+		if (ret)
+			goto out;
+		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
+		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+		thaw_bdev(sb->s_bdev);
+		break;
+	case F2FS_GOING_DOWN_METASYNC:
+		/* do checkpoint only */
+		ret = f2fs_sync_fs(sb, 1);
+		if (ret) {
+			if (ret == -EIO)
+				ret = 0;
+			goto out;
+		}
+		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
+		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+		break;
+	case F2FS_GOING_DOWN_NOSYNC:
+		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
+		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+		break;
+	case F2FS_GOING_DOWN_METAFLUSH:
+		f2fs_sync_meta_pages(sbi, META, LONG_MAX, FS_META_IO);
+		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
+		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+		break;
+	case F2FS_GOING_DOWN_NEED_FSCK:
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		set_sbi_flag(sbi, SBI_CP_DISABLED_QUICK);
+		set_sbi_flag(sbi, SBI_IS_DIRTY);
+		/* do checkpoint only */
+		ret = f2fs_sync_fs(sb, 1);
+		if (ret == -EIO)
+			ret = 0;
+		goto out;
+	default:
+		ret = -EINVAL;
+		goto out;
+	}
+
+	f2fs_stop_gc_thread(sbi);
+	f2fs_stop_discard_thread(sbi);
+
+	f2fs_drop_discard_cmd(sbi);
+	clear_opt(sbi, DISCARD);
+
+	f2fs_update_time(sbi, REQ_TIME);
+out:
+	if (in != F2FS_GOING_DOWN_FULLSYNC)
+		mnt_drop_write_file(filp);
+
+	trace_f2fs_shutdown(sbi, in, ret);
+
+	return ret;
+}
+
+static int f2fs_ioc_fitrim(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct super_block *sb = inode->i_sb;
+	struct fstrim_range range;
+	int ret;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (!f2fs_hw_support_discard(F2FS_SB(sb)))
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&range, (struct fstrim_range __user *)arg,
+				sizeof(range)))
+		return -EFAULT;
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	range.minlen = max((unsigned int)range.minlen,
+			   bdev_discard_granularity(sb->s_bdev));
+	ret = f2fs_trim_fs(F2FS_SB(sb), &range);
+	mnt_drop_write_file(filp);
+	if (ret < 0)
+		return ret;
+
+	if (copy_to_user((struct fstrim_range __user *)arg, &range,
+				sizeof(range)))
+		return -EFAULT;
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	return 0;
+}
+
+static bool uuid_is_nonzero(__u8 u[16])
+{
+	int i;
+
+	for (i = 0; i < 16; i++)
+		if (u[i])
+			return true;
+	return false;
+}
+
+static int f2fs_ioc_set_encryption_policy(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(inode)))
+		return -EOPNOTSUPP;
+
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+
+	return fscrypt_ioctl_set_policy(filp, (const void __user *)arg);
+}
+
+static int f2fs_ioc_get_encryption_policy(struct file *filp, unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+	return fscrypt_ioctl_get_policy(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_get_encryption_pwsalt(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	u8 encrypt_pw_salt[16];
+	int err;
+
+	if (!f2fs_sb_has_encrypt(sbi))
+		return -EOPNOTSUPP;
+
+	err = mnt_want_write_file(filp);
+	if (err)
+		return err;
+
+	f2fs_down_write(&sbi->sb_lock);
+
+	if (uuid_is_nonzero(sbi->raw_super->encrypt_pw_salt))
+		goto got_it;
+
+	/* update superblock with uuid */
+	generate_random_uuid(sbi->raw_super->encrypt_pw_salt);
+
+	err = f2fs_commit_super(sbi, false);
+	if (err) {
+		/* undo new data */
+		memset(sbi->raw_super->encrypt_pw_salt, 0, 16);
+		goto out_err;
+	}
+got_it:
+	memcpy(encrypt_pw_salt, sbi->raw_super->encrypt_pw_salt, 16);
+out_err:
+	f2fs_up_write(&sbi->sb_lock);
+	mnt_drop_write_file(filp);
+
+	if (!err && copy_to_user((__u8 __user *)arg, encrypt_pw_salt, 16))
+		err = -EFAULT;
+
+	return err;
+}
+
+static int f2fs_ioc_get_encryption_policy_ex(struct file *filp,
+					     unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_get_policy_ex(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_add_encryption_key(struct file *filp, unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_add_key(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_remove_encryption_key(struct file *filp, unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_remove_key(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_remove_encryption_key_all_users(struct file *filp,
+						    unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_remove_key_all_users(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_get_encryption_key_status(struct file *filp,
+					      unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_get_key_status(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_get_encryption_nonce(struct file *filp, unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_get_nonce(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_gc(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_gc_control gc_control = { .victim_segno = NULL_SEGNO,
+			.no_bg_gc = false,
+			.should_migrate_blocks = false,
+			.nr_free_secs = 0 };
+	__u32 sync;
+	int ret;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (get_user(sync, (__u32 __user *)arg))
+		return -EFAULT;
+
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
+
+	ret = mnt_want_write_file(filp);
 	if (ret)
 		return ret;
 
-	f2fs_balance_fs(sbi, true);
+	if (!sync) {
+		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
+			ret = -EBUSY;
+			goto out;
+		}
+	} else {
+		f2fs_down_write(&sbi->gc_lock);
+	}
 
-	filemap_invalidate_lock(mapping);
-	ret = f2fs_truncate_blocks(inode, i_size_read(inode), true);
-	filemap_invalidate_unlock(mapping);
+	gc_control.init_gc_type = sync ? FG_GC : BG_GC;
+	gc_control.err_gc_skipped = sync;
+	ret = f2fs_gc(sbi, &gc_control);
+out:
+	mnt_drop_write_file(filp);
+	return ret;
+}
+
+static int __f2fs_ioc_gc_range(struct file *filp, struct f2fs_gc_range *range)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));
+	struct f2fs_gc_control gc_control = {
+			.init_gc_type = range->sync ? FG_GC : BG_GC,
+			.no_bg_gc = false,
+			.should_migrate_blocks = false,
+			.err_gc_skipped = range->sync,
+			.nr_free_secs = 0 };
+	u64 end;
+	int ret;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
+
+	end = range->start + range->len;
+	if (end < range->start || range->start < MAIN_BLKADDR(sbi) ||
+					end >= MAX_BLKADDR(sbi))
+		return -EINVAL;
+
+	ret = mnt_want_write_file(filp);
 	if (ret)
 		return ret;
 
-	/* write out all dirty pages from offset */
-	ret = filemap_write_and_wait_range(mapping, offset, LLONG_MAX);
+do_more:
+	if (!range->sync) {
+		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
+			ret = -EBUSY;
+			goto out;
+		}
+	} else {
+		f2fs_down_write(&sbi->gc_lock);
+	}
+
+	gc_control.victim_segno = GET_SEGNO(sbi, range->start);
+	ret = f2fs_gc(sbi, &gc_control);
+	if (ret) {
+		if (ret == -EBUSY)
+			ret = -EAGAIN;
+		goto out;
+	}
+	range->start += CAP_BLKS_PER_SEC(sbi);
+	if (range->start <= end)
+		goto do_more;
+out:
+	mnt_drop_write_file(filp);
+	return ret;
+}
+
+static int f2fs_ioc_gc_range(struct file *filp, unsigned long arg)
+{
+	struct f2fs_gc_range range;
+
+	if (copy_from_user(&range, (struct f2fs_gc_range __user *)arg,
+							sizeof(range)))
+		return -EFAULT;
+	return __f2fs_ioc_gc_range(filp, &range);
+}
+
+static int f2fs_ioc_write_checkpoint(struct file *filp)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	int ret;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
+
+	if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {
+		f2fs_info(sbi, "Skipping Checkpoint. Checkpoints currently disabled.");
+		return -EINVAL;
+	}
+
+	ret = mnt_want_write_file(filp);
 	if (ret)
 		return ret;
 
-	pg_start = offset >> PAGE_SHIFT;
-	pg_end = (offset + len) >> PAGE_SHIFT;
-	delta = pg_end - pg_start;
-	idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	ret = f2fs_sync_fs(sbi->sb, 1);
 
-	/* avoid gc operation during block exchange */
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(mapping);
-	truncate_pagecache(inode, offset);
+	mnt_drop_write_file(filp);
+	return ret;
+}
 
-	while (!ret && idx > pg_start) {
-		nr = idx - pg_start;
-		if (nr > delta)
-			nr = delta;
-		idx -= nr;
+static int f2fs_defragment_range(struct f2fs_sb_info *sbi,
+					struct file *filp,
+					struct f2fs_defragment *range)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_map_blocks map = { .m_next_extent = NULL,
+					.m_seg_type = NO_CHECK_TYPE,
+					.m_may_create = false };
+	struct extent_info ei = {};
+	pgoff_t pg_start, pg_end, next_pgofs;
+	unsigned int blk_per_seg = sbi->blocks_per_seg;
+	unsigned int total = 0, sec_num;
+	block_t blk_end = 0;
+	bool fragmented = false;
+	int err;
 
-		f2fs_lock_op(sbi);
-		f2fs_drop_extent_tree(inode);
+	pg_start = range->start >> PAGE_SHIFT;
+	pg_end = (range->start + range->len) >> PAGE_SHIFT;
 
-		ret = __exchange_data_block(inode, inode, idx,
-					idx + delta, nr, false);
-		f2fs_unlock_op(sbi);
+	f2fs_balance_fs(sbi, true);
+
+	inode_lock(inode);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		err = f2fs_revoke_deduped_inode(inode, __func__);
+		if (err)
+			goto unlock_out;
 	}
-	filemap_invalidate_unlock(mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+#endif
 
-	/* write out all moved pages, if possible */
-	filemap_invalidate_lock(mapping);
-	filemap_write_and_wait_range(mapping, offset, LLONG_MAX);
-	truncate_pagecache(inode, offset);
-	filemap_invalidate_unlock(mapping);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compressed_file(inode)) {
+		CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+		if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+			err = f2fs_reserve_compress_blocks(inode, NULL);
+			if (err < 0)
+				goto unlock_out;
+		}
+		err = f2fs_decompress_inode(inode);
+		if (err < 0)
+			goto unlock_out;
+	}
+#endif
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED) ||
+		f2fs_is_atomic_file(inode)) {
+		err = -EINVAL;
+		goto unlock_out;
+	}
+
+	/* if in-place-update policy is enabled, don't waste time here */
+	set_inode_flag(inode, FI_OPU_WRITE);
+	if (f2fs_should_update_inplace(inode, NULL)) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	/* writeback all dirty pages in the range */
+	err = filemap_write_and_wait_range(inode->i_mapping, range->start,
+						range->start + range->len - 1);
+	if (err)
+		goto out;
+
+	/*
+	 * lookup mapping info in extent cache, skip defragmenting if physical
+	 * block addresses are continuous.
+	 */
+	if (f2fs_lookup_read_extent_cache(inode, pg_start, &ei)) {
+		if ((pgoff_t)ei.fofs + ei.len >= pg_end)
+			goto out;
+	}
+
+	map.m_lblk = pg_start;
+	map.m_next_pgofs = &next_pgofs;
+
+	/*
+	 * lookup mapping info in dnode page cache, skip defragmenting if all
+	 * physical block addresses are continuous even if there are hole(s)
+	 * in logical blocks.
+	 */
+	while (map.m_lblk < pg_end) {
+		map.m_len = pg_end - map.m_lblk;
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT);
+		if (err)
+			goto out;
+
+		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
+			map.m_lblk = next_pgofs;
+			continue;
+		}
 
-	if (!ret)
-		f2fs_i_size_write(inode, new_size);
-	return ret;
-}
+		if (blk_end && blk_end != map.m_pblk)
+			fragmented = true;
 
-static int f2fs_expand_inode_data(struct inode *inode, loff_t offset,
-					loff_t len, int mode)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_map_blocks map = { .m_next_pgofs = NULL,
-			.m_next_extent = NULL, .m_seg_type = NO_CHECK_TYPE,
-			.m_may_create = true };
-	struct f2fs_gc_control gc_control = { .victim_segno = NULL_SEGNO,
-			.init_gc_type = FG_GC,
-			.should_migrate_blocks = false,
-			.err_gc_skipped = true,
-			.nr_free_secs = 0 };
-	pgoff_t pg_start, pg_end;
-	loff_t new_size;
-	loff_t off_end;
-	block_t expanded = 0;
-	int err;
+		/* record total count of block that we're going to move */
+		total += map.m_len;
 
-	err = inode_newsize_ok(inode, (len + offset));
-	if (err)
-		return err;
+		blk_end = map.m_pblk + map.m_len;
 
-	err = f2fs_convert_inline_inode(inode);
-	if (err)
-		return err;
+		map.m_lblk += map.m_len;
+	}
 
-	f2fs_balance_fs(sbi, true);
+	if (!fragmented) {
+		total = 0;
+		goto out;
+	}
 
-	pg_start = ((unsigned long long)offset) >> PAGE_SHIFT;
-	pg_end = ((unsigned long long)offset + len) >> PAGE_SHIFT;
-	off_end = (offset + len) & (PAGE_SIZE - 1);
+	sec_num = DIV_ROUND_UP(total, CAP_BLKS_PER_SEC(sbi));
+
+	/*
+	 * make sure there are enough free section for LFS allocation, this can
+	 * avoid defragment running in SSR mode when free section are allocated
+	 * intensively
+	 */
+	if (has_not_enough_free_secs(sbi, 0, sec_num)) {
+		err = -EAGAIN;
+		goto out;
+	}
 
 	map.m_lblk = pg_start;
 	map.m_len = pg_end - pg_start;
-	if (off_end)
-		map.m_len++;
-
-	if (!map.m_len)
-		return 0;
-
-	if (f2fs_is_pinned_file(inode)) {
-		block_t sec_blks = CAP_BLKS_PER_SEC(sbi);
-		block_t sec_len = roundup(map.m_len, sec_blks);
+	total = 0;
 
-		map.m_len = sec_blks;
-next_alloc:
-		f2fs_down_write(&sbi->pin_sem);
+	while (map.m_lblk < pg_end) {
+		pgoff_t idx;
+		int cnt = 0;
 
-		if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {
-			if (has_not_enough_free_secs(sbi, 0, 0)) {
-				f2fs_up_write(&sbi->pin_sem);
-				err = -ENOSPC;
-				f2fs_warn(sbi,
-					"ino:%lu, start:%lu, end:%lu, need to trigger GC to "
-					"reclaim enough free segment when checkpoint is enabled",
-					inode->i_ino, pg_start, pg_end);
-				goto out_err;
-			}
-		}
+do_map:
+		map.m_len = pg_end - map.m_lblk;
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT);
+		if (err)
+			goto clear_out;
 
-		if (has_not_enough_free_secs(sbi, 0,
-			GET_SEC_FROM_SEG(sbi, overprovision_segments(sbi)))) {
-			f2fs_down_write(&sbi->gc_lock);
-			err = f2fs_gc(sbi, &gc_control);
-			if (err && err != -ENODATA) {
-				f2fs_up_write(&sbi->pin_sem);
-				goto out_err;
-			}
+		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
+			map.m_lblk = next_pgofs;
+			goto check;
 		}
 
-		f2fs_lock_op(sbi);
-		f2fs_allocate_new_section(sbi, CURSEG_COLD_DATA_PINNED, false);
-		f2fs_unlock_op(sbi);
+		set_inode_flag(inode, FI_SKIP_WRITES);
 
-		map.m_seg_type = CURSEG_COLD_DATA_PINNED;
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRE_DIO);
-		file_dont_truncate(inode);
+		idx = map.m_lblk;
+		while (idx < map.m_lblk + map.m_len && cnt < blk_per_seg) {
+			struct page *page;
 
-		f2fs_up_write(&sbi->pin_sem);
+			page = f2fs_get_lock_data_page(inode, idx, true);
+			if (IS_ERR(page)) {
+				err = PTR_ERR(page);
+				goto clear_out;
+			}
 
-		expanded += map.m_len;
-		sec_len -= map.m_len;
-		map.m_lblk += map.m_len;
-		if (!err && sec_len)
-			goto next_alloc;
+			f2fs_wait_on_page_writeback(page, DATA, true, true);
 
-		map.m_len = expanded;
-	} else {
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRE_AIO);
-		expanded = map.m_len;
-	}
-out_err:
-	if (err) {
-		pgoff_t last_off;
+			set_page_dirty(page);
+			set_page_private_gcing(page);
+			f2fs_put_page(page, 1);
 
-		if (!expanded)
-			return err;
+			idx++;
+			cnt++;
+			total++;
+		}
 
-		last_off = pg_start + expanded - 1;
+		map.m_lblk = idx;
+check:
+		if (map.m_lblk < pg_end && cnt < blk_per_seg)
+			goto do_map;
 
-		/* update new size to the failed position */
-		new_size = (last_off == pg_end) ? offset + len :
-					(loff_t)(last_off + 1) << PAGE_SHIFT;
-	} else {
-		new_size = ((loff_t)pg_end << PAGE_SHIFT) + off_end;
-	}
+		clear_inode_flag(inode, FI_SKIP_WRITES);
 
-	if (new_size > i_size_read(inode)) {
-		if (mode & FALLOC_FL_KEEP_SIZE)
-			file_set_keep_isize(inode);
-		else
-			f2fs_i_size_write(inode, new_size);
+		err = filemap_fdatawrite(inode->i_mapping);
+		if (err)
+			goto out;
 	}
-
+clear_out:
+	clear_inode_flag(inode, FI_SKIP_WRITES);
+out:
+	clear_inode_flag(inode, FI_OPU_WRITE);
+unlock_out:
+	inode_unlock(inode);
+	if (!err)
+		range->len = (u64)total << PAGE_SHIFT;
 	return err;
 }
 
-static long f2fs_fallocate(struct file *file, int mode,
-				loff_t offset, loff_t len)
+static int f2fs_ioc_defragment(struct file *filp, unsigned long arg)
 {
-	struct inode *inode = file_inode(file);
-	long ret = 0;
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_defragment range;
+	int err;
 
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
-		return -EIO;
-	if (!f2fs_is_checkpoint_ready(F2FS_I_SB(inode)))
-		return -ENOSPC;
-	if (!f2fs_is_compress_backend_ready(inode))
-		return -EOPNOTSUPP;
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
-	/* f2fs only support ->fallocate for regular file */
-	if (!S_ISREG(inode->i_mode))
+	if (!S_ISREG(inode->i_mode) || f2fs_is_atomic_file(inode))
 		return -EINVAL;
 
-	if (IS_ENCRYPTED(inode) &&
-		(mode & (FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_INSERT_RANGE)))
-		return -EOPNOTSUPP;
-
-	/*
-	 * Pinned file should not support partial truncation since the block
-	 * can be used by applications.
-	 */
-	if ((f2fs_compressed_file(inode) || f2fs_is_pinned_file(inode)) &&
-		(mode & (FALLOC_FL_PUNCH_HOLE | FALLOC_FL_COLLAPSE_RANGE |
-			FALLOC_FL_ZERO_RANGE | FALLOC_FL_INSERT_RANGE)))
-		return -EOPNOTSUPP;
-
-	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |
-			FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE |
-			FALLOC_FL_INSERT_RANGE))
-		return -EOPNOTSUPP;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	inode_lock(inode);
+	if (copy_from_user(&range, (struct f2fs_defragment __user *)arg,
+							sizeof(range)))
+		return -EFAULT;
 
-	ret = file_modified(file);
-	if (ret)
-		goto out;
+	/* verify alignment of offset & size */
+	if (range.start & (F2FS_BLKSIZE - 1) || range.len & (F2FS_BLKSIZE - 1))
+		return -EINVAL;
 
-	/*
-	 * wait for inflight dio, blocks should be removed after IO
-	 * completion.
-	 */
-	inode_dio_wait(inode);
+	if (unlikely((range.start + range.len) >> PAGE_SHIFT >
+					max_file_blocks(inode)))
+		return -EINVAL;
 
-	if (mode & FALLOC_FL_PUNCH_HOLE) {
-		if (offset >= inode->i_size)
-			goto out;
+	err = mnt_want_write_file(filp);
+	if (err)
+		return err;
 
-		ret = f2fs_punch_hole(inode, offset, len);
-	} else if (mode & FALLOC_FL_COLLAPSE_RANGE) {
-		ret = f2fs_collapse_range(inode, offset, len);
-	} else if (mode & FALLOC_FL_ZERO_RANGE) {
-		ret = f2fs_zero_range(inode, offset, len, mode);
-	} else if (mode & FALLOC_FL_INSERT_RANGE) {
-		ret = f2fs_insert_range(inode, offset, len);
-	} else {
-		ret = f2fs_expand_inode_data(inode, offset, len, mode);
-	}
+	err = f2fs_defragment_range(sbi, filp, &range);
+	mnt_drop_write_file(filp);
 
-	if (!ret) {
-		inode->i_mtime = inode->i_ctime = current_time(inode);
-		f2fs_mark_inode_dirty_sync(inode, false);
-		f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
-	}
+	f2fs_update_time(sbi, REQ_TIME);
+	if (err < 0)
+		return err;
 
-out:
-	inode_unlock(inode);
+	if (copy_to_user((struct f2fs_defragment __user *)arg, &range,
+							sizeof(range)))
+		return -EFAULT;
 
-	trace_f2fs_fallocate(inode, mode, offset, len, ret);
-	return ret;
+	return 0;
 }
 
-static int f2fs_release_file(struct inode *inode, struct file *filp)
+static int f2fs_move_file_range(struct file *file_in, loff_t pos_in,
+			struct file *file_out, loff_t pos_out, size_t len)
 {
-	/*
-	 * f2fs_release_file is called at every close calls. So we should
-	 * not drop any inmemory pages by close called by other process.
-	 */
-	if (!(filp->f_mode & FMODE_WRITE) ||
-			atomic_read(&inode->i_writecount) != 1)
-		return 0;
+	struct inode *src = file_inode(file_in);
+	struct inode *dst = file_inode(file_out);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(src);
+	size_t olen = len, dst_max_i_size = 0;
+	size_t dst_osize;
+	int ret;
 
-	inode_lock(inode);
-	f2fs_abort_atomic_write(inode, true);
-	inode_unlock(inode);
+	if (file_in->f_path.mnt != file_out->f_path.mnt ||
+				src->i_sb != dst->i_sb)
+		return -EXDEV;
 
-	return 0;
-}
+	if (unlikely(f2fs_readonly(src->i_sb)))
+		return -EROFS;
 
-static int f2fs_file_flush(struct file *file, fl_owner_t id)
-{
-	struct inode *inode = file_inode(file);
+	if (!S_ISREG(src->i_mode) || !S_ISREG(dst->i_mode))
+		return -EINVAL;
 
-	/*
-	 * If the process doing a transaction is crashed, we should do
-	 * roll-back. Otherwise, other reader/write can see corrupted database
-	 * until all the writers close its file. Since this should be done
-	 * before dropping file lock, it needs to do in ->flush.
-	 */
-	if (F2FS_I(inode)->atomic_write_task == current &&
-				(current->flags & PF_EXITING)) {
-		inode_lock(inode);
-		f2fs_abort_atomic_write(inode, true);
-		inode_unlock(inode);
+	if (IS_ENCRYPTED(src) || IS_ENCRYPTED(dst))
+		return -EOPNOTSUPP;
+
+	if (pos_out < 0 || pos_in < 0)
+		return -EINVAL;
+
+	if (src == dst) {
+		if (pos_in == pos_out)
+			return 0;
+		if (pos_out > pos_in && pos_out < pos_in + len)
+			return -EINVAL;
 	}
 
-	return 0;
-}
+	inode_lock(src);
+	if (src != dst) {
+		ret = -EBUSY;
+		if (!inode_trylock(dst))
+			goto out;
+	}
 
-static int f2fs_setflags_common(struct inode *inode, u32 iflags, u32 mask)
-{
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	u32 masked_flags = fi->i_flags & mask;
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(src) || f2fs_seqzone_file(dst)) {
+		ret = -EOPNOTSUPP;
+		goto out_unlock;
+	}
+#endif
 
-	/* mask can be shrunk by flags_valid selector */
-	iflags &= mask;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(src);
+	if (f2fs_is_outer_inode(src)) {
+		ret = f2fs_revoke_deduped_inode(src, __func__);
+		if (ret)
+			goto out_unlock;
+	}
 
-	/* Is it quota file? Do not allow user to mess with it */
-	if (IS_NOQUOTA(inode))
-		return -EPERM;
+	mark_file_modified(dst);
+	if (f2fs_is_outer_inode(dst)) {
+		ret = f2fs_revoke_deduped_inode(dst, __func__);
+		if (ret)
+			goto out_unlock;
+	}
+#endif
 
-	if ((iflags ^ masked_flags) & F2FS_CASEFOLD_FL) {
-		if (!f2fs_sb_has_casefold(F2FS_I_SB(inode)))
-			return -EOPNOTSUPP;
-		if (!f2fs_empty_dir(inode))
-			return -ENOTEMPTY;
+	if (f2fs_is_atomic_file(src) || f2fs_is_atomic_file(dst)) {
+		ret = -EINVAL;
+		goto out_unlock;
 	}
 
-	if (iflags & (F2FS_COMPR_FL | F2FS_NOCOMP_FL)) {
-		if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
-			return -EOPNOTSUPP;
-		if ((iflags & F2FS_COMPR_FL) && (iflags & F2FS_NOCOMP_FL))
-			return -EINVAL;
+	ret = -EINVAL;
+	if (pos_in + len > src->i_size || pos_in + len < pos_in)
+		goto out_unlock;
+	if (len == 0)
+		olen = len = src->i_size - pos_in;
+	if (pos_in + len == src->i_size)
+		len = ALIGN(src->i_size, F2FS_BLKSIZE) - pos_in;
+	if (len == 0) {
+		ret = 0;
+		goto out_unlock;
 	}
 
-	if ((iflags ^ masked_flags) & F2FS_COMPR_FL) {
-		if (masked_flags & F2FS_COMPR_FL) {
-			if (!f2fs_disable_compressed_file(inode))
-				return -EINVAL;
-		} else {
-			/* try to convert inline_data to support compression */
-			int err = f2fs_convert_inline_inode(inode);
-			if (err)
-				return err;
+	dst_osize = dst->i_size;
+	if (pos_out + olen > dst->i_size)
+		dst_max_i_size = pos_out + olen;
 
-			f2fs_down_write(&F2FS_I(inode)->i_sem);
-			if (!f2fs_may_compress(inode) ||
-					(S_ISREG(inode->i_mode) &&
-					F2FS_HAS_BLOCKS(inode))) {
-				f2fs_up_write(&F2FS_I(inode)->i_sem);
-				return -EINVAL;
-			}
-			err = set_compress_context(inode);
-			f2fs_up_write(&F2FS_I(inode)->i_sem);
+	/* verify the end result is block aligned */
+	if (!IS_ALIGNED(pos_in, F2FS_BLKSIZE) ||
+			!IS_ALIGNED(pos_in + len, F2FS_BLKSIZE) ||
+			!IS_ALIGNED(pos_out, F2FS_BLKSIZE))
+		goto out_unlock;
 
-			if (err)
-				return err;
+	ret = f2fs_convert_inline_inode(src);
+	if (ret)
+		goto out_unlock;
+
+	ret = f2fs_convert_inline_inode(dst);
+	if (ret)
+		goto out_unlock;
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compressed_file(src)) {
+		CLEAR_IFLAG_IF_SET(src, F2FS_NOCOMP_FL);
+		CLEAR_IFLAG_IF_SET(dst, F2FS_NOCOMP_FL);
+		if (is_inode_flag_set(src, FI_COMPRESS_RELEASED)) {
+			ret = f2fs_reserve_compress_blocks(src, NULL);
+			if (ret < 0)
+				goto out_unlock;
 		}
+		ret = f2fs_decompress_inode(src);
+		if (ret < 0)
+			goto out_unlock;
 	}
 
-	fi->i_flags = iflags | (fi->i_flags & ~mask);
-	f2fs_bug_on(F2FS_I_SB(inode), (fi->i_flags & F2FS_COMPR_FL) &&
-					(fi->i_flags & F2FS_NOCOMP_FL));
+	if (f2fs_compressed_file(dst)) {
+		if (is_inode_flag_set(dst, FI_COMPRESS_RELEASED)) {
+			ret = f2fs_reserve_compress_blocks(dst, NULL);
+			if (ret < 0)
+				goto out_unlock;
+		}
+		ret = f2fs_decompress_inode(dst);
+		if (ret < 0)
+			goto out_unlock;
+	}
+#endif
 
-	if (fi->i_flags & F2FS_PROJINHERIT_FL)
-		set_inode_flag(inode, FI_PROJ_INHERIT);
-	else
-		clear_inode_flag(inode, FI_PROJ_INHERIT);
+	/* write out all dirty pages from offset */
+	ret = filemap_write_and_wait_range(src->i_mapping,
+					pos_in, pos_in + len);
+	if (ret)
+		goto out_unlock;
 
-	inode->i_ctime = current_time(inode);
-	f2fs_set_inode_flags(inode);
-	f2fs_mark_inode_dirty_sync(inode, true);
-	return 0;
-}
+	ret = filemap_write_and_wait_range(dst->i_mapping,
+					pos_out, pos_out + len);
+	if (ret)
+		goto out_unlock;
 
-/* FS_IOC_[GS]ETFLAGS and FS_IOC_FS[GS]ETXATTR support */
+	f2fs_balance_fs(sbi, true);
 
-/*
- * To make a new on-disk f2fs i_flag gettable via FS_IOC_GETFLAGS, add an entry
- * for it to f2fs_fsflags_map[], and add its FS_*_FL equivalent to
- * F2FS_GETTABLE_FS_FL.  To also make it settable via FS_IOC_SETFLAGS, also add
- * its FS_*_FL equivalent to F2FS_SETTABLE_FS_FL.
- *
- * Translating flags to fsx_flags value used by FS_IOC_FSGETXATTR and
- * FS_IOC_FSSETXATTR is done by the VFS.
- */
+	f2fs_down_write(&F2FS_I(src)->i_gc_rwsem[WRITE]);
+	if (src != dst) {
+		ret = -EBUSY;
+		if (!f2fs_down_write_trylock(&F2FS_I(dst)->i_gc_rwsem[WRITE]))
+			goto out_src;
+	}
 
-static const struct {
-	u32 iflag;
-	u32 fsflag;
-} f2fs_fsflags_map[] = {
-	{ F2FS_COMPR_FL,	FS_COMPR_FL },
-	{ F2FS_SYNC_FL,		FS_SYNC_FL },
-	{ F2FS_IMMUTABLE_FL,	FS_IMMUTABLE_FL },
-	{ F2FS_APPEND_FL,	FS_APPEND_FL },
-	{ F2FS_NODUMP_FL,	FS_NODUMP_FL },
-	{ F2FS_NOATIME_FL,	FS_NOATIME_FL },
-	{ F2FS_NOCOMP_FL,	FS_NOCOMP_FL },
-	{ F2FS_INDEX_FL,	FS_INDEX_FL },
-	{ F2FS_DIRSYNC_FL,	FS_DIRSYNC_FL },
-	{ F2FS_PROJINHERIT_FL,	FS_PROJINHERIT_FL },
-	{ F2FS_CASEFOLD_FL,	FS_CASEFOLD_FL },
-};
+	f2fs_lock_op(sbi);
+	ret = __exchange_data_block(src, dst, pos_in >> F2FS_BLKSIZE_BITS,
+				pos_out >> F2FS_BLKSIZE_BITS,
+				len >> F2FS_BLKSIZE_BITS, false);
 
-#define F2FS_GETTABLE_FS_FL (		\
-		FS_COMPR_FL |		\
-		FS_SYNC_FL |		\
-		FS_IMMUTABLE_FL |	\
-		FS_APPEND_FL |		\
-		FS_NODUMP_FL |		\
-		FS_NOATIME_FL |		\
-		FS_NOCOMP_FL |		\
-		FS_INDEX_FL |		\
-		FS_DIRSYNC_FL |		\
-		FS_PROJINHERIT_FL |	\
-		FS_ENCRYPT_FL |		\
-		FS_INLINE_DATA_FL |	\
-		FS_NOCOW_FL |		\
-		FS_VERITY_FL |		\
-		FS_CASEFOLD_FL)
+	if (!ret) {
+		if (dst_max_i_size)
+			f2fs_i_size_write(dst, dst_max_i_size);
+		else if (dst_osize != dst->i_size)
+			f2fs_i_size_write(dst, dst_osize);
+	}
+	f2fs_unlock_op(sbi);
 
-#define F2FS_SETTABLE_FS_FL (		\
-		FS_COMPR_FL |		\
-		FS_SYNC_FL |		\
-		FS_IMMUTABLE_FL |	\
-		FS_APPEND_FL |		\
-		FS_NODUMP_FL |		\
-		FS_NOATIME_FL |		\
-		FS_NOCOMP_FL |		\
-		FS_DIRSYNC_FL |		\
-		FS_PROJINHERIT_FL |	\
-		FS_CASEFOLD_FL)
+	if (src != dst)
+		f2fs_up_write(&F2FS_I(dst)->i_gc_rwsem[WRITE]);
+out_src:
+	f2fs_up_write(&F2FS_I(src)->i_gc_rwsem[WRITE]);
+#ifdef CONFIG_F2FS_APPBOOST
+	src->i_mtime = src->i_ctime = current_time(src);
+	f2fs_mark_inode_dirty_sync(src, false);
+	if (src != dst) {
+		dst->i_mtime = dst->i_ctime = current_time(dst);
+		f2fs_mark_inode_dirty_sync(dst, false);
+	}
+	f2fs_update_time(sbi, REQ_TIME);
+#endif
+out_unlock:
+	if (src != dst)
+		inode_unlock(dst);
+out:
+	inode_unlock(src);
+	return ret;
+}
 
-/* Convert f2fs on-disk i_flags to FS_IOC_{GET,SET}FLAGS flags */
-static inline u32 f2fs_iflags_to_fsflags(u32 iflags)
+static int __f2fs_ioc_move_range(struct file *filp,
+				struct f2fs_move_range *range)
 {
-	u32 fsflags = 0;
-	int i;
+	struct fd dst;
+	int err;
 
-	for (i = 0; i < ARRAY_SIZE(f2fs_fsflags_map); i++)
-		if (iflags & f2fs_fsflags_map[i].iflag)
-			fsflags |= f2fs_fsflags_map[i].fsflag;
+	if (!(filp->f_mode & FMODE_READ) ||
+			!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
+
+	dst = fdget(range->dst_fd);
+	if (!dst.file)
+		return -EBADF;
 
-	return fsflags;
-}
+	if (!(dst.file->f_mode & FMODE_WRITE)) {
+		err = -EBADF;
+		goto err_out;
+	}
 
-/* Convert FS_IOC_{GET,SET}FLAGS flags to f2fs on-disk i_flags */
-static inline u32 f2fs_fsflags_to_iflags(u32 fsflags)
-{
-	u32 iflags = 0;
-	int i;
+	err = mnt_want_write_file(filp);
+	if (err)
+		goto err_out;
 
-	for (i = 0; i < ARRAY_SIZE(f2fs_fsflags_map); i++)
-		if (fsflags & f2fs_fsflags_map[i].fsflag)
-			iflags |= f2fs_fsflags_map[i].iflag;
+	err = f2fs_move_file_range(filp, range->pos_in, dst.file,
+					range->pos_out, range->len);
 
-	return iflags;
+	mnt_drop_write_file(filp);
+err_out:
+	fdput(dst);
+	return err;
 }
 
-static int f2fs_ioc_getversion(struct file *filp, unsigned long arg)
+static int f2fs_ioc_move_range(struct file *filp, unsigned long arg)
 {
-	struct inode *inode = file_inode(filp);
+	struct f2fs_move_range range;
 
-	return put_user(inode->i_generation, (int __user *)arg);
+	if (copy_from_user(&range, (struct f2fs_move_range __user *)arg,
+							sizeof(range)))
+		return -EFAULT;
+	return __f2fs_ioc_move_range(filp, &range);
 }
 
-static int f2fs_ioc_start_atomic_write(struct file *filp, bool truncate)
+static int f2fs_ioc_flush_device(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
-	struct f2fs_inode_info *fi = F2FS_I(inode);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	loff_t isize;
+	struct sit_info *sm = SIT_I(sbi);
+	unsigned int start_segno = 0, end_segno = 0;
+	unsigned int dev_start_segno = 0, dev_end_segno = 0;
+	struct f2fs_flush_device range;
+	struct f2fs_gc_control gc_control = {
+			.init_gc_type = FG_GC,
+			.should_migrate_blocks = true,
+			.err_gc_skipped = true,
+			.nr_free_secs = 0 };
 	int ret;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
-	if (!inode_owner_or_capable(mnt_userns, inode))
-		return -EACCES;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	if (!S_ISREG(inode->i_mode))
+	if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED)))
 		return -EINVAL;
 
-	if (filp->f_flags & O_DIRECT)
+	if (copy_from_user(&range, (struct f2fs_flush_device __user *)arg,
+							sizeof(range)))
+		return -EFAULT;
+
+	if (!f2fs_is_multi_device(sbi) || sbi->s_ndevs - 1 <= range.dev_num ||
+			__is_large_section(sbi)) {
+		f2fs_warn(sbi, "Can't flush %u in %d for segs_per_sec %u != 1",
+			  range.dev_num, sbi->s_ndevs, sbi->segs_per_sec);
 		return -EINVAL;
+	}
 
 	ret = mnt_want_write_file(filp);
 	if (ret)
 		return ret;
 
-	inode_lock(inode);
+	if (range.dev_num != 0)
+		dev_start_segno = GET_SEGNO(sbi, FDEV(range.dev_num).start_blk);
+	dev_end_segno = GET_SEGNO(sbi, FDEV(range.dev_num).end_blk);
 
-	if (!f2fs_disable_compressed_file(inode)) {
-		ret = -EINVAL;
-		goto out;
+	start_segno = sm->last_victim[FLUSH_DEVICE];
+	if (start_segno < dev_start_segno || start_segno >= dev_end_segno)
+		start_segno = dev_start_segno;
+	end_segno = min(start_segno + range.segments, dev_end_segno);
+
+	while (start_segno < end_segno) {
+		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
+			ret = -EBUSY;
+			goto out;
+		}
+		sm->last_victim[GC_CB] = end_segno + 1;
+		sm->last_victim[GC_GREEDY] = end_segno + 1;
+		sm->last_victim[ALLOC_NEXT] = end_segno + 1;
+
+		gc_control.victim_segno = start_segno;
+		ret = f2fs_gc(sbi, &gc_control);
+		if (ret == -EAGAIN)
+			ret = 0;
+		else if (ret < 0)
+			break;
+		start_segno++;
 	}
+out:
+	mnt_drop_write_file(filp);
+	return ret;
+}
 
-	if (f2fs_is_atomic_file(inode))
-		goto out;
+static int f2fs_ioc_get_features(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	u32 sb_feature = le32_to_cpu(F2FS_I_SB(inode)->raw_super->feature);
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		goto out;
+	/* Must validate to set it with SQLite behavior in Android. */
+	sb_feature |= F2FS_FEATURE_ATOMIC_WRITE;
 
-	f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+	return put_user(sb_feature, (u32 __user *)arg);
+}
 
-	/*
-	 * Should wait end_io to count F2FS_WB_CP_DATA correctly by
-	 * f2fs_is_atomic_file.
-	 */
-	if (get_dirty_pages(inode))
-		f2fs_warn(sbi, "Unexpected flush for atomic writes: ino=%lu, npages=%u",
-			  inode->i_ino, get_dirty_pages(inode));
-	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
-	if (ret) {
-		f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
-		goto out;
+#ifdef CONFIG_QUOTA
+int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid)
+{
+	struct dquot *transfer_to[MAXQUOTAS] = {};
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct super_block *sb = sbi->sb;
+	int err;
+
+	transfer_to[PRJQUOTA] = dqget(sb, make_kqid_projid(kprojid));
+	if (IS_ERR(transfer_to[PRJQUOTA]))
+		return PTR_ERR(transfer_to[PRJQUOTA]);
+
+	err = __dquot_transfer(inode, transfer_to);
+	if (err)
+		set_sbi_flag(sbi, SBI_QUOTA_NEED_REPAIR);
+	dqput(transfer_to[PRJQUOTA]);
+	return err;
+}
+
+static int f2fs_ioc_setproject(struct inode *inode, __u32 projid)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_inode *ri = NULL;
+	kprojid_t kprojid;
+	int err;
+
+	if (!f2fs_sb_has_project_quota(sbi)) {
+		if (projid != F2FS_DEF_PROJID)
+			return -EOPNOTSUPP;
+		else
+			return 0;
 	}
 
-	/* Check if the inode already has a COW inode */
-	if (fi->cow_inode == NULL) {
-		/* Create a COW inode for atomic write */
-		struct dentry *dentry = file_dentry(filp);
-		struct inode *dir = d_inode(dentry->d_parent);
+	if (!f2fs_has_extra_attr(inode))
+		return -EOPNOTSUPP;
 
-		ret = f2fs_get_tmpfile(mnt_userns, dir, &fi->cow_inode);
-		if (ret) {
-			f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
-			goto out;
-		}
+	kprojid = make_kprojid(&init_user_ns, (projid_t)projid);
 
-		set_inode_flag(fi->cow_inode, FI_COW_FILE);
-		clear_inode_flag(fi->cow_inode, FI_INLINE_DATA);
+	if (projid_eq(kprojid, fi->i_projid))
+		return 0;
 
-		/* Set the COW inode's atomic_inode to the atomic inode */
-		F2FS_I(fi->cow_inode)->atomic_inode = inode;
-	} else {
-		/* Reuse the already created COW inode */
-		f2fs_bug_on(sbi, get_dirty_pages(fi->cow_inode));
+	err = -EPERM;
+	/* Is it quota file? Do not allow user to mess with it */
+	if (IS_NOQUOTA(inode))
+		return err;
 
-		invalidate_mapping_pages(fi->cow_inode->i_mapping, 0, -1);
+	if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid))
+		return -EOVERFLOW;
 
-		ret = f2fs_do_truncate_blocks(fi->cow_inode, 0, true);
-		if (ret) {
-			f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
-			goto out;
-		}
-	}
+	err = f2fs_dquot_initialize(inode);
+	if (err)
+		return err;
 
-	f2fs_write_inode(inode, NULL);
+	f2fs_lock_op(sbi);
+	err = f2fs_transfer_project_quota(inode, kprojid);
+	if (err)
+		goto out_unlock;
 
-	stat_inc_atomic_inode(inode);
+	fi->i_projid = kprojid;
+	inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, true);
+out_unlock:
+	f2fs_unlock_op(sbi);
+	return err;
+}
+#else
+int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid)
+{
+	return 0;
+}
 
-	set_inode_flag(inode, FI_ATOMIC_FILE);
+static int f2fs_ioc_setproject(struct inode *inode, __u32 projid)
+{
+	if (projid != F2FS_DEF_PROJID)
+		return -EOPNOTSUPP;
+	return 0;
+}
+#endif
 
-	isize = i_size_read(inode);
-	fi->original_i_size = isize;
-	if (truncate) {
-		set_inode_flag(inode, FI_ATOMIC_REPLACE);
-		truncate_inode_pages_final(inode->i_mapping);
-		f2fs_i_size_write(inode, 0);
-		isize = 0;
-	}
-	f2fs_i_size_write(fi->cow_inode, isize);
+int f2fs_fileattr_get(struct dentry *dentry, struct fileattr *fa)
+{
+	struct inode *inode = d_inode(dentry);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	u32 fsflags = f2fs_iflags_to_fsflags(fi->i_flags);
 
-	f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+	if (IS_ENCRYPTED(inode))
+		fsflags |= FS_ENCRYPT_FL;
+	if (IS_VERITY(inode))
+		fsflags |= FS_VERITY_FL;
+	if (f2fs_has_inline_data(inode) || f2fs_has_inline_dentry(inode))
+		fsflags |= FS_INLINE_DATA_FL;
+	if (is_inode_flag_set(inode, FI_PIN_FILE))
+		fsflags |= FS_NOCOW_FL;
+	if (!may_compress)
+		fsflags &= ~FS_COMPR_FL;
 
-	f2fs_update_time(sbi, REQ_TIME);
-	fi->atomic_write_task = current;
-	stat_update_max_atomic_write(inode);
-	fi->atomic_write_cnt = 0;
-out:
-	inode_unlock(inode);
-	mnt_drop_write_file(filp);
-	return ret;
+	fileattr_fill_flags(fa, fsflags & F2FS_GETTABLE_FS_FL);
+
+	if (f2fs_sb_has_project_quota(F2FS_I_SB(inode)))
+		fa->fsx_projid = from_kprojid(&init_user_ns, fi->i_projid);
+
+	return 0;
 }
 
-static int f2fs_ioc_commit_atomic_write(struct file *filp)
+int f2fs_fileattr_set(struct user_namespace *mnt_userns,
+		      struct dentry *dentry, struct fileattr *fa)
 {
-	struct inode *inode = file_inode(filp);
-	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
-	int ret;
+	struct inode *inode = d_inode(dentry);
+	u32 fsflags = fa->flags, mask = F2FS_SETTABLE_FS_FL;
+	u32 iflags;
+	int err;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+	if (!f2fs_is_checkpoint_ready(F2FS_I_SB(inode)))
+		return -ENOSPC;
+	if (fsflags & ~F2FS_GETTABLE_FS_FL)
+		return -EOPNOTSUPP;
+	fsflags &= F2FS_SETTABLE_FS_FL;
+	if (!fa->flags_valid)
+		mask &= FS_COMMON_FL;
 
-	if (!inode_owner_or_capable(mnt_userns, inode))
-		return -EACCES;
+	iflags = f2fs_fsflags_to_iflags(fsflags);
+	if (f2fs_mask_flags(inode->i_mode, iflags) != iflags)
+		return -EOPNOTSUPP;
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	err = f2fs_setflags_common(inode, iflags, f2fs_fsflags_to_iflags(mask));
+	if (!err)
+		err = f2fs_ioc_setproject(inode, fa->fsx_projid);
 
-	f2fs_balance_fs(F2FS_I_SB(inode), true);
+	return err;
+}
 
-	inode_lock(inode);
+int f2fs_pin_file_control(struct inode *inode, bool inc)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	if (f2fs_is_atomic_file(inode)) {
-		ret = f2fs_commit_atomic_write(inode);
-		if (!ret)
-			ret = f2fs_do_sync_file(filp, 0, LLONG_MAX, 0, true);
+	/* Use i_gc_failures for normal file as a risk signal. */
+	if (inc)
+		f2fs_i_gc_failures_write(inode,
+				fi->i_gc_failures[GC_FAILURE_PIN] + 1);
 
-		f2fs_abort_atomic_write(inode, ret);
-	} else {
-		ret = f2fs_do_sync_file(filp, 0, LLONG_MAX, 1, false);
+	if (fi->i_gc_failures[GC_FAILURE_PIN] > sbi->gc_pin_file_threshold) {
+		f2fs_warn(sbi, "%s: Enable GC = ino %lx after %x GC trials",
+			  __func__, inode->i_ino,
+			  fi->i_gc_failures[GC_FAILURE_PIN]);
+		clear_inode_flag(inode, FI_PIN_FILE);
+		return -EAGAIN;
 	}
-
-	inode_unlock(inode);
-	mnt_drop_write_file(filp);
-	return ret;
+	return 0;
 }
 
-static int f2fs_ioc_abort_atomic_write(struct file *filp)
+static int f2fs_ioc_set_pin_file(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
-	int ret;
+	__u32 pin;
+	int ret = 0;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (get_user(pin, (__u32 __user *)arg))
+		return -EFAULT;
 
-	if (!inode_owner_or_capable(mnt_userns, inode))
-		return -EACCES;
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	if (f2fs_readonly(F2FS_I_SB(inode)->sb))
+		return -EROFS;
 
 	ret = mnt_want_write_file(filp);
 	if (ret)
@@ -2320,2054 +5282,2810 @@ static int f2fs_ioc_abort_atomic_write(struct file *filp)
 
 	inode_lock(inode);
 
-	f2fs_abort_atomic_write(inode, true);
+	if (f2fs_is_atomic_file(inode)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
-	inode_unlock(inode);
+	if (!pin) {
+		clear_inode_flag(inode, FI_PIN_FILE);
+		f2fs_i_gc_failures_write(inode, 0);
+		goto done;
+	}
 
-	mnt_drop_write_file(filp);
-	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
-	return ret;
-}
+	if (f2fs_should_update_outplace(inode, NULL)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
-static int f2fs_ioc_shutdown(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct super_block *sb = sbi->sb;
-	__u32 in;
-	int ret = 0;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		ret = f2fs_revoke_deduped_inode(inode, __func__);
+		if (ret)
+			goto out;
+	}
+#endif
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	if (f2fs_pin_file_control(inode, false)) {
+		ret = -EAGAIN;
+		goto out;
+	}
 
-	if (get_user(in, (__u32 __user *)arg))
-		return -EFAULT;
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		goto out;
 
-	if (in != F2FS_GOING_DOWN_FULLSYNC) {
-		ret = mnt_want_write_file(filp);
-		if (ret) {
-			if (ret == -EROFS) {
-				ret = 0;
-				f2fs_stop_checkpoint(sbi, false,
-						STOP_CP_REASON_SHUTDOWN);
-				set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-				trace_f2fs_shutdown(sbi, in, ret);
-			}
-			return ret;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compressed_file(inode)) {
+		if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+			ret = f2fs_reserve_compress_blocks(inode, NULL);
+			if (ret < 0)
+				goto out;
 		}
+		ret = f2fs_decompress_inode(inode);
+		if (ret < 0)
+			goto out;
 	}
+#endif
 
-	switch (in) {
-	case F2FS_GOING_DOWN_FULLSYNC:
-		ret = freeze_bdev(sb->s_bdev);
-		if (ret)
-			goto out;
-		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
-		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-		thaw_bdev(sb->s_bdev);
-		break;
-	case F2FS_GOING_DOWN_METASYNC:
-		/* do checkpoint only */
-		ret = f2fs_sync_fs(sb, 1);
-		if (ret) {
-			if (ret == -EIO)
-				ret = 0;
-			goto out;
-		}
-		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
-		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-		break;
-	case F2FS_GOING_DOWN_NOSYNC:
-		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
-		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-		break;
-	case F2FS_GOING_DOWN_METAFLUSH:
-		f2fs_sync_meta_pages(sbi, META, LONG_MAX, FS_META_IO);
-		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
-		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-		break;
-	case F2FS_GOING_DOWN_NEED_FSCK:
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		set_sbi_flag(sbi, SBI_CP_DISABLED_QUICK);
-		set_sbi_flag(sbi, SBI_IS_DIRTY);
-		/* do checkpoint only */
-		ret = f2fs_sync_fs(sb, 1);
-		if (ret == -EIO)
-			ret = 0;
-		goto out;
-	default:
-		ret = -EINVAL;
+	if (!f2fs_disable_compressed_file(inode)) {
+		ret = -EOPNOTSUPP;
 		goto out;
 	}
 
-	f2fs_stop_gc_thread(sbi);
-	f2fs_stop_discard_thread(sbi);
-
-	f2fs_drop_discard_cmd(sbi);
-	clear_opt(sbi, DISCARD);
-
-	f2fs_update_time(sbi, REQ_TIME);
+	set_inode_flag(inode, FI_PIN_FILE);
+	ret = F2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN];
+done:
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 out:
-	if (in != F2FS_GOING_DOWN_FULLSYNC)
-		mnt_drop_write_file(filp);
-
-	trace_f2fs_shutdown(sbi, in, ret);
-
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
 	return ret;
 }
 
-static int f2fs_ioc_fitrim(struct file *filp, unsigned long arg)
+static int f2fs_ioc_get_pin_file(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct super_block *sb = inode->i_sb;
-	struct fstrim_range range;
-	int ret;
+	__u32 pin = 0;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	if (is_inode_flag_set(inode, FI_PIN_FILE))
+		pin = F2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN];
+	return put_user(pin, (u32 __user *)arg);
+}
 
-	if (!f2fs_hw_support_discard(F2FS_SB(sb)))
+int f2fs_precache_extents(struct inode *inode)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_map_blocks map;
+	pgoff_t m_next_extent;
+	loff_t end;
+	int err;
+
+	if (is_inode_flag_set(inode, FI_NO_EXTENT))
 		return -EOPNOTSUPP;
 
-	if (copy_from_user(&range, (struct fstrim_range __user *)arg,
-				sizeof(range)))
-		return -EFAULT;
+	map.m_lblk = 0;
+	map.m_pblk = 0;
+	map.m_next_pgofs = NULL;
+	map.m_next_extent = &m_next_extent;
+	map.m_seg_type = NO_CHECK_TYPE;
+	map.m_may_create = false;
+	end = max_file_blocks(inode);
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	while (map.m_lblk < end) {
+		map.m_len = end - map.m_lblk;
 
-	range.minlen = max((unsigned int)range.minlen,
-			   bdev_discard_granularity(sb->s_bdev));
-	ret = f2fs_trim_fs(F2FS_SB(sb), &range);
-	mnt_drop_write_file(filp);
-	if (ret < 0)
-		return ret;
+		f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRECACHE);
+		f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+		if (err)
+			return err;
+
+		map.m_lblk = m_next_extent;
+	}
 
-	if (copy_to_user((struct fstrim_range __user *)arg, &range,
-				sizeof(range)))
-		return -EFAULT;
-	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 	return 0;
 }
 
-static bool uuid_is_nonzero(__u8 u[16])
+static int f2fs_ioc_precache_extents(struct file *filp)
+{
+	return f2fs_precache_extents(file_inode(filp));
+}
+
+static int f2fs_ioc_resize_fs(struct file *filp, unsigned long arg)
 {
-	int i;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));
+	__u64 block_count;
 
-	for (i = 0; i < 16; i++)
-		if (u[i])
-			return true;
-	return false;
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
+
+	if (copy_from_user(&block_count, (void __user *)arg,
+			   sizeof(block_count)))
+		return -EFAULT;
+
+	return f2fs_resize_fs(filp, block_count);
 }
 
-static int f2fs_ioc_set_encryption_policy(struct file *filp, unsigned long arg)
+static int f2fs_ioc_enable_verity(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(inode)))
-		return -EOPNOTSUPP;
-
 	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 
-	return fscrypt_ioctl_set_policy(filp, (const void __user *)arg);
+	if (!f2fs_sb_has_verity(F2FS_I_SB(inode))) {
+		f2fs_warn(F2FS_I_SB(inode),
+			  "Can't enable fs-verity on inode %lu: the verity feature is not enabled on this filesystem",
+			  inode->i_ino);
+		return -EOPNOTSUPP;
+	}
+
+	return fsverity_ioctl_enable(filp, (const void __user *)arg);
 }
 
-static int f2fs_ioc_get_encryption_policy(struct file *filp, unsigned long arg)
+static int f2fs_ioc_measure_verity(struct file *filp, unsigned long arg)
 {
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+	if (!f2fs_sb_has_verity(F2FS_I_SB(file_inode(filp))))
 		return -EOPNOTSUPP;
-	return fscrypt_ioctl_get_policy(filp, (void __user *)arg);
+
+	return fsverity_ioctl_measure(filp, (void __user *)arg);
 }
 
-static int f2fs_ioc_get_encryption_pwsalt(struct file *filp, unsigned long arg)
+static int f2fs_ioc_read_verity_metadata(struct file *filp, unsigned long arg)
 {
-	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	u8 encrypt_pw_salt[16];
-	int err;
-
-	if (!f2fs_sb_has_encrypt(sbi))
+	if (!f2fs_sb_has_verity(F2FS_I_SB(file_inode(filp))))
 		return -EOPNOTSUPP;
 
-	err = mnt_want_write_file(filp);
-	if (err)
-		return err;
-
-	f2fs_down_write(&sbi->sb_lock);
+	return fsverity_ioctl_read_metadata(filp, (const void __user *)arg);
+}
 
-	if (uuid_is_nonzero(sbi->raw_super->encrypt_pw_salt))
-		goto got_it;
+static int f2fs_ioc_getfslabel(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	char *vbuf;
+	int count;
+	int err = 0;
 
-	/* update superblock with uuid */
-	generate_random_uuid(sbi->raw_super->encrypt_pw_salt);
+	vbuf = f2fs_kzalloc(sbi, MAX_VOLUME_NAME, GFP_KERNEL);
+	if (!vbuf)
+		return -ENOMEM;
 
-	err = f2fs_commit_super(sbi, false);
-	if (err) {
-		/* undo new data */
-		memset(sbi->raw_super->encrypt_pw_salt, 0, 16);
-		goto out_err;
-	}
-got_it:
-	memcpy(encrypt_pw_salt, sbi->raw_super->encrypt_pw_salt, 16);
-out_err:
-	f2fs_up_write(&sbi->sb_lock);
-	mnt_drop_write_file(filp);
+	f2fs_down_read(&sbi->sb_lock);
+	count = utf16s_to_utf8s(sbi->raw_super->volume_name,
+			ARRAY_SIZE(sbi->raw_super->volume_name),
+			UTF16_LITTLE_ENDIAN, vbuf, MAX_VOLUME_NAME);
+	f2fs_up_read(&sbi->sb_lock);
 
-	if (!err && copy_to_user((__u8 __user *)arg, encrypt_pw_salt, 16))
+	if (copy_to_user((char __user *)arg, vbuf,
+				min(FSLABEL_MAX, count)))
 		err = -EFAULT;
 
+	kfree(vbuf);
 	return err;
 }
 
-static int f2fs_ioc_get_encryption_policy_ex(struct file *filp,
-					     unsigned long arg)
+static int f2fs_ioc_setfslabel(struct file *filp, unsigned long arg)
 {
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	char *vbuf;
+	int err = 0;
 
-	return fscrypt_ioctl_get_policy_ex(filp, (void __user *)arg);
-}
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
-static int f2fs_ioc_add_encryption_key(struct file *filp, unsigned long arg)
-{
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	vbuf = strndup_user((const char __user *)arg, FSLABEL_MAX);
+	if (IS_ERR(vbuf))
+		return PTR_ERR(vbuf);
 
-	return fscrypt_ioctl_add_key(filp, (void __user *)arg);
-}
+	err = mnt_want_write_file(filp);
+	if (err)
+		goto out;
 
-static int f2fs_ioc_remove_encryption_key(struct file *filp, unsigned long arg)
-{
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	f2fs_down_write(&sbi->sb_lock);
 
-	return fscrypt_ioctl_remove_key(filp, (void __user *)arg);
-}
+	memset(sbi->raw_super->volume_name, 0,
+			sizeof(sbi->raw_super->volume_name));
+	utf8s_to_utf16s(vbuf, strlen(vbuf), UTF16_LITTLE_ENDIAN,
+			sbi->raw_super->volume_name,
+			ARRAY_SIZE(sbi->raw_super->volume_name));
 
-static int f2fs_ioc_remove_encryption_key_all_users(struct file *filp,
-						    unsigned long arg)
-{
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	err = f2fs_commit_super(sbi, false);
 
-	return fscrypt_ioctl_remove_key_all_users(filp, (void __user *)arg);
+	f2fs_up_write(&sbi->sb_lock);
+
+	mnt_drop_write_file(filp);
+out:
+	kfree(vbuf);
+	return err;
 }
 
-static int f2fs_ioc_get_encryption_key_status(struct file *filp,
-					      unsigned long arg)
+static int f2fs_get_compress_blocks(struct inode *inode, __u64 *blocks)
 {
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
 		return -EOPNOTSUPP;
 
-	return fscrypt_ioctl_get_key_status(filp, (void __user *)arg);
-}
+	if (!f2fs_compressed_file(inode))
+		return -EINVAL;
 
-static int f2fs_ioc_get_encryption_nonce(struct file *filp, unsigned long arg)
-{
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	*blocks = atomic_read(&F2FS_I(inode)->i_compr_blocks);
 
-	return fscrypt_ioctl_get_nonce(filp, (void __user *)arg);
+	return 0;
 }
 
-static int f2fs_ioc_gc(struct file *filp, unsigned long arg)
+static int f2fs_ioc_get_compress_blocks(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_gc_control gc_control = { .victim_segno = NULL_SEGNO,
-			.no_bg_gc = false,
-			.should_migrate_blocks = false,
-			.nr_free_secs = 0 };
-	__u32 sync;
+	__u64 blocks;
 	int ret;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	ret = f2fs_get_compress_blocks(inode, &blocks);
+	if (ret < 0)
+		return ret;
 
-	if (get_user(sync, (__u32 __user *)arg))
-		return -EFAULT;
+	return put_user(blocks, (u64 __user *)arg);
+}
 
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+static int release_compress_blocks(struct dnode_of_data *dn, pgoff_t count)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	unsigned int released_blocks = 0;
+	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
+	block_t blkaddr;
+	int i;
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	for (i = 0; i < count; i++) {
+		blkaddr = data_blkaddr(dn->inode, dn->node_page,
+						dn->ofs_in_node + i);
 
-	if (!sync) {
-		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
-			ret = -EBUSY;
-			goto out;
+		if (!__is_valid_data_blkaddr(blkaddr))
+			continue;
+		if (unlikely(!f2fs_is_valid_blkaddr(sbi, blkaddr,
+					DATA_GENERIC_ENHANCE))) {
+			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
+			return -EFSCORRUPTED;
 		}
-	} else {
-		f2fs_down_write(&sbi->gc_lock);
 	}
 
-	gc_control.init_gc_type = sync ? FG_GC : BG_GC;
-	gc_control.err_gc_skipped = sync;
-	ret = f2fs_gc(sbi, &gc_control);
-out:
-	mnt_drop_write_file(filp);
-	return ret;
-}
+	while (count) {
+		int compr_blocks = 0;
 
-static int __f2fs_ioc_gc_range(struct file *filp, struct f2fs_gc_range *range)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));
-	struct f2fs_gc_control gc_control = {
-			.init_gc_type = range->sync ? FG_GC : BG_GC,
-			.no_bg_gc = false,
-			.should_migrate_blocks = false,
-			.err_gc_skipped = range->sync,
-			.nr_free_secs = 0 };
-	u64 end;
-	int ret;
+		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
+			blkaddr = f2fs_data_blkaddr(dn);
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+			if (i == 0) {
+				if (blkaddr == COMPRESS_ADDR)
+					continue;
+				dn->ofs_in_node += cluster_size;
+				goto next;
+			}
 
-	end = range->start + range->len;
-	if (end < range->start || range->start < MAIN_BLKADDR(sbi) ||
-					end >= MAX_BLKADDR(sbi))
-		return -EINVAL;
+			if (__is_valid_data_blkaddr(blkaddr))
+				compr_blocks++;
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+			if (blkaddr != NEW_ADDR)
+				continue;
 
-do_more:
-	if (!range->sync) {
-		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
-			ret = -EBUSY;
-			goto out;
+			f2fs_set_data_blkaddr(dn, NULL_ADDR);
 		}
-	} else {
-		f2fs_down_write(&sbi->gc_lock);
-	}
 
-	gc_control.victim_segno = GET_SEGNO(sbi, range->start);
-	ret = f2fs_gc(sbi, &gc_control);
-	if (ret) {
-		if (ret == -EBUSY)
-			ret = -EAGAIN;
-		goto out;
+		f2fs_i_compr_blocks_update(dn->inode, compr_blocks, false);
+		dec_valid_block_count(sbi, dn->inode,
+					cluster_size - compr_blocks);
+
+		released_blocks += cluster_size - compr_blocks;
+next:
+		count -= cluster_size;
 	}
-	range->start += CAP_BLKS_PER_SEC(sbi);
-	if (range->start <= end)
-		goto do_more;
-out:
-	mnt_drop_write_file(filp);
-	return ret;
-}
 
-static int f2fs_ioc_gc_range(struct file *filp, unsigned long arg)
-{
-	struct f2fs_gc_range range;
-
-	if (copy_from_user(&range, (struct f2fs_gc_range __user *)arg,
-							sizeof(range)))
-		return -EFAULT;
-	return __f2fs_ioc_gc_range(filp, &range);
+	return released_blocks;
 }
 
-static int f2fs_ioc_write_checkpoint(struct file *filp)
+static int f2fs_release_compress_blocks(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	pgoff_t page_idx = 0, last_idx;
+	unsigned int released_blocks = 0;
 	int ret;
+	int writecount;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	if (!f2fs_sb_has_compression(sbi))
+		return -EOPNOTSUPP;
+
+	if (!f2fs_compressed_file(inode))
+		return -EINVAL;
 
 	if (f2fs_readonly(sbi->sb))
 		return -EROFS;
 
-	if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {
-		f2fs_info(sbi, "Skipping Checkpoint. Checkpoints currently disabled.");
-		return -EINVAL;
-	}
-
 	ret = mnt_want_write_file(filp);
 	if (ret)
 		return ret;
 
-	ret = f2fs_sync_fs(sbi->sb, 1);
-
-	mnt_drop_write_file(filp);
-	return ret;
-}
-
-static int f2fs_defragment_range(struct f2fs_sb_info *sbi,
-					struct file *filp,
-					struct f2fs_defragment *range)
-{
-	struct inode *inode = file_inode(filp);
-	struct f2fs_map_blocks map = { .m_next_extent = NULL,
-					.m_seg_type = NO_CHECK_TYPE,
-					.m_may_create = false };
-	struct extent_info ei = {};
-	pgoff_t pg_start, pg_end, next_pgofs;
-	unsigned int blk_per_seg = sbi->blocks_per_seg;
-	unsigned int total = 0, sec_num;
-	block_t blk_end = 0;
-	bool fragmented = false;
-	int err;
-
-	pg_start = range->start >> PAGE_SHIFT;
-	pg_end = (range->start + range->len) >> PAGE_SHIFT;
-
 	f2fs_balance_fs(sbi, true);
 
 	inode_lock(inode);
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED) ||
-		f2fs_is_atomic_file(inode)) {
-		err = -EINVAL;
-		goto unlock_out;
+	writecount = atomic_read(&inode->i_writecount);
+	if ((filp->f_mode & FMODE_WRITE && writecount != 1) ||
+			(!(filp->f_mode & FMODE_WRITE) && writecount)) {
+		ret = -EBUSY;
+		goto out;
 	}
 
-	/* if in-place-update policy is enabled, don't waste time here */
-	set_inode_flag(inode, FI_OPU_WRITE);
-	if (f2fs_should_update_inplace(inode, NULL)) {
-		err = -EINVAL;
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+		ret = -EINVAL;
 		goto out;
 	}
 
-	/* writeback all dirty pages in the range */
-	err = filemap_write_and_wait_range(inode->i_mapping, range->start,
-						range->start + range->len - 1);
-	if (err)
+	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+	if (ret)
 		goto out;
 
-	/*
-	 * lookup mapping info in extent cache, skip defragmenting if physical
-	 * block addresses are continuous.
-	 */
-	if (f2fs_lookup_read_extent_cache(inode, pg_start, &ei)) {
-		if ((pgoff_t)ei.fofs + ei.len >= pg_end)
-			goto out;
+	if (!atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
+		ret = -EPERM;
+		goto out;
 	}
 
-	map.m_lblk = pg_start;
-	map.m_next_pgofs = &next_pgofs;
+	f2fs_info(sbi, "start release cblocks ino %lu (%pd) size %llu blocks %llu "
+		"cblocks %d\n", inode->i_ino, file_dentry(filp),
+		i_size_read(inode), inode->i_blocks,
+		atomic_read(&F2FS_I(inode)->i_compr_blocks));
 
-	/*
-	 * lookup mapping info in dnode page cache, skip defragmenting if all
-	 * physical block addresses are continuous even if there are hole(s)
-	 * in logical blocks.
-	 */
-	while (map.m_lblk < pg_end) {
-		map.m_len = pg_end - map.m_lblk;
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT);
-		if (err)
-			goto out;
+	set_inode_flag(inode, FI_COMPRESS_RELEASED);
+	inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, true);
 
-		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
-			map.m_lblk = next_pgofs;
-			continue;
-		}
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(inode->i_mapping);
 
-		if (blk_end && blk_end != map.m_pblk)
-			fragmented = true;
+	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
 
-		/* record total count of block that we're going to move */
-		total += map.m_len;
+	while (page_idx < last_idx) {
+		struct dnode_of_data dn;
+		pgoff_t end_offset, count;
 
-		blk_end = map.m_pblk + map.m_len;
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		ret = f2fs_get_dnode_of_data(&dn, page_idx, LOOKUP_NODE);
+		if (ret) {
+			if (ret == -ENOENT) {
+				page_idx = f2fs_get_next_page_offset(&dn,
+								page_idx);
+				ret = 0;
+				continue;
+			}
+			break;
+		}
 
-		map.m_lblk += map.m_len;
-	}
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+		count = min(end_offset - dn.ofs_in_node, last_idx - page_idx);
+		count = round_up(count, F2FS_I(inode)->i_cluster_size);
 
-	if (!fragmented) {
-		total = 0;
-		goto out;
-	}
+		ret = release_compress_blocks(&dn, count);
 
-	sec_num = DIV_ROUND_UP(total, CAP_BLKS_PER_SEC(sbi));
+		f2fs_put_dnode(&dn);
 
-	/*
-	 * make sure there are enough free section for LFS allocation, this can
-	 * avoid defragment running in SSR mode when free section are allocated
-	 * intensively
-	 */
-	if (has_not_enough_free_secs(sbi, 0, sec_num)) {
-		err = -EAGAIN;
-		goto out;
-	}
+		if (ret < 0)
+			break;
 
-	map.m_lblk = pg_start;
-	map.m_len = pg_end - pg_start;
-	total = 0;
+		page_idx += count;
+		released_blocks += ret;
+	}
 
-	while (map.m_lblk < pg_end) {
-		pgoff_t idx;
-		int cnt = 0;
+	filemap_invalidate_unlock(inode->i_mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 
-do_map:
-		map.m_len = pg_end - map.m_lblk;
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT);
-		if (err)
-			goto clear_out;
+	f2fs_info(sbi, "end release cblocks ino %lu (%pd) size %llu blocks %llu "
+		"cblocks %d rblocks %u ret %d\n", inode->i_ino, file_dentry(filp),
+		i_size_read(inode), inode->i_blocks,
+		atomic_read(&F2FS_I(inode)->i_compr_blocks), released_blocks, ret);
 
-		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
-			map.m_lblk = next_pgofs;
-			goto check;
-		}
+out:
+	inode_unlock(inode);
 
-		set_inode_flag(inode, FI_SKIP_WRITES);
+	mnt_drop_write_file(filp);
 
-		idx = map.m_lblk;
-		while (idx < map.m_lblk + map.m_len && cnt < blk_per_seg) {
-			struct page *page;
+	if (ret >= 0) {
+		ret = put_user(released_blocks, (u64 __user *)arg);
+	} else if (released_blocks &&
+			atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		f2fs_warn(sbi, "%s: partial blocks were released i_ino=%lx "
+			"iblocks=%llu, released=%u, compr_blocks=%u, "
+			"run fsck to fix.",
+			__func__, inode->i_ino, inode->i_blocks,
+			released_blocks,
+			atomic_read(&F2FS_I(inode)->i_compr_blocks));
+	}
 
-			page = f2fs_get_lock_data_page(inode, idx, true);
-			if (IS_ERR(page)) {
-				err = PTR_ERR(page);
-				goto clear_out;
-			}
+	return ret;
+}
 
-			f2fs_wait_on_page_writeback(page, DATA, true, true);
+static int reserve_compress_blocks(struct dnode_of_data *dn, pgoff_t count,
+		unsigned int *reserved_blocks)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
+	block_t blkaddr;
+	int i;
 
-			set_page_dirty(page);
-			set_page_private_gcing(page);
-			f2fs_put_page(page, 1);
+	for (i = 0; i < count; i++) {
+		blkaddr = data_blkaddr(dn->inode, dn->node_page,
+						dn->ofs_in_node + i);
 
-			idx++;
-			cnt++;
-			total++;
+		if (!__is_valid_data_blkaddr(blkaddr))
+			continue;
+		if (unlikely(!f2fs_is_valid_blkaddr(sbi, blkaddr,
+					DATA_GENERIC_ENHANCE))) {
+			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
+			return -EFSCORRUPTED;
 		}
-
-		map.m_lblk = idx;
-check:
-		if (map.m_lblk < pg_end && cnt < blk_per_seg)
-			goto do_map;
-
-		clear_inode_flag(inode, FI_SKIP_WRITES);
-
-		err = filemap_fdatawrite(inode->i_mapping);
-		if (err)
-			goto out;
 	}
-clear_out:
-	clear_inode_flag(inode, FI_SKIP_WRITES);
-out:
-	clear_inode_flag(inode, FI_OPU_WRITE);
-unlock_out:
-	inode_unlock(inode);
-	if (!err)
-		range->len = (u64)total << PAGE_SHIFT;
-	return err;
-}
 
-static int f2fs_ioc_defragment(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_defragment range;
-	int err;
+	while (count) {
+		int compr_blocks = 0;
+		blkcnt_t reserved = 0;
+		blkcnt_t to_reserved;
+		int ret;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
+			blkaddr = f2fs_data_blkaddr(dn);
 
-	if (!S_ISREG(inode->i_mode) || f2fs_is_atomic_file(inode))
-		return -EINVAL;
+			if (i == 0) {
+				if (blkaddr == COMPRESS_ADDR)
+					continue;
+				dn->ofs_in_node += cluster_size;
+				goto next;
+			}
 
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+			/*
+			 * compressed cluster was not released due to it
+			 * fails in release_compress_blocks(), so NEW_ADDR
+			 * is a possible case.
+			 */
+			if (blkaddr == NEW_ADDR) {
+				reserved++;
+				continue;
+			}
+			if (__is_valid_data_blkaddr(blkaddr)) {
+				compr_blocks++;
+				continue;
+			}
 
-	if (copy_from_user(&range, (struct f2fs_defragment __user *)arg,
-							sizeof(range)))
-		return -EFAULT;
+			f2fs_set_data_blkaddr(dn,NEW_ADDR);
+		}
 
-	/* verify alignment of offset & size */
-	if (range.start & (F2FS_BLKSIZE - 1) || range.len & (F2FS_BLKSIZE - 1))
-		return -EINVAL;
+		to_reserved = cluster_size - compr_blocks - reserved;
+		if (time_to_inject(sbi, FAULT_COMPRESS_RESERVE_NOSPC))
+			return -ENOSPC;
 
-	if (unlikely((range.start + range.len) >> PAGE_SHIFT >
-					max_file_blocks(inode)))
-		return -EINVAL;
+		/* for the case all blocks in cluster were reserved */
+		if (to_reserved == 1) {
+			dn->ofs_in_node += cluster_size;
+			goto next;
+		}
 
-	err = mnt_want_write_file(filp);
-	if (err)
-		return err;
+		ret = inc_valid_block_count(sbi, dn->inode,
+						&to_reserved, false);
+		if (unlikely(ret))
+			return ret;
 
-	err = f2fs_defragment_range(sbi, filp, &range);
-	mnt_drop_write_file(filp);
+		if (reserved != cluster_size - compr_blocks)
+			return -ENOSPC;
+		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
+			if (f2fs_data_blkaddr(dn) == NULL_ADDR)
+				f2fs_set_data_blkaddr(dn, NEW_ADDR);
+		}
 
-	f2fs_update_time(sbi, REQ_TIME);
-	if (err < 0)
-		return err;
+		f2fs_i_compr_blocks_update(dn->inode, compr_blocks, true);
 
-	if (copy_to_user((struct f2fs_defragment __user *)arg, &range,
-							sizeof(range)))
-		return -EFAULT;
+		*reserved_blocks += to_reserved;
+next:
+		count -= cluster_size;
+	}
 
 	return 0;
 }
 
-static int f2fs_move_file_range(struct file *file_in, loff_t pos_in,
-			struct file *file_out, loff_t pos_out, size_t len)
+int f2fs_reserve_compress_blocks(struct inode *inode, unsigned int *ret_rsvd_blks)
 {
-	struct inode *src = file_inode(file_in);
-	struct inode *dst = file_inode(file_out);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(src);
-	size_t olen = len, dst_max_i_size = 0;
-	size_t dst_osize;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	pgoff_t page_idx = 0, last_idx;
+	unsigned int reserved_blocks = 0;
 	int ret;
 
-	if (file_in->f_path.mnt != file_out->f_path.mnt ||
-				src->i_sb != dst->i_sb)
-		return -EXDEV;
-
-	if (unlikely(f2fs_readonly(src->i_sb)))
-		return -EROFS;
-
-	if (!S_ISREG(src->i_mode) || !S_ISREG(dst->i_mode))
-		return -EINVAL;
-
-	if (IS_ENCRYPTED(src) || IS_ENCRYPTED(dst))
-		return -EOPNOTSUPP;
+	f2fs_bug_on(sbi, !inode_is_locked(inode));
 
-	if (pos_out < 0 || pos_in < 0)
+	if (!is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
 		return -EINVAL;
 
-	if (src == dst) {
-		if (pos_in == pos_out)
-			return 0;
-		if (pos_out > pos_in && pos_out < pos_in + len)
-			return -EINVAL;
-	}
-
-	inode_lock(src);
-	if (src != dst) {
-		ret = -EBUSY;
-		if (!inode_trylock(dst))
-			goto out;
-	}
-
-	if (f2fs_compressed_file(src) || f2fs_compressed_file(dst)) {
-		ret = -EOPNOTSUPP;
-		goto out_unlock;
-	}
-
-	if (f2fs_is_atomic_file(src) || f2fs_is_atomic_file(dst)) {
-		ret = -EINVAL;
-		goto out_unlock;
-	}
+	f2fs_info(sbi, "start reserve cblocks ino %lu size %llu blocks %llu "
+		"cblocks %d caller %ps\n", inode->i_ino, i_size_read(inode),
+		inode->i_blocks, atomic_read(&F2FS_I(inode)->i_compr_blocks),
+		__builtin_return_address(0));
 
-	ret = -EINVAL;
-	if (pos_in + len > src->i_size || pos_in + len < pos_in)
-		goto out_unlock;
-	if (len == 0)
-		olen = len = src->i_size - pos_in;
-	if (pos_in + len == src->i_size)
-		len = ALIGN(src->i_size, F2FS_BLKSIZE) - pos_in;
-	if (len == 0) {
-		ret = 0;
-		goto out_unlock;
-	}
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(inode->i_mapping);
 
-	dst_osize = dst->i_size;
-	if (pos_out + olen > dst->i_size)
-		dst_max_i_size = pos_out + olen;
+	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
 
-	/* verify the end result is block aligned */
-	if (!IS_ALIGNED(pos_in, F2FS_BLKSIZE) ||
-			!IS_ALIGNED(pos_in + len, F2FS_BLKSIZE) ||
-			!IS_ALIGNED(pos_out, F2FS_BLKSIZE))
-		goto out_unlock;
+	while (page_idx < last_idx) {
+		struct dnode_of_data dn;
+		pgoff_t end_offset, count;
 
-	ret = f2fs_convert_inline_inode(src);
-	if (ret)
-		goto out_unlock;
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		ret = f2fs_get_dnode_of_data(&dn, page_idx, LOOKUP_NODE);
+		if (ret) {
+			if (ret == -ENOENT) {
+				page_idx = f2fs_get_next_page_offset(&dn,
+								page_idx);
+				ret = 0;
+				continue;
+			}
+			break;
+		}
 
-	ret = f2fs_convert_inline_inode(dst);
-	if (ret)
-		goto out_unlock;
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+		count = min(end_offset - dn.ofs_in_node, last_idx - page_idx);
+		count = round_up(count, F2FS_I(inode)->i_cluster_size);
 
-	/* write out all dirty pages from offset */
-	ret = filemap_write_and_wait_range(src->i_mapping,
-					pos_in, pos_in + len);
-	if (ret)
-		goto out_unlock;
+		ret = reserve_compress_blocks(&dn, count, &reserved_blocks);
 
-	ret = filemap_write_and_wait_range(dst->i_mapping,
-					pos_out, pos_out + len);
-	if (ret)
-		goto out_unlock;
+		f2fs_put_dnode(&dn);
 
-	f2fs_balance_fs(sbi, true);
+		if (ret < 0)
+			break;
 
-	f2fs_down_write(&F2FS_I(src)->i_gc_rwsem[WRITE]);
-	if (src != dst) {
-		ret = -EBUSY;
-		if (!f2fs_down_write_trylock(&F2FS_I(dst)->i_gc_rwsem[WRITE]))
-			goto out_src;
+		page_idx += count;
 	}
 
-	f2fs_lock_op(sbi);
-	ret = __exchange_data_block(src, dst, pos_in >> F2FS_BLKSIZE_BITS,
-				pos_out >> F2FS_BLKSIZE_BITS,
-				len >> F2FS_BLKSIZE_BITS, false);
+	filemap_invalidate_unlock(inode->i_mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 
 	if (!ret) {
-		if (dst_max_i_size)
-			f2fs_i_size_write(dst, dst_max_i_size);
-		else if (dst_osize != dst->i_size)
-			f2fs_i_size_write(dst, dst_osize);
-	}
-	f2fs_unlock_op(sbi);
-
-	if (src != dst)
-		f2fs_up_write(&F2FS_I(dst)->i_gc_rwsem[WRITE]);
-out_src:
-	f2fs_up_write(&F2FS_I(src)->i_gc_rwsem[WRITE]);
-out_unlock:
-	if (src != dst)
-		inode_unlock(dst);
-out:
-	inode_unlock(src);
-	return ret;
-}
-
-static int __f2fs_ioc_move_range(struct file *filp,
-				struct f2fs_move_range *range)
-{
-	struct fd dst;
-	int err;
-
-	if (!(filp->f_mode & FMODE_READ) ||
-			!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
-
-	dst = fdget(range->dst_fd);
-	if (!dst.file)
-		return -EBADF;
-
-	if (!(dst.file->f_mode & FMODE_WRITE)) {
-		err = -EBADF;
-		goto err_out;
+		clear_inode_flag(inode, FI_COMPRESS_RELEASED);
+		inode->i_ctime = current_time(inode);
+		f2fs_mark_inode_dirty_sync(inode, true);
+		if (ret_rsvd_blks)
+			*ret_rsvd_blks = reserved_blocks;
+	} else if (reserved_blocks &&
+			atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		f2fs_warn(sbi, "%s: partial blocks were released i_ino=%lx "
+			"iblocks=%llu, reserved=%u, compr_blocks=%u, "
+			"run fsck to fix.",
+			__func__, inode->i_ino, inode->i_blocks,
+			reserved_blocks,
+			atomic_read(&F2FS_I(inode)->i_compr_blocks));
 	}
 
-	err = mnt_want_write_file(filp);
-	if (err)
-		goto err_out;
-
-	err = f2fs_move_file_range(filp, range->pos_in, dst.file,
-					range->pos_out, range->len);
-
-	mnt_drop_write_file(filp);
-err_out:
-	fdput(dst);
-	return err;
-}
-
-static int f2fs_ioc_move_range(struct file *filp, unsigned long arg)
-{
-	struct f2fs_move_range range;
+	f2fs_info(sbi, "end reserve cblocks ino %lu size %llu blocks %llu "
+		"cblocks %d rsvd %d caller %ps ret %d\n", inode->i_ino,
+		i_size_read(inode), inode->i_blocks,
+		atomic_read(&F2FS_I(inode)->i_compr_blocks), reserved_blocks,
+		__builtin_return_address(0), ret);
 
-	if (copy_from_user(&range, (struct f2fs_move_range __user *)arg,
-							sizeof(range)))
-		return -EFAULT;
-	return __f2fs_ioc_move_range(filp, &range);
+	return ret;
 }
 
-static int f2fs_ioc_flush_device(struct file *filp, unsigned long arg)
+static int f2fs_ioc_reserve_compress_blocks(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct sit_info *sm = SIT_I(sbi);
-	unsigned int start_segno = 0, end_segno = 0;
-	unsigned int dev_start_segno = 0, dev_end_segno = 0;
-	struct f2fs_flush_device range;
-	struct f2fs_gc_control gc_control = {
-			.init_gc_type = FG_GC,
-			.should_migrate_blocks = true,
-			.err_gc_skipped = true,
-			.nr_free_secs = 0 };
+	unsigned int reserved_blocks = 0;
 	int ret;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
-
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
+		return -EOPNOTSUPP;
 
-	if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED)))
+	if (!f2fs_compressed_file(inode))
 		return -EINVAL;
 
-	if (copy_from_user(&range, (struct f2fs_flush_device __user *)arg,
-							sizeof(range)))
-		return -EFAULT;
-
-	if (!f2fs_is_multi_device(sbi) || sbi->s_ndevs - 1 <= range.dev_num ||
-			__is_large_section(sbi)) {
-		f2fs_warn(sbi, "Can't flush %u in %d for segs_per_sec %u != 1",
-			  range.dev_num, sbi->s_ndevs, sbi->segs_per_sec);
-		return -EINVAL;
-	}
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
 	ret = mnt_want_write_file(filp);
 	if (ret)
 		return ret;
 
-	if (range.dev_num != 0)
-		dev_start_segno = GET_SEGNO(sbi, FDEV(range.dev_num).start_blk);
-	dev_end_segno = GET_SEGNO(sbi, FDEV(range.dev_num).end_blk);
+	if (atomic_read(&F2FS_I(inode)->i_compr_blocks))
+		goto out;
 
-	start_segno = sm->last_victim[FLUSH_DEVICE];
-	if (start_segno < dev_start_segno || start_segno >= dev_end_segno)
-		start_segno = dev_start_segno;
-	end_segno = min(start_segno + range.segments, dev_end_segno);
+	f2fs_balance_fs(F2FS_I_SB(inode), true);
 
-	while (start_segno < end_segno) {
-		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
-			ret = -EBUSY;
-			goto out;
-		}
-		sm->last_victim[GC_CB] = end_segno + 1;
-		sm->last_victim[GC_GREEDY] = end_segno + 1;
-		sm->last_victim[ALLOC_NEXT] = end_segno + 1;
+	inode_lock(inode);
+	ret = f2fs_reserve_compress_blocks(inode, &reserved_blocks);
+	inode_unlock(inode);
 
-		gc_control.victim_segno = start_segno;
-		ret = f2fs_gc(sbi, &gc_control);
-		if (ret == -EAGAIN)
-			ret = 0;
-		else if (ret < 0)
-			break;
-		start_segno++;
-	}
 out:
 	mnt_drop_write_file(filp);
+
+	if (ret >= 0)
+		ret = put_user(reserved_blocks, (u64 __user *)arg);
+
 	return ret;
 }
 
-static int f2fs_ioc_get_features(struct file *filp, unsigned long arg)
+static int f2fs_secure_erase(struct block_device *bdev, struct inode *inode,
+		pgoff_t off, block_t block, block_t len, u32 flags)
 {
-	struct inode *inode = file_inode(filp);
-	u32 sb_feature = le32_to_cpu(F2FS_I_SB(inode)->raw_super->feature);
+	sector_t sector = SECTOR_FROM_BLOCK(block);
+	sector_t nr_sects = SECTOR_FROM_BLOCK(len);
+	int ret = 0;
 
-	/* Must validate to set it with SQLite behavior in Android. */
-	sb_feature |= F2FS_FEATURE_ATOMIC_WRITE;
+	if (flags & F2FS_TRIM_FILE_DISCARD) {
+		if (bdev_max_secure_erase_sectors(bdev))
+			ret = blkdev_issue_secure_erase(bdev, sector, nr_sects,
+					GFP_NOFS);
+		else
+			ret = blkdev_issue_discard(bdev, sector, nr_sects,
+					GFP_NOFS);
+	}
 
-	return put_user(sb_feature, (u32 __user *)arg);
+	if (!ret && (flags & F2FS_TRIM_FILE_ZEROOUT)) {
+		if (IS_ENCRYPTED(inode))
+			ret = fscrypt_zeroout_range(inode, off, block, len);
+		else
+			ret = blkdev_issue_zeroout(bdev, sector, nr_sects,
+					GFP_NOFS, 0);
+	}
+
+	return ret;
 }
 
-#ifdef CONFIG_QUOTA
-int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid)
+static int f2fs_sec_trim_file(struct file *filp, unsigned long arg)
 {
-	struct dquot *transfer_to[MAXQUOTAS] = {};
+	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct super_block *sb = sbi->sb;
-	int err;
+	struct address_space *mapping = inode->i_mapping;
+	struct block_device *prev_bdev = NULL;
+	struct f2fs_sectrim_range range;
+	pgoff_t index, pg_end, prev_index = 0;
+	block_t prev_block = 0, len = 0;
+	loff_t end_addr;
+	bool to_end = false;
+	int ret = 0;
 
-	transfer_to[PRJQUOTA] = dqget(sb, make_kqid_projid(kprojid));
-	if (IS_ERR(transfer_to[PRJQUOTA]))
-		return PTR_ERR(transfer_to[PRJQUOTA]);
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
 
-	err = __dquot_transfer(inode, transfer_to);
-	if (err)
-		set_sbi_flag(sbi, SBI_QUOTA_NEED_REPAIR);
-	dqput(transfer_to[PRJQUOTA]);
-	return err;
-}
+	if (copy_from_user(&range, (struct f2fs_sectrim_range __user *)arg,
+				sizeof(range)))
+		return -EFAULT;
 
-static int f2fs_ioc_setproject(struct inode *inode, __u32 projid)
-{
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_inode *ri = NULL;
-	kprojid_t kprojid;
-	int err;
+	if (range.flags == 0 || (range.flags & ~F2FS_TRIM_FILE_MASK) ||
+			!S_ISREG(inode->i_mode))
+		return -EINVAL;
 
-	if (!f2fs_sb_has_project_quota(sbi)) {
-		if (projid != F2FS_DEF_PROJID)
-			return -EOPNOTSUPP;
-		else
-			return 0;
+	if (((range.flags & F2FS_TRIM_FILE_DISCARD) &&
+			!f2fs_hw_support_discard(sbi)) ||
+			((range.flags & F2FS_TRIM_FILE_ZEROOUT) &&
+			 IS_ENCRYPTED(inode) && f2fs_is_multi_device(sbi)))
+		return -EOPNOTSUPP;
+
+	file_start_write(filp);
+	inode_lock(inode);
+
+	if (f2fs_is_atomic_file(inode) || f2fs_compressed_file(inode) ||
+			range.start >= inode->i_size) {
+		ret = -EINVAL;
+		goto err;
 	}
 
-	if (!f2fs_has_extra_attr(inode))
-		return -EOPNOTSUPP;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		ret = f2fs_revoke_deduped_inode(inode, __func__);
+		if (ret)
+			goto err;
+	}
+#endif
 
-	kprojid = make_kprojid(&init_user_ns, (projid_t)projid);
+	if (range.len == 0)
+		goto err;
 
-	if (projid_eq(kprojid, fi->i_projid))
-		return 0;
+	if (inode->i_size - range.start > range.len) {
+		end_addr = range.start + range.len;
+	} else {
+		end_addr = range.len == (u64)-1 ?
+			sbi->sb->s_maxbytes : inode->i_size;
+		to_end = true;
+	}
 
-	err = -EPERM;
-	/* Is it quota file? Do not allow user to mess with it */
-	if (IS_NOQUOTA(inode))
-		return err;
+	if (!IS_ALIGNED(range.start, F2FS_BLKSIZE) ||
+			(!to_end && !IS_ALIGNED(end_addr, F2FS_BLKSIZE))) {
+		ret = -EINVAL;
+		goto err;
+	}
+
+	index = F2FS_BYTES_TO_BLK(range.start);
+	pg_end = DIV_ROUND_UP(end_addr, F2FS_BLKSIZE);
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		goto err;
+
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(mapping);
+
+	ret = filemap_write_and_wait_range(mapping, range.start,
+			to_end ? LLONG_MAX : end_addr - 1);
+	if (ret)
+		goto out;
+
+	truncate_inode_pages_range(mapping, range.start,
+			to_end ? -1 : end_addr - 1);
+
+	while (index < pg_end) {
+		struct dnode_of_data dn;
+		pgoff_t end_offset, count;
+		int i;
+
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		ret = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);
+		if (ret) {
+			if (ret == -ENOENT) {
+				index = f2fs_get_next_page_offset(&dn, index);
+				continue;
+			}
+			goto out;
+		}
+
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+		count = min(end_offset - dn.ofs_in_node, pg_end - index);
+		for (i = 0; i < count; i++, index++, dn.ofs_in_node++) {
+			struct block_device *cur_bdev;
+			block_t blkaddr = f2fs_data_blkaddr(&dn);
+
+			if (!__is_valid_data_blkaddr(blkaddr))
+				continue;
+
+			if (!f2fs_is_valid_blkaddr(sbi, blkaddr,
+						DATA_GENERIC_ENHANCE)) {
+				ret = -EFSCORRUPTED;
+				f2fs_put_dnode(&dn);
+				f2fs_handle_error(sbi,
+						ERROR_INVALID_BLKADDR);
+				goto out;
+			}
+
+			cur_bdev = f2fs_target_device(sbi, blkaddr, NULL);
+			if (f2fs_is_multi_device(sbi)) {
+				int di = f2fs_target_device_index(sbi, blkaddr);
+
+				blkaddr -= FDEV(di).start_blk;
+			}
+
+			if (len) {
+				if (prev_bdev == cur_bdev &&
+						index == prev_index + len &&
+						blkaddr == prev_block + len) {
+					len++;
+				} else {
+					ret = f2fs_secure_erase(prev_bdev,
+						inode, prev_index, prev_block,
+						len, range.flags);
+					if (ret) {
+						f2fs_put_dnode(&dn);
+						goto out;
+					}
+
+					len = 0;
+				}
+			}
+
+			if (!len) {
+				prev_bdev = cur_bdev;
+				prev_index = index;
+				prev_block = blkaddr;
+				len = 1;
+			}
+		}
 
-	if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid))
-		return -EOVERFLOW;
+		f2fs_put_dnode(&dn);
 
-	err = f2fs_dquot_initialize(inode);
-	if (err)
-		return err;
+		if (fatal_signal_pending(current)) {
+			ret = -EINTR;
+			goto out;
+		}
+		cond_resched();
+	}
 
-	f2fs_lock_op(sbi);
-	err = f2fs_transfer_project_quota(inode, kprojid);
-	if (err)
-		goto out_unlock;
+	if (len)
+		ret = f2fs_secure_erase(prev_bdev, inode, prev_index,
+				prev_block, len, range.flags);
+out:
+	filemap_invalidate_unlock(mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+err:
+	inode_unlock(inode);
+	file_end_write(filp);
 
-	fi->i_projid = kprojid;
-	inode->i_ctime = current_time(inode);
-	f2fs_mark_inode_dirty_sync(inode, true);
-out_unlock:
-	f2fs_unlock_op(sbi);
-	return err;
-}
-#else
-int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid)
-{
-	return 0;
+	return ret;
 }
 
-static int f2fs_ioc_setproject(struct inode *inode, __u32 projid)
+static int f2fs_get_compress_option_v2(struct file *filp,
+				       unsigned long attr, __u16 *attr_size)
 {
-	if (projid != F2FS_DEF_PROJID)
+	struct inode *inode = file_inode(filp);
+	struct f2fs_comp_option_v2 option;
+
+	if (sizeof(option) < *attr_size)
+		*attr_size = sizeof(option);
+
+	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
 		return -EOPNOTSUPP;
-	return 0;
-}
-#endif
 
-int f2fs_fileattr_get(struct dentry *dentry, struct fileattr *fa)
-{
-	struct inode *inode = d_inode(dentry);
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	u32 fsflags = f2fs_iflags_to_fsflags(fi->i_flags);
+	inode_lock_shared(inode);
 
-	if (IS_ENCRYPTED(inode))
-		fsflags |= FS_ENCRYPT_FL;
-	if (IS_VERITY(inode))
-		fsflags |= FS_VERITY_FL;
-	if (f2fs_has_inline_data(inode) || f2fs_has_inline_dentry(inode))
-		fsflags |= FS_INLINE_DATA_FL;
-	if (is_inode_flag_set(inode, FI_PIN_FILE))
-		fsflags |= FS_NOCOW_FL;
+	if (!f2fs_compressed_file(inode)) {
+		inode_unlock_shared(inode);
+		return -ENODATA;
+	}
 
-	fileattr_fill_flags(fa, fsflags & F2FS_GETTABLE_FS_FL);
+	option.algorithm = F2FS_I(inode)->i_compress_algorithm;
+	option.log_cluster_size = F2FS_I(inode)->i_log_cluster_size;
+	option.level = F2FS_I(inode)->i_compress_level;
+	option.flag = F2FS_I(inode)->i_compress_flag;
 
-	if (f2fs_sb_has_project_quota(F2FS_I_SB(inode)))
-		fa->fsx_projid = from_kprojid(&init_user_ns, fi->i_projid);
+	inode_unlock_shared(inode);
+
+	if (copy_to_user((void __user *)attr, &option, *attr_size))
+		return -EFAULT;
 
 	return 0;
 }
 
-int f2fs_fileattr_set(struct user_namespace *mnt_userns,
-		      struct dentry *dentry, struct fileattr *fa)
+static int f2fs_ioc_get_compress_option(struct file *filp, unsigned long arg)
 {
-	struct inode *inode = d_inode(dentry);
-	u32 fsflags = fa->flags, mask = F2FS_SETTABLE_FS_FL;
-	u32 iflags;
-	int err;
-
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
-		return -EIO;
-	if (!f2fs_is_checkpoint_ready(F2FS_I_SB(inode)))
-		return -ENOSPC;
-	if (fsflags & ~F2FS_GETTABLE_FS_FL)
-		return -EOPNOTSUPP;
-	fsflags &= F2FS_SETTABLE_FS_FL;
-	if (!fa->flags_valid)
-		mask &= FS_COMMON_FL;
-
-	iflags = f2fs_fsflags_to_iflags(fsflags);
-	if (f2fs_mask_flags(inode->i_mode, iflags) != iflags)
-		return -EOPNOTSUPP;
+	__u16 size = sizeof(struct f2fs_comp_option);
 
-	err = f2fs_setflags_common(inode, iflags, f2fs_fsflags_to_iflags(mask));
-	if (!err)
-		err = f2fs_ioc_setproject(inode, fa->fsx_projid);
-
-	return err;
+	return f2fs_get_compress_option_v2(filp, arg, &size);
 }
 
-int f2fs_pin_file_control(struct inode *inode, bool inc)
+static int f2fs_set_compress_option_v2(struct file *filp,
+				       unsigned long attr, __u16 *attr_size)
 {
-	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_comp_option_v2 option;
+	/*
+	 * if compress_layout is not set, set file in fixed-input mode.
+	 * no need to shift COMPRESS_LEVEL
+	 */
+	short init_compr_flag = COMPRESS_FIXED_INPUT;
+	int ret = 0;
 
-	/* Use i_gc_failures for normal file as a risk signal. */
-	if (inc)
-		f2fs_i_gc_failures_write(inode,
-				fi->i_gc_failures[GC_FAILURE_PIN] + 1);
+	if (sizeof(option) < *attr_size)
+		*attr_size = sizeof(option);
 
-	if (fi->i_gc_failures[GC_FAILURE_PIN] > sbi->gc_pin_file_threshold) {
-		f2fs_warn(sbi, "%s: Enable GC = ino %lx after %x GC trials",
-			  __func__, inode->i_ino,
-			  fi->i_gc_failures[GC_FAILURE_PIN]);
-		clear_inode_flag(inode, FI_PIN_FILE);
-		return -EAGAIN;
-	}
-	return 0;
-}
+	if (!f2fs_sb_has_compression(sbi))
+		return -EOPNOTSUPP;
 
-static int f2fs_ioc_set_pin_file(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
-	__u32 pin;
-	int ret = 0;
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
 
-	if (get_user(pin, (__u32 __user *)arg))
+	if (copy_from_user(&option, (void __user *)attr, *attr_size))
 		return -EFAULT;
 
-	if (!S_ISREG(inode->i_mode))
+	if (option.log_cluster_size < MIN_COMPRESS_LOG_SIZE ||
+		option.log_cluster_size > MAX_COMPRESS_LOG_SIZE ||
+		option.algorithm >= COMPRESS_MAX)
 		return -EINVAL;
 
-	if (f2fs_readonly(F2FS_I_SB(inode)->sb))
-		return -EROFS;
-
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	if (*attr_size == sizeof(struct f2fs_comp_option_v2)) {
+		if (!f2fs_is_compress_level_valid(option.algorithm,
+						  option.level))
+			return -EINVAL;
+		/* fix coverity error: Operands don't affect result, COMPRESS_MAX_FLAG==9, always false*/
+		//if (option.flag > BIT(COMPRESS_MAX_FLAG) - 1)
+		//	return -EINVAL;
+	}
 
+	file_start_write(filp);
 	inode_lock(inode);
 
-	if (f2fs_is_atomic_file(inode)) {
+	f2fs_down_write(&F2FS_I(inode)->i_sem);
+	if (!f2fs_compressed_file(inode)) {
 		ret = -EINVAL;
 		goto out;
 	}
 
-	if (!pin) {
-		clear_inode_flag(inode, FI_PIN_FILE);
-		f2fs_i_gc_failures_write(inode, 0);
-		goto done;
-	}
-
-	if (f2fs_should_update_outplace(inode, NULL)) {
-		ret = -EINVAL;
+	if (f2fs_is_mmap_file(inode) || get_dirty_pages(inode)) {
+		ret = -EBUSY;
 		goto out;
 	}
 
-	if (f2fs_pin_file_control(inode, false)) {
-		ret = -EAGAIN;
+	if (F2FS_HAS_BLOCKS(inode)) {
+		if (*attr_size == sizeof(struct f2fs_comp_option_v2) &&
+		    F2FS_I(inode)->i_compress_algorithm == option.algorithm &&
+		    F2FS_I(inode)->i_log_cluster_size == option.log_cluster_size &&
+		    F2FS_I(inode)->i_compress_level == option.level) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+			if (option.flag & COMPRESS_ATIME_MASK) {
+				F2FS_I(inode)->i_compress_flag |= BIT(COMPRESS_ATIME);
+				inode->i_atime = current_time(inode);
+			}
+#endif
+			goto mark_dirty;
+		}
+		ret = -EFBIG;
 		goto out;
 	}
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		goto out;
-
-	if (!f2fs_disable_compressed_file(inode)) {
-		ret = -EOPNOTSUPP;
-		goto out;
+	F2FS_I(inode)->i_compress_algorithm = option.algorithm;
+	F2FS_I(inode)->i_log_cluster_size = option.log_cluster_size;
+	F2FS_I(inode)->i_cluster_size = BIT(option.log_cluster_size);
+	if (F2FS_I(inode)->i_compress_flag & COMPRESS_CHKSUM_MASK)
+		init_compr_flag |= BIT(COMPRESS_CHKSUM);
+	if (F2FS_I(inode)->i_compress_flag & COMPRESS_ATIME_MASK)
+		init_compr_flag |= BIT(COMPRESS_ATIME);
+	F2FS_I(inode)->i_compress_flag = init_compr_flag;
+	if (*attr_size == sizeof(struct f2fs_comp_option_v2)) {
+		F2FS_I(inode)->i_compress_level = option.level;
+		F2FS_I(inode)->i_compress_flag = option.flag;
+		if (f2fs_compress_layout(inode) == COMPRESS_FIXED_OUTPUT)
+			F2FS_I(inode)->i_compress_flag &= ~COMPRESS_CHKSUM_MASK;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (option.flag & COMPRESS_ATIME_MASK)
+			inode->i_atime = current_time(inode);
+#endif
 	}
+mark_dirty:
+	f2fs_mark_inode_dirty_sync(inode, true);
 
-	set_inode_flag(inode, FI_PIN_FILE);
-	ret = F2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN];
-done:
-	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	if (!f2fs_is_compress_backend_ready(inode))
+		f2fs_warn(sbi, "compression algorithm is successfully set, "
+			"but current kernel doesn't support this algorithm.");
 out:
+	f2fs_up_write(&F2FS_I(inode)->i_sem);
 	inode_unlock(inode);
-	mnt_drop_write_file(filp);
+	file_end_write(filp);
+
 	return ret;
 }
 
-static int f2fs_ioc_get_pin_file(struct file *filp, unsigned long arg)
+static int f2fs_ioc_set_compress_option(struct file *filp, unsigned long arg)
 {
-	struct inode *inode = file_inode(filp);
-	__u32 pin = 0;
+	__u16 size = sizeof(struct f2fs_comp_option);
 
-	if (is_inode_flag_set(inode, FI_PIN_FILE))
-		pin = F2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN];
-	return put_user(pin, (u32 __user *)arg);
+	return f2fs_set_compress_option_v2(filp, arg, &size);
 }
 
-int f2fs_precache_extents(struct inode *inode)
+static int redirty_blocks(struct inode *inode, pgoff_t page_idx, int len)
 {
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	struct f2fs_map_blocks map;
-	pgoff_t m_next_extent;
-	loff_t end;
-	int err;
+	DEFINE_READAHEAD(ractl, NULL, NULL, inode->i_mapping, page_idx);
+	struct address_space *mapping = inode->i_mapping;
+	struct page *page;
+	pgoff_t redirty_idx = page_idx;
+	int i, page_len = 0, ret = 0;
 
-	if (is_inode_flag_set(inode, FI_NO_EXTENT))
-		return -EOPNOTSUPP;
+	page_cache_ra_unbounded(&ractl, len, 0);
 
-	map.m_lblk = 0;
-	map.m_pblk = 0;
-	map.m_next_pgofs = NULL;
-	map.m_next_extent = &m_next_extent;
-	map.m_seg_type = NO_CHECK_TYPE;
-	map.m_may_create = false;
-	end = max_file_blocks(inode);
+	for (i = 0; i < len; i++, page_idx++) {
+		if (time_to_inject(F2FS_M_SB(mapping), FAULT_COMPRESS_REDIRTY)) {
+			ret = -ENOMEM;
+			break;
+		}
+		page = read_cache_page(mapping, page_idx, NULL, NULL);
+		if (IS_ERR(page)) {
+			ret = PTR_ERR(page);
+			break;
+		}
+		page_len++;
+	}
 
-	while (map.m_lblk < end) {
-		map.m_len = end - map.m_lblk;
+	for (i = 0; i < page_len; i++, redirty_idx++) {
+		page = find_lock_page(mapping, redirty_idx);
 
-		f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRECACHE);
-		f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
-		if (err)
-			return err;
+		/* It will never fail, when page has pinned above */
+		f2fs_bug_on(F2FS_I_SB(inode), !page);
+
+		f2fs_wait_on_page_writeback(page, DATA, true, true);
+
+		set_page_dirty(page);
+		f2fs_put_page(page, 1);
+		f2fs_put_page(page, 0);
+	}
+
+	return ret;
+}
+
+int f2fs_decompress_inode(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	pgoff_t page_idx = 0, last_idx, cluster_idx;
+	unsigned int blk_per_seg = sbi->blocks_per_seg;
+	int ret;
+
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
+		return -EPERM;
 
-		map.m_lblk = m_next_extent;
-	}
+	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+	if (ret)
+		return ret;
 
-	return 0;
-}
+	if (!atomic_read(&fi->i_compr_blocks))
+		return 0;
 
-static int f2fs_ioc_precache_extents(struct file *filp)
-{
-	return f2fs_precache_extents(file_inode(filp));
-}
+	f2fs_info(sbi, "start decompress ino %lu size %llu blocks %llu "
+		"cblocks %d caller %ps\n", inode->i_ino, i_size_read(inode),
+		inode->i_blocks, atomic_read(&F2FS_I(inode)->i_compr_blocks),
+		__builtin_return_address(0));
 
-static int f2fs_ioc_resize_fs(struct file *filp, unsigned long arg)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));
-	__u64 block_count;
+	clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+	inode->i_ctime = current_time(inode);
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	last_idx >>= fi->i_log_cluster_size;
 
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+	for (cluster_idx = 0; cluster_idx < last_idx; cluster_idx++) {
+		page_idx = cluster_idx << fi->i_log_cluster_size;
 
-	if (copy_from_user(&block_count, (void __user *)arg,
-			   sizeof(block_count)))
-		return -EFAULT;
+		if (!f2fs_is_compressed_cluster(inode, page_idx))
+			continue;
 
-	return f2fs_resize_fs(filp, block_count);
-}
+		ret = redirty_blocks(inode, page_idx, fi->i_cluster_size);
+		if (ret < 0)
+			break;
 
-static int f2fs_ioc_enable_verity(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
+		if (get_dirty_pages(inode) >= blk_per_seg) {
+			ret = filemap_fdatawrite(inode->i_mapping);
+			if (ret < 0)
+				break;
+		}
 
-	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+		cond_resched();
+		if (fatal_signal_pending(current)) {
+			ret = -EINTR;
+			break;
+		}
+	}
 
-	if (!f2fs_sb_has_verity(F2FS_I_SB(inode))) {
-		f2fs_warn(F2FS_I_SB(inode),
-			  "Can't enable fs-verity on inode %lu: the verity feature is not enabled on this filesystem",
-			  inode->i_ino);
-		return -EOPNOTSUPP;
+	if (!ret) {
+		if (time_to_inject(sbi, FAULT_COMPRESS_WRITEBACK)) {
+			ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+							i_size_read(inode) / 2);
+			if (!ret)
+				ret = -EIO;
+			goto out;
+		}
+		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+							LLONG_MAX);
 	}
 
-	return fsverity_ioctl_enable(filp, (const void __user *)arg);
+out:
+	if (ret)
+		f2fs_warn(sbi, "%s: The file might be partially decompressed (errno=%d). Please delete the file.",
+			  __func__, ret);
+	f2fs_info(sbi, "end decompress ino %lu size %llu blocks %llu cblocks %d ret %d\n",
+		inode->i_ino, i_size_read(inode), inode->i_blocks,
+		atomic_read(&F2FS_I(inode)->i_compr_blocks), ret);
+
+	return ret;
 }
 
-static int f2fs_ioc_measure_verity(struct file *filp, unsigned long arg)
+static int f2fs_ioc_decompress_file(struct file *filp)
 {
-	if (!f2fs_sb_has_verity(F2FS_I_SB(file_inode(filp))))
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	int ret;
+
+	if (!f2fs_sb_has_compression(sbi) ||
+			F2FS_OPTION(sbi).compress_mode != COMPR_MODE_USER)
 		return -EOPNOTSUPP;
 
-	return fsverity_ioctl_measure(filp, (void __user *)arg);
-}
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
 
-static int f2fs_ioc_read_verity_metadata(struct file *filp, unsigned long arg)
-{
-	if (!f2fs_sb_has_verity(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	if (!f2fs_compressed_file(inode))
+		return -EINVAL;
 
-	return fsverity_ioctl_read_metadata(filp, (const void __user *)arg);
-}
+	f2fs_balance_fs(F2FS_I_SB(inode), true);
 
-static int f2fs_ioc_getfslabel(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	char *vbuf;
-	int count;
-	int err = 0;
+	file_start_write(filp);
+	inode_lock(inode);
 
-	vbuf = f2fs_kzalloc(sbi, MAX_VOLUME_NAME, GFP_KERNEL);
-	if (!vbuf)
-		return -ENOMEM;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_deduped_inode(inode)) {
+		ret = -EACCES;
+		goto out;
+	}
+#endif
 
-	f2fs_down_read(&sbi->sb_lock);
-	count = utf16s_to_utf8s(sbi->raw_super->volume_name,
-			ARRAY_SIZE(sbi->raw_super->volume_name),
-			UTF16_LITTLE_ENDIAN, vbuf, MAX_VOLUME_NAME);
-	f2fs_up_read(&sbi->sb_lock);
+	if (!f2fs_is_compress_backend_ready(inode)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
 
-	if (copy_to_user((char __user *)arg, vbuf,
-				min(FSLABEL_MAX, count)))
-		err = -EFAULT;
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
-	kfree(vbuf);
-	return err;
+	ret = f2fs_decompress_inode(inode);
+out:
+	inode_unlock(inode);
+	file_end_write(filp);
+
+	return ret;
 }
 
-static int f2fs_ioc_setfslabel(struct file *filp, unsigned long arg)
+static int f2fs_ioc_compress_file(struct file *filp)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	char *vbuf;
-	int err = 0;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	pgoff_t page_idx = 0, last_idx, cluster_idx;
+	unsigned int blk_per_seg = sbi->blocks_per_seg;
+	int ret;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	if (!f2fs_sb_has_compression(sbi) || !may_compress ||
+			F2FS_OPTION(sbi).compress_mode != COMPR_MODE_USER)
+		return -EOPNOTSUPP;
 
-	vbuf = strndup_user((const char __user *)arg, FSLABEL_MAX);
-	if (IS_ERR(vbuf))
-		return PTR_ERR(vbuf);
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
 
-	err = mnt_want_write_file(filp);
-	if (err)
-		goto out;
+	if (!f2fs_compressed_file(inode))
+		return -EINVAL;
 
-	f2fs_down_write(&sbi->sb_lock);
+	f2fs_balance_fs(sbi, true);
 
-	memset(sbi->raw_super->volume_name, 0,
-			sizeof(sbi->raw_super->volume_name));
-	utf8s_to_utf16s(vbuf, strlen(vbuf), UTF16_LITTLE_ENDIAN,
-			sbi->raw_super->volume_name,
-			ARRAY_SIZE(sbi->raw_super->volume_name));
+	file_start_write(filp);
+	inode_lock(inode);
 
-	err = f2fs_commit_super(sbi, false);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_deduped_inode(inode)) {
+		ret = -EACCES;
+		goto out;
+	}
+#endif
 
-	f2fs_up_write(&sbi->sb_lock);
+	if (!f2fs_is_compress_backend_ready(inode)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
 
-	mnt_drop_write_file(filp);
-out:
-	kfree(vbuf);
-	return err;
-}
+	f2fs_info(sbi, "compress ino %lu (%pd) size %llu blocks %llu released %d\n",
+		inode->i_ino, file_dentry(filp), i_size_read(inode),
+		inode->i_blocks, is_inode_flag_set(inode, FI_COMPRESS_RELEASED));
 
-static int f2fs_get_compress_blocks(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
-	__u64 blocks;
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
-	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
-		return -EOPNOTSUPP;
+	inode->i_ctime = current_time(inode);
 
-	if (!f2fs_compressed_file(inode))
-		return -EINVAL;
+	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+	if (ret)
+		goto out;
 
-	blocks = atomic_read(&F2FS_I(inode)->i_compr_blocks);
-	return put_user(blocks, (u64 __user *)arg);
-}
+	set_inode_flag(inode, FI_ENABLE_COMPRESS);
 
-static int release_compress_blocks(struct dnode_of_data *dn, pgoff_t count)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
-	unsigned int released_blocks = 0;
-	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
-	block_t blkaddr;
-	int i;
+	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	last_idx >>= fi->i_log_cluster_size;
 
-	for (i = 0; i < count; i++) {
-		blkaddr = data_blkaddr(dn->inode, dn->node_page,
-						dn->ofs_in_node + i);
+	for (cluster_idx = 0; cluster_idx < last_idx; cluster_idx++) {
+		page_idx = cluster_idx << fi->i_log_cluster_size;
 
-		if (!__is_valid_data_blkaddr(blkaddr))
+		if (f2fs_is_sparse_cluster(inode, page_idx))
 			continue;
-		if (unlikely(!f2fs_is_valid_blkaddr(sbi, blkaddr,
-					DATA_GENERIC_ENHANCE))) {
-			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-			return -EFSCORRUPTED;
+
+		ret = redirty_blocks(inode, page_idx, fi->i_cluster_size);
+		if (ret < 0)
+			break;
+
+		if (get_dirty_pages(inode) >= blk_per_seg) {
+			ret = filemap_fdatawrite(inode->i_mapping);
+			if (ret < 0)
+				break;
+		}
+
+		cond_resched();
+		if (fatal_signal_pending(current)) {
+			ret = -EINTR;
+			break;
+		}
+	}
+
+	if (!ret) {
+		if (time_to_inject(sbi, FAULT_COMPRESS_WRITEBACK)) {
+			ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+							i_size_read(inode) / 2);
+			if (!ret)
+				ret = -EIO;
+			goto next;
 		}
+		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+							LLONG_MAX);
+	}
+next:
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (!atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
+		clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+		F2FS_I(inode)->i_flags |= F2FS_NOCOMP_FL;
+		f2fs_mark_inode_dirty_sync(inode, true);
 	}
+#else
+	clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+#endif
+
+	if (ret)
+		f2fs_warn(sbi, "%s: The file might be partially compressed (errno=%d). Please delete the file.",
+			  __func__, ret);
+out:
+	f2fs_info(sbi, "end compress ino %lu (%pd) size %llu blocks %llu cblocks %d ret %d\n",
+		inode->i_ino, file_dentry(filp), i_size_read(inode),
+		inode->i_blocks, atomic_read(&F2FS_I(inode)->i_compr_blocks), ret);
+
+	inode_unlock(inode);
+	file_end_write(filp);
+
+	return ret;
+}
+
+#ifdef CONFIG_F2FS_APPBOOST
+#define BOOST_MAX_FILES 1019
+#define BOOST_FILE_STATE_FINISH 1
+#define F2FS_BOOSTFILE_VERSION 0xF2F5
+#define BOOSTFILE_MAX_BITMAP (1<<20)
+#define PRELOAD_MAX_TIME	(2000)
+
+/* structure on disk */
+struct merge_summary_dinfo {
+	__le32 num;
+	__le32 version;
+	__le32 state;
+	__le32 tail;
+	__le32 checksum;
+	__le32 fsize[BOOST_MAX_FILES];
+};
+
+struct merge_extent_dinfo {
+	__le32 index;
+	__le32 length;
+	__le32 index_in_mfile;
+};
 
-	while (count) {
-		int compr_blocks = 0;
+struct merge_file_dinfo {
+	__le32 ino;
+	__le32 extent_count;
+	__le32 i_generation;
+	__le32 REV;
+	__le64 mtime;
+	struct merge_extent_dinfo extents[0];
+};
 
-		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
-			blkaddr = f2fs_data_blkaddr(dn);
+/* inmem manage structure */
+struct merge_summary {
+	int num;
+	int version;
+	int state;
+	int tail;
+	u32 checksum;
+	int fsize[BOOST_MAX_FILES];
+};
 
-			if (i == 0) {
-				if (blkaddr == COMPRESS_ADDR)
-					continue;
-				dn->ofs_in_node += cluster_size;
-				goto next;
-			}
+struct merge_extent {
+	unsigned index;
+	unsigned length;
+	unsigned index_in_mfile;
+};
 
-			if (__is_valid_data_blkaddr(blkaddr))
-				compr_blocks++;
+struct merge_file {
+	unsigned ino;
+	unsigned extent_count;
+	unsigned i_generation;
+	unsigned REV;
+	u64 mtime;
+	struct merge_extent extents[0];
+};
 
-			if (blkaddr != NEW_ADDR)
-				continue;
+/* manage structure in f2fs inode info */
+struct fi_merge_manage {
+	int num;
+	unsigned long cur_blocks;
+	struct list_head list;
+};
 
-			f2fs_set_data_blkaddr(dn, NULL_ADDR);
-		}
+struct file_list_node {
+	struct list_head list;
+	struct list_head ext_list;
+	u64 bitmax;
+	unsigned long *bitmap;
+	struct merge_file merge_file;
+};
 
-		f2fs_i_compr_blocks_update(dn->inode, compr_blocks, false);
-		dec_valid_block_count(sbi, dn->inode,
-					cluster_size - compr_blocks);
+struct extent_list_node {
+	struct list_head list;
+	struct merge_extent extent;
+};
 
-		released_blocks += cluster_size - compr_blocks;
-next:
-		count -= cluster_size;
-	}
+static bool f2fs_appboost_enable(struct f2fs_sb_info *sbi)
+{
+	return sbi->appboost;
+}
 
-	return released_blocks;
+static unsigned int f2fs_appboost_maxblocks(struct f2fs_sb_info *sbi)
+{
+	return sbi->appboost_max_blocks;
 }
 
-static int f2fs_release_compress_blocks(struct file *filp, unsigned long arg)
+static int f2fs_file_read(struct file *file, loff_t offset, unsigned char *data, unsigned int size)
 {
-	struct inode *inode = file_inode(filp);
+	struct inode *inode = file_inode(file);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	pgoff_t page_idx = 0, last_idx;
-	unsigned int released_blocks = 0;
-	int ret;
-	int writecount;
+	if (time_to_inject(sbi, FAULT_READ_ERROR)) {
+		return -EIO;
+	}
+	return kernel_read(file, data, size, &offset);
+}
 
-	if (!f2fs_sb_has_compression(sbi))
-		return -EOPNOTSUPP;
+static int f2fs_file_write(struct file *file, loff_t off, unsigned char *data, unsigned int size)
+{
+	struct inode *inode = file_inode(file);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct address_space *mapping = inode->i_mapping;
+	const struct address_space_operations *a_ops = mapping->a_ops;
+	loff_t offset = off & (PAGE_SIZE - 1);
+	size_t towrite = size;
+	struct page *page;
+	void *fsdata = NULL;
+	char *kaddr;
+	int err = 0;
+	int tocopy;
 
-	if (!f2fs_compressed_file(inode))
-		return -EINVAL;
+	if (time_to_inject(sbi, FAULT_WRITE_ERROR)) {
+		return -EIO;
+	}
+	// if no set this, prepare_write_begin will return 0 directly and get the error block
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 15, 0)
+	set_inode_flag(inode, FI_NO_PREALLOC);
+#endif
+	while (towrite > 0) {
+		tocopy = min_t(unsigned long, PAGE_SIZE - offset, towrite);
+retry:
+		err = a_ops->write_begin(NULL, mapping, off, tocopy,
+								&page, &fsdata);
+		if (unlikely(err)) {
+			if (err == -ENOMEM) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 15, 0)
+				congestion_wait(BLK_RW_ASYNC,
+							DEFAULT_IO_TIMEOUT);
+#else
+				f2fs_io_schedule_timeout(DEFAULT_IO_TIMEOUT);
+#endif
+				goto retry;
+			}
+			break;
+		}
 
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+		kaddr = kmap_atomic(page);
+		memcpy(kaddr + offset, data, tocopy);
+		kunmap_atomic(kaddr);
+		flush_dcache_page(page);
+		a_ops->write_end(NULL, mapping, off, tocopy, tocopy,
+							page, fsdata);
+		offset = 0;
+		towrite -= tocopy;
+		off += tocopy;
+		data += tocopy;
+		cond_resched();
+	}
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 15, 0)
+	clear_inode_flag(inode, FI_NO_PREALLOC);
+#endif
+	if (size == towrite)
+		return err;
 
-	f2fs_balance_fs(sbi, true);
+	inode->i_mtime = inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, false);
 
-	inode_lock(inode);
+	return size - towrite;
+}
 
-	writecount = atomic_read(&inode->i_writecount);
-	if ((filp->f_mode & FMODE_WRITE && writecount != 1) ||
-			(!(filp->f_mode & FMODE_WRITE) && writecount)) {
-		ret = -EBUSY;
-		goto out;
+static struct fi_merge_manage *f2fs_init_merge_manage(struct inode *inode)
+{
+	struct fi_merge_manage *fmm;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	fmm = f2fs_kmalloc(sbi, sizeof(struct fi_merge_manage), GFP_KERNEL);
+	if (!fmm) {
+		return NULL;
 	}
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
-		ret = -EINVAL;
-		goto out;
-	}
+	INIT_LIST_HEAD(&fmm->list);
+	fmm->num = 0;
+	fmm->cur_blocks = 0;
 
-	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
-	if (ret)
-		goto out;
+	return fmm;
+}
 
-	set_inode_flag(inode, FI_COMPRESS_RELEASED);
-	inode->i_ctime = current_time(inode);
-	f2fs_mark_inode_dirty_sync(inode, true);
+void f2fs_boostfile_free(struct inode *inode)
+{
+	struct fi_merge_manage *fmm;
+	struct file_list_node *fm_node, *fm_tmp;
+	struct extent_list_node *fm_ext_node, *fm_ext_tmp;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
 
-	if (!atomic_read(&F2FS_I(inode)->i_compr_blocks))
-		goto out;
+	if (!fi->i_boostfile) {
+		return;
+	}
 
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(inode->i_mapping);
+	fmm = (struct fi_merge_manage *)fi->i_boostfile;
+	list_for_each_entry_safe(fm_node, fm_tmp, &fmm->list, list) {
+		kvfree(fm_node->bitmap);
 
-	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+		list_for_each_entry_safe(fm_ext_node, fm_ext_tmp, &fm_node->ext_list, list) {
+			list_del(&fm_ext_node->list);
+			kfree(fm_ext_node);
+		}
 
-	while (page_idx < last_idx) {
-		struct dnode_of_data dn;
-		pgoff_t end_offset, count;
+		list_del(&fm_node->list);
+		kfree(fm_node);
+	}
+	kfree(fmm);
+	fi->i_boostfile = NULL;
+}
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		ret = f2fs_get_dnode_of_data(&dn, page_idx, LOOKUP_NODE);
-		if (ret) {
-			if (ret == -ENOENT) {
-				page_idx = f2fs_get_next_page_offset(&dn,
-								page_idx);
-				ret = 0;
-				continue;
-			}
-			break;
+static struct file_list_node *f2fs_search_merge_file(struct fi_merge_manage *fmm, unsigned ino)
+{
+	struct file_list_node *fm_node, *fm_tmp;
+
+	list_for_each_entry_safe(fm_node, fm_tmp, &fmm->list, list) {
+		if (fm_node->merge_file.ino == ino) {
+			return fm_node;
 		}
+	}
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-		count = min(end_offset - dn.ofs_in_node, last_idx - page_idx);
-		count = round_up(count, F2FS_I(inode)->i_cluster_size);
+	return NULL;
+}
 
-		ret = release_compress_blocks(&dn, count);
+static int _f2fs_insert_merge_extent(struct f2fs_sb_info* sbi,
+				     struct file_list_node *fm_node, unsigned start,
+				     unsigned end, struct fi_merge_manage *fmm, u32 max_blocks)
+{
+	struct extent_list_node *ext_node;
+	struct extent_list_node *ext_tail;
+	unsigned length;
 
-		f2fs_put_dnode(&dn);
+	if (fmm->cur_blocks >= max_blocks)
+		return -EOVERFLOW;
 
-		if (ret < 0)
-			break;
+	// update the end
+	if (((end - start + 1) + fmm->cur_blocks) >= max_blocks)
+		end = (max_blocks - fmm->cur_blocks) + start - 1;
 
-		page_idx += count;
-		released_blocks += ret;
+	length = end - start + 1;
+	ext_tail = list_last_entry(&(fm_node->ext_list), struct extent_list_node, list);
+	if (ext_tail) {
+		if (start == (ext_tail->extent.index + ext_tail->extent.length)) {
+			ext_tail->extent.length += length;
+			fmm->cur_blocks += length;
+			return 0;
+		}
 	}
 
-	filemap_invalidate_unlock(inode->i_mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-out:
-	inode_unlock(inode);
+	ext_node = (struct extent_list_node *)f2fs_kmalloc(sbi, sizeof(struct extent_list_node), GFP_KERNEL);
+	if (!ext_node) {
+		return -ENOMEM;
+	}
+	ext_node->extent.index = start;
+	ext_node->extent.length = length;
+	ext_node->extent.index_in_mfile = 0;
 
-	mnt_drop_write_file(filp);
+	INIT_LIST_HEAD(&ext_node->list);
+	list_add_tail(&(ext_node->list), &(fm_node->ext_list));
+	fm_node->merge_file.extent_count++;
+	fmm->cur_blocks += length;
 
-	if (ret >= 0) {
-		ret = put_user(released_blocks, (u64 __user *)arg);
-	} else if (released_blocks &&
-			atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		f2fs_warn(sbi, "%s: partial blocks were released i_ino=%lx "
-			"iblocks=%llu, released=%u, compr_blocks=%u, "
-			"run fsck to fix.",
-			__func__, inode->i_ino, inode->i_blocks,
-			released_blocks,
-			atomic_read(&F2FS_I(inode)->i_compr_blocks));
-	}
+	return 0;
 
-	return ret;
 }
 
-static int reserve_compress_blocks(struct dnode_of_data *dn, pgoff_t count,
-		unsigned int *reserved_blocks)
+static int f2fs_insert_merge_extent(struct fi_merge_manage *fmm, struct f2fs_sb_info *sbi,
+				 struct file_list_node *fm_node, struct merge_extent *fm_ext)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
-	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
-	block_t blkaddr;
-	int i;
-
-	for (i = 0; i < count; i++) {
-		blkaddr = data_blkaddr(dn->inode, dn->node_page,
-						dn->ofs_in_node + i);
+	unsigned low = fm_ext->index;
+	unsigned high = low + fm_ext->length - 1;
+	unsigned start = 1, end = 0;
+	unsigned i;
+	bool spliting = false;
+	int ret = 0;
 
-		if (!__is_valid_data_blkaddr(blkaddr))
-			continue;
-		if (unlikely(!f2fs_is_valid_blkaddr(sbi, blkaddr,
-					DATA_GENERIC_ENHANCE))) {
-			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-			return -EFSCORRUPTED;
-		}
+	if (high >= fm_node->bitmax) {
+		f2fs_warn(sbi, "f2fs_insert_merge_extent range bad value [%u,%u,%llu]",
+							low, high, fm_node->bitmax);
+		return -EINVAL;
 	}
 
-	while (count) {
-		int compr_blocks = 0;
-		blkcnt_t reserved = 0;
-		blkcnt_t to_reserved;
-		int ret;
-
-		for (i = 0; i < cluster_size; i++) {
-			blkaddr = data_blkaddr(dn->inode, dn->node_page,
-						dn->ofs_in_node + i);
-
-			if (i == 0) {
-				if (blkaddr != COMPRESS_ADDR) {
-					dn->ofs_in_node += cluster_size;
-					goto next;
-				}
-				continue;
-			}
+	for (i = low; i <= high; i++) {
+		if (test_and_set_bit(i, fm_node->bitmap)) {
+			if (spliting) {
+				ret = _f2fs_insert_merge_extent(sbi, fm_node, start,
+						end, fmm, f2fs_appboost_maxblocks(sbi));
+				if (ret)
+					return ret;
 
-			/*
-			 * compressed cluster was not released due to it
-			 * fails in release_compress_blocks(), so NEW_ADDR
-			 * is a possible case.
-			 */
-			if (blkaddr == NEW_ADDR) {
-				reserved++;
+				// reset
+				spliting = false;
+				start = 1;
+				end = 0;
+			} else {
 				continue;
 			}
-			if (__is_valid_data_blkaddr(blkaddr)) {
-				compr_blocks++;
+		} else {
+			if (spliting) {
+				end = i;
 				continue;
+			} else {
+				start = i;
+				end = i;
+				spliting = true;
 			}
 		}
+	}
 
-		to_reserved = cluster_size - compr_blocks - reserved;
+	if (end >= start && spliting)
+		ret = _f2fs_insert_merge_extent(sbi, fm_node, start,
+						end, fmm, f2fs_appboost_maxblocks(sbi));
 
-		/* for the case all blocks in cluster were reserved */
-		if (reserved && to_reserved == 1) {
-			dn->ofs_in_node += cluster_size;
-			goto next;
-		}
+	return ret;
+}
 
-		ret = inc_valid_block_count(sbi, dn->inode,
-						&to_reserved, false);
-		if (unlikely(ret))
-			return ret;
+static int f2fs_insert_merge_file_user(struct fi_merge_manage *fmm, struct f2fs_sb_info *sbi,
+					struct file_list_node *fm_node, struct merge_file_user *fm_u)
+{
+	int ret = 0;
+	struct merge_extent *fm_ext = NULL;
+	int i;
 
-		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
-			if (f2fs_data_blkaddr(dn) == NULL_ADDR)
-				f2fs_set_data_blkaddr(dn, NEW_ADDR);
-		}
+	fm_ext = (struct merge_extent *)f2fs_kvmalloc(sbi,
+				sizeof(struct merge_extent) * fm_u->extent_count, GFP_KERNEL);
+	if (!fm_ext) {
+		ret = -ENOMEM;
+		goto fail;
+	}
 
-		f2fs_i_compr_blocks_update(dn->inode, compr_blocks, true);
+	if (copy_from_user(fm_ext, (struct merge_extent __user *)fm_u->extents,
+		sizeof(struct merge_extent) * fm_u->extent_count)) {
+		ret = -EFAULT;
+		goto fail;
+	}
 
-		*reserved_blocks += to_reserved;
-next:
-		count -= cluster_size;
+	for (i = 0; i < fm_u->extent_count; i++) {
+		if (fm_ext[i].length == 0) {
+			f2fs_warn(sbi, "f2fs_ioc_merge_user check ext length == 0!");
+			ret = -EINVAL;
+			goto fail;
+		}
+
+		ret = f2fs_insert_merge_extent(fmm, sbi, fm_node, &fm_ext[i]);
+		if (ret) {
+			goto fail;
+		}
 	}
 
-	return 0;
+fail:
+	if (fm_ext)
+		kvfree(fm_ext);
+
+	return ret;
 }
 
-static int f2fs_reserve_compress_blocks(struct file *filp, unsigned long arg)
+static int f2fs_ioc_start_file_merge(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	pgoff_t page_idx = 0, last_idx;
-	unsigned int reserved_blocks = 0;
-	int ret;
-
-	if (!f2fs_sb_has_compression(sbi))
-		return -EOPNOTSUPP;
-
-	if (!f2fs_compressed_file(inode))
-		return -EINVAL;
+	struct inode *inode_source;
+	struct fi_merge_manage *fmm;
+	struct file_list_node *fm_node;
+	struct merge_file_user fm_u;
+	int ret = 0;
+	loff_t i_size;
 
 	if (f2fs_readonly(sbi->sb))
 		return -EROFS;
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	if  (!f2fs_appboost_enable(sbi))
+		return -ENOTTY;
 
-	f2fs_balance_fs(sbi, true);
+	if (!inode_trylock(inode))
+		return -EAGAIN;
+
+	if (!(filp->f_flags & __O_TMPFILE)) {
+		f2fs_warn(sbi, "f2fs_ioc_start_file_merge check flags failed!");
+		inode_unlock(inode);
+		return -EINVAL;
+	}
 
-	inode_lock(inode);
+	if (!fi->i_boostfile) {
+		fi->i_boostfile = f2fs_init_merge_manage(inode);
+		if (!fi->i_boostfile) {
+			f2fs_warn(sbi, "f2fs_ioc_start_file_merge init private failed!");
+			inode_unlock(inode);
+			return -ENOMEM;
+		}
+	}
 
-	if (!is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
-		ret = -EINVAL;
-		goto unlock_inode;
+	fmm = fi->i_boostfile;
+	if (fmm->num >= BOOST_MAX_FILES) {
+		f2fs_warn(sbi, "f2fs_ioc_start_file_merge num overflow!");
+		ret = -EFAULT;
+		goto fail;
 	}
 
-	if (atomic_read(&F2FS_I(inode)->i_compr_blocks))
-		goto unlock_inode;
+	if (copy_from_user(&fm_u, (struct merge_file_user __user *)arg,
+		sizeof(struct merge_file_user))) {
+		ret = -EFAULT;
+		goto fail;
+	}
 
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(inode->i_mapping);
+	fm_node = f2fs_search_merge_file(fmm, fm_u.ino);
+	if (!fm_node) {
+		inode_source = f2fs_iget(sbi->sb, fm_u.ino);
+		if (IS_ERR(inode_source)) {
+			ret = PTR_ERR(inode_source);
+			f2fs_warn(sbi, "f2fs_ioc_start_file_merge no found ino=%d", fm_u.ino);
+			goto fail;
+		}
 
-	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+		if (is_bad_inode(inode_source)) {
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
 
-	while (page_idx < last_idx) {
-		struct dnode_of_data dn;
-		pgoff_t end_offset, count;
+		if (!inode_trylock(inode_source)) {
+			iput(inode_source);
+			ret = -EAGAIN;
+			goto fail;
+		}
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		ret = f2fs_get_dnode_of_data(&dn, page_idx, LOOKUP_NODE);
-		if (ret) {
-			if (ret == -ENOENT) {
-				page_idx = f2fs_get_next_page_offset(&dn,
-								page_idx);
-				ret = 0;
-				continue;
-			}
-			break;
+		i_size = i_size_read(inode_source);
+		if (DIV_ROUND_UP(i_size, PAGE_SIZE) > BOOSTFILE_MAX_BITMAP) {
+			f2fs_warn(sbi, "f2fs_ioc_start_file_merge ino=%d, i_size=%lld", fm_u.ino, i_size);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EFAULT;
+			goto fail;
 		}
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-		count = min(end_offset - dn.ofs_in_node, last_idx - page_idx);
-		count = round_up(count, F2FS_I(inode)->i_cluster_size);
+		if (fm_u.mtime != timespec64_to_ns(&inode_source->i_mtime) ||
+			fm_u.i_generation != inode_source->i_generation) {
+			f2fs_warn(sbi, "f2fs_ioc_start_file_merge EKEYEXPIRED ino=%d", fm_u.ino);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_inode_support_dedup(sbi, inode_source)
+		    && !is_inode_flag_set(inode_source, FI_DATA_UN_MODIFY))
+			set_inode_flag(inode_source, FI_DATA_UN_MODIFY);
+#endif
+		inode_unlock(inode_source);
+		iput(inode_source);
+		fm_node = (struct file_list_node *)f2fs_kmalloc(sbi,
+						sizeof(struct file_list_node), GFP_KERNEL);
+		if (!fm_node) {
+			ret = -ENOMEM;
+			goto fail;
+		}
+		fm_node->merge_file.ino = fm_u.ino;
+		fm_node->merge_file.extent_count = 0;
+		fm_node->merge_file.mtime = fm_u.mtime;
+		fm_node->merge_file.i_generation = fm_u.i_generation;
+		fm_node->bitmax = DIV_ROUND_UP(i_size, PAGE_SIZE);
+		fm_node->bitmap = (unsigned long*)f2fs_kvzalloc(sbi,
+						f2fs_bitmap_size(fm_node->bitmax), GFP_KERNEL);
+		if (!fm_node->bitmap) {
+			kfree(fm_node);
+			fm_node = NULL;
+			ret = -ENOMEM;
+			goto fail;
+		}
 
-		ret = reserve_compress_blocks(&dn, count, &reserved_blocks);
+		INIT_LIST_HEAD(&(fm_node->ext_list));
+		list_add_tail(&(fm_node->list), &(fmm->list));
+		fmm->num++;
+	} else {
+		if (fm_node->merge_file.i_generation != fm_u.i_generation ||
+			fm_node->merge_file.mtime  != fm_u.mtime) {
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
+	}
 
-		f2fs_put_dnode(&dn);
+	if (fm_u.extent_count > f2fs_appboost_maxblocks(sbi)) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
 
-		if (ret < 0)
-			break;
+	ret = f2fs_insert_merge_file_user(fmm, sbi, fm_node, &fm_u);
+	// if return EOVERFLOW, we support max blocks
+	if (ret == -EOVERFLOW)
+		ret = 0;
+fail:
+	inode_unlock(inode);
+	return ret;
+}
 
-		page_idx += count;
-	}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+static void f2fs_file_read_pages(struct inode *inode)
+{
+	struct backing_dev_info *bdi;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 0)
+	DEFINE_READAHEAD(ractl, NULL, NULL, inode->i_mapping, 0);
+#else
+	DEFINE_READAHEAD(ractl, NULL, inode->i_mapping, 0);
+#endif
+	unsigned long max_blocks = (inode->i_size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	unsigned long nr_to_read = 0;
+	unsigned long index = 0;
 
-	filemap_invalidate_unlock(inode->i_mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	bdi = inode_to_bdi(inode);
+	if (!bdi)
+		return;
 
-	if (!ret) {
-		clear_inode_flag(inode, FI_COMPRESS_RELEASED);
-		inode->i_ctime = current_time(inode);
-		f2fs_mark_inode_dirty_sync(inode, true);
-	}
-unlock_inode:
-	inode_unlock(inode);
-	mnt_drop_write_file(filp);
+	while (1) {
+		ractl._index = index;
+		/* equal to POSIX_FADV_SEQUENTIAL */
+		nr_to_read = min(2 * bdi->ra_pages, max_blocks - index + 1);
+		page_cache_ra_unbounded(&ractl, nr_to_read, 0);
 
-	if (!ret) {
-		ret = put_user(reserved_blocks, (u64 __user *)arg);
-	} else if (reserved_blocks &&
-			atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		f2fs_warn(sbi, "%s: partial blocks were released i_ino=%lx "
-			"iblocks=%llu, reserved=%u, compr_blocks=%u, "
-			"run fsck to fix.",
-			__func__, inode->i_ino, inode->i_blocks,
-			reserved_blocks,
-			atomic_read(&F2FS_I(inode)->i_compr_blocks));
+		index += nr_to_read;
+		if (index >= max_blocks)
+			return;
 	}
-
-	return ret;
 }
+#endif
 
-static int f2fs_secure_erase(struct block_device *bdev, struct inode *inode,
-		pgoff_t off, block_t block, block_t len, u32 flags)
+static int merge_sync_file(struct f2fs_sb_info *sbi, struct file *file)
 {
-	sector_t sector = SECTOR_FROM_BLOCK(block);
-	sector_t nr_sects = SECTOR_FROM_BLOCK(len);
 	int ret = 0;
+	struct inode *inode = file->f_mapping->host;
 
-	if (flags & F2FS_TRIM_FILE_DISCARD) {
-		if (bdev_max_secure_erase_sectors(bdev))
-			ret = blkdev_issue_secure_erase(bdev, sector, nr_sects,
-					GFP_NOFS);
-		else
-			ret = blkdev_issue_discard(bdev, sector, nr_sects,
-					GFP_NOFS);
+	if (time_to_inject(sbi, FAULT_FSYNC_ERROR)) {
+		ret = 0;
+	} else {
+		ret = f2fs_do_sync_file(file, 0, LLONG_MAX, 0, 0);
 	}
 
-	if (!ret && (flags & F2FS_TRIM_FILE_ZEROOUT)) {
-		if (IS_ENCRYPTED(inode))
-			ret = fscrypt_zeroout_range(inode, off, block, len);
-		else
-			ret = blkdev_issue_zeroout(bdev, sector, nr_sects,
-					GFP_NOFS, 0);
+	if (ret != 0) {
+		f2fs_err(sbi, "f2fs_end_file_merge:failed to sync");
+		return ret;
+	}
+
+	if (time_to_inject(sbi, FAULT_FLUSH_ERROR)) {
+		ret = 0;
+	} else {
+		ret = f2fs_issue_flush(sbi, inode->i_ino);
 	}
 
+	if (ret != 0)
+		f2fs_err(sbi, "f2fs_end_file_merge:failed to flush");
+
 	return ret;
 }
 
-static int f2fs_sec_trim_file(struct file *filp, unsigned long arg)
+static void copy_summary_info_to_disk(struct merge_summary *summary,
+					struct merge_summary_dinfo *summary_dinfo)
 {
-	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct address_space *mapping = inode->i_mapping;
-	struct block_device *prev_bdev = NULL;
-	struct f2fs_sectrim_range range;
-	pgoff_t index, pg_end, prev_index = 0;
-	block_t prev_block = 0, len = 0;
-	loff_t end_addr;
-	bool to_end = false;
-	int ret = 0;
+	if (!summary || !summary_dinfo)
+		return;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	summary_dinfo->version = cpu_to_le32(summary->version);
+	summary_dinfo->state = cpu_to_le32(summary->state);
+	summary_dinfo->tail = cpu_to_le32(summary->tail);
+	summary_dinfo->checksum = cpu_to_le32(summary->checksum);
+	summary_dinfo->num = cpu_to_le32(summary->num);
+}
 
-	if (copy_from_user(&range, (struct f2fs_sectrim_range __user *)arg,
-				sizeof(range)))
-		return -EFAULT;
+static void copy_file_merge_info_to_disk(struct merge_file *info, struct merge_file_dinfo *dinfo)
+{
+	if (!info || !dinfo)
+		return;
 
-	if (range.flags == 0 || (range.flags & ~F2FS_TRIM_FILE_MASK) ||
-			!S_ISREG(inode->i_mode))
-		return -EINVAL;
+	dinfo->ino = cpu_to_le32(info->ino);
+	dinfo->extent_count = cpu_to_le32(info->extent_count);
+	dinfo->i_generation = cpu_to_le32(info->i_generation);
+	dinfo->mtime = cpu_to_le64(info->mtime);
+}
 
-	if (((range.flags & F2FS_TRIM_FILE_DISCARD) &&
-			!f2fs_hw_support_discard(sbi)) ||
-			((range.flags & F2FS_TRIM_FILE_ZEROOUT) &&
-			 IS_ENCRYPTED(inode) && f2fs_is_multi_device(sbi)))
-		return -EOPNOTSUPP;
+static void copy_extent_info_to_disk(struct merge_extent *extent,
+					struct merge_extent_dinfo *extent_dinfo)
+{
+	if (!extent || !extent_dinfo)
+		return;
 
-	file_start_write(filp);
-	inode_lock(inode);
+	extent_dinfo->index = cpu_to_le32(extent->index);
+	extent_dinfo->length = cpu_to_le32(extent->length);
+	extent_dinfo->index_in_mfile = cpu_to_le32(extent->index_in_mfile);
+}
 
-	if (f2fs_is_atomic_file(inode) || f2fs_compressed_file(inode) ||
-			range.start >= inode->i_size) {
+static int end_file_merge(struct file *filp, unsigned long arg)
+{
+	struct merge_summary *summary = NULL;
+	struct merge_summary_dinfo *summary_dinfo = NULL;
+	struct fi_merge_manage *fmm;
+	struct file_list_node *fm_node, *fm_tmp;
+	struct extent_list_node *fm_ext_node, *fm_ext_tmp;
+	struct merge_file *cur_merge_file;
+	struct file_list_node **merge_files_lists = NULL;
+	struct inode *inode = file_inode(filp);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct inode *inode_source;
+	struct page *page = NULL;
+	unsigned char *addr = NULL;
+	unsigned long merged_blocks = 0;
+	unsigned int max_blocks = f2fs_appboost_maxblocks(sbi);
+	struct merge_extent_dinfo extent_dinfo;
+	struct merge_file_dinfo file_dinfo;
+	loff_t offset = 0;
+	loff_t tail = 0;
+	int ret, i, k, result;
+
+	/* disk free space is not enough */
+	if (has_not_enough_free_secs(sbi, 0, max_blocks >> sbi->log_blocks_per_seg))
+		return -ENOSPC;
+
+	if (!inode_trylock(inode))
+		return -EAGAIN;
+
+	if (!(filp->f_flags & __O_TMPFILE)) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge check flags failed!");
 		ret = -EINVAL;
-		goto err;
+		goto fail;
 	}
 
-	if (range.len == 0)
-		goto err;
+	// when end and private is NULL, will return err
+	if (!fi->i_boostfile) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge: i_boostfile is null");
+		ret = -EFAULT;
+		goto fail;
+	}
 
-	if (inode->i_size - range.start > range.len) {
-		end_addr = range.start + range.len;
-	} else {
-		end_addr = range.len == (u64)-1 ?
-			sbi->sb->s_maxbytes : inode->i_size;
-		to_end = true;
+	fmm = (struct fi_merge_manage *)fi->i_boostfile;
+	merge_files_lists = (struct file_list_node **)f2fs_kvmalloc(sbi,
+				sizeof(struct file_list_node *) * fmm->num, GFP_KERNEL);
+	if (!merge_files_lists) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge: merge_files_lists is null");
+		ret = -ENOMEM;
+		goto fail;
 	}
 
-	if (!IS_ALIGNED(range.start, F2FS_BLKSIZE) ||
-			(!to_end && !IS_ALIGNED(end_addr, F2FS_BLKSIZE))) {
-		ret = -EINVAL;
-		goto err;
+	// 1. fill summary.
+	summary = f2fs_kzalloc(sbi, sizeof(struct merge_summary), GFP_KERNEL);
+	if (!summary) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge: summary is null");
+		ret = -ENOMEM;
+		goto fail;
 	}
 
-	index = F2FS_BYTES_TO_BLK(range.start);
-	pg_end = DIV_ROUND_UP(end_addr, F2FS_BLKSIZE);
+	summary_dinfo = f2fs_kzalloc(sbi, sizeof(struct merge_summary_dinfo), GFP_KERNEL);
+	if (!summary_dinfo) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge: summary_dinfo is null");
+		ret = -ENOMEM;
+		goto fail;
+	}
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		goto err;
+	list_for_each_entry_safe(fm_node, fm_tmp, &fmm->list, list) {
+		u64 size = fm_node->merge_file.extent_count *
+				sizeof(struct merge_extent_dinfo) + sizeof(struct merge_file_dinfo);
+		if (unlikely(size > INT_MAX)) {
+			ret = -EINVAL;
+			goto fail;
+		}
+		summary->fsize[summary->num] = size;
+		summary_dinfo->fsize[summary->num] = cpu_to_le32(size);
+		merge_files_lists[summary->num] = fm_node;
+		summary->num++;
+	}
+
+	// calc the offset
+	offset = sizeof(struct merge_summary);
+	for (i = 0; i < fmm->num; i++) {
+		offset += summary->fsize[i];
+	}
+	// align PAGE_SIZE
+	offset = (offset + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1);
+
+	// write file
+	for (i = 0; i < fmm->num; i++) {
+		cur_merge_file = &(merge_files_lists[i]->merge_file);
+		inode_source = f2fs_iget(sbi->sb, cur_merge_file->ino);
+		if (IS_ERR(inode_source)) {
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge NOFOUND ino=%d", cur_merge_file->ino);
+			ret = PTR_ERR(inode_source);
+			goto fail;
+		}
 
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(mapping);
+		if (is_bad_inode(inode_source)) {
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
 
-	ret = filemap_write_and_wait_range(mapping, range.start,
-			to_end ? LLONG_MAX : end_addr - 1);
-	if (ret)
-		goto out;
+		if (!inode_trylock(inode_source)) {
+			iput(inode_source);
+			ret = -EAGAIN;
+			goto fail;
+		}
 
-	truncate_inode_pages_range(mapping, range.start,
-			to_end ? -1 : end_addr - 1);
+		if (!S_ISREG(inode_source->i_mode)) {
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
+		if (cur_merge_file->mtime != timespec64_to_ns(&inode_source->i_mtime)) {
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge mtime expired ino = %u, new_time = %llu, expired_time = %llu",
+						cur_merge_file->ino, timespec64_to_ns(&inode_source->i_mtime), cur_merge_file->mtime);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
 
-	while (index < pg_end) {
-		struct dnode_of_data dn;
-		pgoff_t end_offset, count;
-		int i;
+		if (cur_merge_file->i_generation != inode_source->i_generation) {
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge i_generation has been changed ino = %u!",  cur_merge_file->ino);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		ret = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_inode_support_dedup(sbi, inode_source) &&
+			!is_inode_flag_set(inode_source, FI_DATA_UN_MODIFY)) {
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge FI_DATA_UN_MODIFY flag has been changed ino = %u!",  cur_merge_file->ino);
+                        inode_unlock(inode_source);
+                        iput(inode_source);
+                        ret = -EKEYEXPIRED;
+                        goto fail;
+		}
+#endif
+
+		ret = fscrypt_require_key(inode_source);
 		if (ret) {
-			if (ret == -ENOENT) {
-				index = f2fs_get_next_page_offset(&dn, index);
-				continue;
-			}
-			goto out;
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge get file ino = %lu encrypt info failed\n", inode_source->i_ino);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			goto fail;
 		}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+		f2fs_file_read_pages(inode_source);
+#endif
+		list_for_each_entry_safe(fm_ext_node, fm_ext_tmp, &merge_files_lists[i]->ext_list, list) {
+			fm_ext_node->extent.index_in_mfile = offset >> PAGE_SHIFT;
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-		count = min(end_offset - dn.ofs_in_node, pg_end - index);
-		for (i = 0; i < count; i++, index++, dn.ofs_in_node++) {
-			struct block_device *cur_bdev;
-			block_t blkaddr = f2fs_data_blkaddr(&dn);
+			for (k = 0; k < fm_ext_node->extent.length; k++) {
+				if (time_to_inject(sbi, FAULT_PAGE_ERROR)) {
+					page = ERR_PTR(-EIO);
+				} else {
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+					if (f2fs_compressed_file(inode_source) &&
+						f2fs_is_compressed_cluster(inode_source,
+							fm_ext_node->extent.index + k)) {
+						f2fs_warn(sbi, "f2fs_ioc_end_file_merge ino = %u,(%d, %d) is compressed\n",
+								cur_merge_file->ino, fm_ext_node->extent.index, k);
+						inode_unlock(inode_source);
+						iput(inode_source);
+						ret = -EKEYEXPIRED;
+						goto fail;
+					}
+#endif
+					page = f2fs_get_lock_data_page(inode_source, fm_ext_node->extent.index + k, false);
+				}
+				if (IS_ERR(page)) {
+					f2fs_warn(sbi, "f2fs_find_data_page err (%d,%d) ino:%d",
+								fm_ext_node->extent.index, k, cur_merge_file->ino);
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -EIO;
+					goto fail;
+				}
 
-			if (!__is_valid_data_blkaddr(blkaddr))
-				continue;
+				addr = kmap(page);
+				if (f2fs_file_write(filp, offset, addr, PAGE_SIZE) != PAGE_SIZE) {
+					kunmap(page);
+					flush_dcache_page(page);
+					f2fs_put_page(page, 1);
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -EIO;
+					goto fail;
+				}
+				kunmap(page);
+				flush_dcache_page(page);
+				offset += PAGE_SIZE;
+				merged_blocks++;
+				f2fs_put_page(page, 1);
+				if (merged_blocks > max_blocks) {
+					f2fs_warn(sbi, "f2fs_ioc_end_file_merge excess max_blocks %lu,%u!!!",
+							merged_blocks, max_blocks);
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -EFAULT;
+					goto fail;
+				}
 
-			if (!f2fs_is_valid_blkaddr(sbi, blkaddr,
-						DATA_GENERIC_ENHANCE)) {
-				ret = -EFSCORRUPTED;
-				f2fs_put_dnode(&dn);
-				f2fs_handle_error(sbi,
-						ERROR_INVALID_BLKADDR);
-				goto out;
+				if (fatal_signal_pending(current)) {
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -EINTR;
+					goto fail;
+				}
+
+				if (!f2fs_appboost_enable(sbi)) {
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -ENOTTY;
+					goto fail;
+				}
 			}
+		}
+		/* invalidate clean page */
+		invalidate_mapping_pages(inode_source->i_mapping, 0, -1);
+		inode_unlock(inode_source);
+		iput(inode_source);
+	}
+
+	if (unlikely(offset > INT_MAX)) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
+
+	tail = offset;
+
+	// first write summary
+	offset = 0;
+	summary->version = F2FS_BOOSTFILE_VERSION;
+	summary->state = BOOST_FILE_STATE_FINISH;
+	summary->tail = tail;
+	summary->checksum = 0;
+	summary->checksum = f2fs_crc32(sbi, (void *)summary , sizeof(struct merge_summary));
 
-			cur_bdev = f2fs_target_device(sbi, blkaddr, NULL);
-			if (f2fs_is_multi_device(sbi)) {
-				int di = f2fs_target_device_index(sbi, blkaddr);
+	copy_summary_info_to_disk(summary, summary_dinfo);
+	result = f2fs_file_write(filp, offset, (unsigned char *)summary_dinfo,
+				 sizeof(struct merge_summary_dinfo));
+	if (result != sizeof(struct merge_summary_dinfo)) {
+		ret = -EIO;
+		goto fail;
+	}
 
-				blkaddr -= FDEV(di).start_blk;
-			}
+	// write the file info
+	offset = sizeof(struct merge_summary_dinfo);
+	for (i = 0; i < fmm->num; i++) {
+		copy_file_merge_info_to_disk(&(merge_files_lists[i]->merge_file), &file_dinfo);
+		result = f2fs_file_write(filp, offset, (unsigned char *)&(file_dinfo),
+					 sizeof(file_dinfo));
+		if (result != sizeof(file_dinfo)) {
+			ret = -EIO;
+			goto fail;
+		}
+		offset += sizeof(file_dinfo);
 
-			if (len) {
-				if (prev_bdev == cur_bdev &&
-						index == prev_index + len &&
-						blkaddr == prev_block + len) {
-					len++;
-				} else {
-					ret = f2fs_secure_erase(prev_bdev,
-						inode, prev_index, prev_block,
-						len, range.flags);
-					if (ret) {
-						f2fs_put_dnode(&dn);
-						goto out;
-					}
+		list_for_each_entry_safe(fm_ext_node, fm_ext_tmp, &merge_files_lists[i]->ext_list, list) {
+			copy_extent_info_to_disk(&fm_ext_node->extent, &extent_dinfo);
+			result = f2fs_file_write(filp, offset, (unsigned char *)&extent_dinfo, sizeof(extent_dinfo));
+			if (result != sizeof(extent_dinfo)) {
+				ret = -EIO;
+				goto fail;
+			}
+			offset += sizeof(extent_dinfo);
 
-					len = 0;
-				}
+			if (fatal_signal_pending(current)) {
+				ret = -EINTR;
+				goto fail;
 			}
 
-			if (!len) {
-				prev_bdev = cur_bdev;
-				prev_index = index;
-				prev_block = blkaddr;
-				len = 1;
+			if (!f2fs_appboost_enable(sbi)) {
+				ret = -ENOTTY;
+				goto fail;
 			}
 		}
+	}
 
-		f2fs_put_dnode(&dn);
+	ret = merge_sync_file(sbi, filp);
+	if (ret)
+		goto fail;
 
-		if (fatal_signal_pending(current)) {
-			ret = -EINTR;
-			goto out;
-		}
-		cond_resched();
+	offset = tail;
+	if (time_to_inject(sbi, FAULT_WRITE_TAIL_ERROR)) {
+		summary_dinfo->checksum = F2FS_BOOSTFILE_VERSION;
+	}
+	result = f2fs_file_write(filp, offset, (unsigned char *)summary_dinfo,
+				 sizeof(struct merge_summary_dinfo));
+	if (result != sizeof(struct merge_summary_dinfo)) {
+		ret = -EIO;
+		goto fail;
 	}
 
-	if (len)
-		ret = f2fs_secure_erase(prev_bdev, inode, prev_index,
-				prev_block, len, range.flags);
-out:
-	filemap_invalidate_unlock(mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-err:
-	inode_unlock(inode);
-	file_end_write(filp);
+#ifdef CONFIG_F2FS_FS_DEDUP
+        if (f2fs_inode_support_dedup(sbi, inode)) {
+                set_inode_flag(inode, FI_MERGED_FILE);
+                set_inode_flag(inode, FI_DATA_UN_MODIFY);
+        }
+#endif
+	ret = merge_sync_file(sbi, filp);
+fail:
+	f2fs_boostfile_free(inode);
+        inode_unlock(inode);
+	if (merge_files_lists)
+		kvfree(merge_files_lists);
+	if (summary)
+		kfree(summary);
+	if (summary_dinfo)
+		kfree(summary_dinfo);
 
 	return ret;
 }
 
-static int f2fs_ioc_get_compress_option(struct file *filp, unsigned long arg)
+static int f2fs_ioc_end_file_merge(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct f2fs_comp_option option;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	int ret;
 
-	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
-		return -EOPNOTSUPP;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	inode_lock_shared(inode);
+	if  (!f2fs_appboost_enable(sbi))
+		return -ENOTTY;
 
-	if (!f2fs_compressed_file(inode)) {
-		inode_unlock_shared(inode);
-		return -ENODATA;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	ret = end_file_merge(filp, arg);
+
+	mnt_drop_write_file(filp);
+
+	return ret;
+}
+
+static inline bool appboost_should_abort(struct inode *inode,
+					 unsigned long boost_start, unsigned int interval)
+{
+	if (time_after(jiffies, boost_start + interval))
+		return true;
+
+	if (atomic_read(&(F2FS_I(inode)->appboost_abort))) {
+		atomic_set(&(F2FS_I(inode)->appboost_abort), 0);
+		return true;
 	}
 
-	option.algorithm = F2FS_I(inode)->i_compress_algorithm;
-	option.log_cluster_size = F2FS_I(inode)->i_log_cluster_size;
+	return false;
+}
 
-	inode_unlock_shared(inode);
+static int f2fs_ioc_abort_preload_file(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	if (copy_to_user((struct f2fs_comp_option __user *)arg, &option,
-				sizeof(option)))
-		return -EFAULT;
+	if (!f2fs_appboost_enable(sbi))
+		return -ENOTTY;
+
+	atomic_set(&(F2FS_I(inode)->appboost_abort), 1);
 
 	return 0;
 }
 
-static int f2fs_ioc_set_compress_option(struct file *filp, unsigned long arg)
+static void copy_summary_info_from_disk(struct merge_summary *summary,
+					struct merge_summary_dinfo *summary_dinfo)
+{
+	int i;
+
+	if (!summary || !summary_dinfo)
+		return;
+
+	summary->num = le32_to_cpu(summary_dinfo->num);
+	summary->version = le32_to_cpu(summary_dinfo->version);
+	summary->state = le32_to_cpu(summary_dinfo->state);
+	summary->tail = le32_to_cpu(summary_dinfo->tail);
+	summary->checksum = le32_to_cpu(summary_dinfo->checksum);
+
+	for (i = 0; i < min(summary->num, BOOST_MAX_FILES); i++)
+		summary->fsize[i] = le32_to_cpu(summary_dinfo->fsize[i]);
+}
+
+static void copy_extent_info_from_disk(struct merge_extent *extent,
+					struct merge_extent_dinfo *d_extent)
+{
+	if (!extent || !d_extent)
+		return;
+
+	extent->index = le32_to_cpu(d_extent->index);
+	extent->length = le32_to_cpu(d_extent->length);
+	extent->index_in_mfile = le32_to_cpu(d_extent->index_in_mfile);
+}
+
+static void copy_file_merge_info_from_disk(struct merge_file *info, struct merge_file_dinfo *dinfo)
+{
+	if (!info || !dinfo)
+		return;
+
+	info->ino = le32_to_cpu(dinfo->ino);
+	info->extent_count = le32_to_cpu(dinfo->extent_count);
+	info->i_generation = le32_to_cpu(dinfo->i_generation);
+	info->mtime = le64_to_cpu(dinfo->mtime);
+}
+
+static int f2fs_ioc_preload_file(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_comp_option option;
+	struct inode *inode_source = NULL;
+	struct merge_summary *summary = NULL;
+	struct merge_summary_dinfo *summary_dinfo = NULL;
+	unsigned char *page_addr = NULL;
+	struct page *page = NULL;
+	unsigned char *buf = NULL;
+	unsigned long boost_start = jiffies;
+	/* arg indicate ms to run */
+	unsigned long interval, interval_ms;
+	loff_t pos = 0;
+	loff_t tail = 0;
+	long long pos_in = 0;
+	unsigned long to_read = 0;
 	int ret = 0;
+	int i, j, k;
+	int checksum = 0;
 
-	if (!f2fs_sb_has_compression(sbi))
-		return -EOPNOTSUPP;
-
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (!f2fs_appboost_enable(sbi))
+		return -ENOTTY;
 
-	if (copy_from_user(&option, (struct f2fs_comp_option __user *)arg,
-				sizeof(option)))
+	if (get_user(interval_ms, (unsigned long __user *)arg))
 		return -EFAULT;
 
-	if (option.log_cluster_size < MIN_COMPRESS_LOG_SIZE ||
-		option.log_cluster_size > MAX_COMPRESS_LOG_SIZE ||
-		option.algorithm >= COMPRESS_MAX)
-		return -EINVAL;
+	if (interval_ms > PRELOAD_MAX_TIME)
+		interval = PRELOAD_MAX_TIME * HZ / 1000;
+	else
+		interval = interval_ms * HZ / 1000;
 
-	file_start_write(filp);
-	inode_lock(inode);
+	if (!inode_trylock(inode))
+		return -EAGAIN;
 
-	f2fs_down_write(&F2FS_I(inode)->i_sem);
-	if (!f2fs_compressed_file(inode)) {
-		ret = -EINVAL;
-		goto out;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_inode_support_dedup(sbi, inode) &&
+			(!is_inode_flag_set(inode, FI_MERGED_FILE) ||
+			!is_inode_flag_set(inode, FI_DATA_UN_MODIFY))) {
+		f2fs_err(sbi, "merged file has something wrong\n");
+		ret = -EKEYEXPIRED;
+		goto fail;
 	}
+#endif
 
-	if (f2fs_is_mmap_file(inode) || get_dirty_pages(inode)) {
-		ret = -EBUSY;
-		goto out;
+	if (atomic_read(&(F2FS_I(inode)->appboost_abort))) {
+		atomic_set(&(F2FS_I(inode)->appboost_abort), 0);
+		ret = -EAGAIN;
+		goto fail;
 	}
 
-	if (F2FS_HAS_BLOCKS(inode)) {
-		ret = -EFBIG;
-		goto out;
+	//check head summary
+	summary = f2fs_kzalloc(sbi, sizeof(struct merge_summary), GFP_KERNEL);
+	if (!summary) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file: summary is null\n");
+		ret = -ENOMEM;
+		goto fail;
 	}
 
-	F2FS_I(inode)->i_compress_algorithm = option.algorithm;
-	F2FS_I(inode)->i_log_cluster_size = option.log_cluster_size;
-	F2FS_I(inode)->i_cluster_size = BIT(option.log_cluster_size);
-	/* Set default level */
-	if (F2FS_I(inode)->i_compress_algorithm == COMPRESS_ZSTD)
-		F2FS_I(inode)->i_compress_level = F2FS_ZSTD_DEFAULT_CLEVEL;
-	else
-		F2FS_I(inode)->i_compress_level = 0;
-	/* Adjust mount option level */
-	if (option.algorithm == F2FS_OPTION(sbi).compress_algorithm &&
-	    F2FS_OPTION(sbi).compress_level)
-		F2FS_I(inode)->i_compress_level = F2FS_OPTION(sbi).compress_level;
-	f2fs_mark_inode_dirty_sync(inode, true);
+	summary_dinfo = f2fs_kzalloc(sbi, sizeof(struct merge_summary_dinfo), GFP_KERNEL);
+	if (!summary_dinfo) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file: summary_dinfo is null\n");
+		ret = -ENOMEM;
+		goto fail;
+	}
 
-	if (!f2fs_is_compress_backend_ready(inode))
-		f2fs_warn(sbi, "compression algorithm is successfully set, "
-			"but current kernel doesn't support this algorithm.");
-out:
-	f2fs_up_write(&F2FS_I(inode)->i_sem);
-	inode_unlock(inode);
-	file_end_write(filp);
+	to_read = sizeof(struct merge_summary_dinfo);
+	if (f2fs_file_read(filp, 0, (unsigned char*)summary_dinfo, to_read) != to_read) {
+		ret = -EIO;
+		goto fail;
+	}
 
-	return ret;
-}
+	copy_summary_info_from_disk(summary, summary_dinfo);
 
-static int redirty_blocks(struct inode *inode, pgoff_t page_idx, int len)
-{
-	DEFINE_READAHEAD(ractl, NULL, NULL, inode->i_mapping, page_idx);
-	struct address_space *mapping = inode->i_mapping;
-	struct page *page;
-	pgoff_t redirty_idx = page_idx;
-	int i, page_len = 0, ret = 0;
+	checksum = summary->checksum;
+	summary->checksum = 0;
+	if (!f2fs_crc_valid(sbi, checksum, summary, sizeof(struct merge_summary))) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
 
-	page_cache_ra_unbounded(&ractl, len, 0);
+	if (summary->version != F2FS_BOOSTFILE_VERSION) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file boost file version mismatch!\n");
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
 
-	for (i = 0; i < len; i++, page_idx++) {
-		page = read_cache_page(mapping, page_idx, NULL, NULL);
-		if (IS_ERR(page)) {
-			ret = PTR_ERR(page);
-			break;
-		}
-		page_len++;
+	if (summary->state != BOOST_FILE_STATE_FINISH) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file boost file not ready!\n");
+		ret = -EKEYEXPIRED;
+		goto fail;
 	}
 
-	for (i = 0; i < page_len; i++, redirty_idx++) {
-		page = find_lock_page(mapping, redirty_idx);
+	//check tail summary
+	tail = summary->tail;
+	to_read = sizeof(struct merge_summary);
+	if (f2fs_file_read(filp, tail, (unsigned char*)summary_dinfo, to_read) != to_read) {
+		ret = -EIO;
+		goto fail;
+	}
 
-		/* It will never fail, when page has pinned above */
-		f2fs_bug_on(F2FS_I_SB(inode), !page);
+	copy_summary_info_from_disk(summary, summary_dinfo);
+	if (checksum != summary->checksum) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
+
+	summary->checksum = 0;
+	if (!f2fs_crc_valid(sbi, checksum, summary, sizeof(struct merge_summary))) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
+
+	if (summary->num > BOOST_MAX_FILES) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
+
+	pos += sizeof(struct merge_summary);
+	for (i = 0; i < summary->num; i++) {
+		pos += summary->fsize[i];
+	}
+
+	buf = f2fs_kvmalloc(sbi, pos - sizeof(struct merge_summary), GFP_KERNEL);
+	if (!buf) {
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	to_read = pos - sizeof(struct merge_summary_dinfo);
+	if (f2fs_file_read(filp, sizeof(struct merge_summary), buf, to_read) != to_read) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file read buf failed!\n");
+                ret = -EIO;
+		goto fail;
+	}
+
+	// align the pos
+	pos = (pos + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1);
+	for (i = 0; i < summary->num; i++) {
+		u64 real_size = 0;
+		struct merge_file merge_file;
+		struct merge_file_dinfo *merge_file_dinfo = (struct merge_file_dinfo *)(buf + pos_in);
+		copy_file_merge_info_from_disk(&merge_file, merge_file_dinfo);
+
+		real_size = sizeof(struct merge_file_dinfo) +
+				   merge_file.extent_count * sizeof(struct merge_extent_dinfo);
+		if (summary->fsize[i] != real_size) {
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
+
+		pos_in += summary->fsize[i];
+		inode_source = f2fs_iget(sbi->sb, merge_file.ino);
+		if (IS_ERR(inode_source)) {
+			f2fs_err(sbi, "f2fs_ioc_preload_file no found!\n");
+			ret = -EFAULT;
+			goto fail;
+		}
+
+		if (is_bad_inode(inode_source)) {
+			ret = -EKEYEXPIRED;
+			goto fail_iput_source;
+		}
+
+		if (!S_ISREG(inode_source->i_mode)) {
+			ret = -EKEYEXPIRED;
+			goto fail_iput_source;
+		}
+
+		if (!inode_trylock(inode_source)) {
+			ret = -EAGAIN;
+			goto fail_iput_source;
+		}
+
+		if (merge_file.mtime != timespec64_to_ns(&inode_source->i_mtime)) {
+			f2fs_warn(sbi, "f2fs_ioc_preload_file file_merge has been changed! ino = %u, source_time = %llu, merge_time = %llu\n",
+								merge_file.ino, timespec64_to_ns(&inode_source->i_mtime), merge_file.mtime);
+			ret = -EKEYEXPIRED;
+			goto fail_unlock_source;
+		}
+
+		if (merge_file.i_generation != inode_source->i_generation) {
+                        f2fs_warn(sbi, "f2fs_ioc_preload_file i_generation has been changed!");
+			ret = -EKEYEXPIRED;
+			goto fail_unlock_source;
+		}
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_inode_support_dedup(sbi, inode_source) &&
+				!is_inode_flag_set(inode_source, FI_DATA_UN_MODIFY)) {
+			f2fs_err(sbi, "source file has modified\n");
+			ret = -EKEYEXPIRED;
+			goto fail_unlock_source;
+		}
+#endif
+
+		ret = fscrypt_require_key(inode_source);
+		if (ret) {
+			f2fs_warn(sbi, "f2fs_ioc_preload_file get file ino = %lu encrypt info failed\n", inode_source->i_ino);
+			goto fail_unlock_source;
+		}
+
+		for (j = 0; j < merge_file.extent_count; j++) {
+			struct merge_extent extent;
+			struct merge_extent extent_next;
+			copy_extent_info_from_disk(&extent, &(merge_file_dinfo->extents[j]));
+			if (pos >> PAGE_SHIFT != extent.index_in_mfile) {
+				f2fs_err(sbi, "f2fs_ioc_preload_file invalid index in merge file\n");
+				ret = -EKEYEXPIRED;
+				goto fail_unlock_source;
+			}
+
+			if (j < merge_file.extent_count - 1) {
+				copy_extent_info_from_disk(&extent_next, &(merge_file_dinfo->extents[j + 1]));
+				if (extent.index_in_mfile + extent.length != extent_next.index_in_mfile) {
+					f2fs_err(sbi, "f2fs_ioc_preload_file invalid extent len in merge file\n");
+					ret = -EKEYEXPIRED;
+					goto fail_unlock_source;
+				}
+			}
+
+			for (k = 0; k < extent.length; k++, pos += PAGE_SIZE) {
+				if (time_to_inject(sbi, FAULT_PAGE_ERROR)) {
+					page = NULL;
+				} else {
+					page = f2fs_pagecache_get_page(inode_source->i_mapping, extent.index + k,
+								FGP_LOCK | FGP_CREAT, GFP_NOFS);
+				}
+				if (!page) {
+					f2fs_warn(sbi, "f2fs_ioc_preload_file can not get page cache!\n");
+					ret = -ENOMEM;
+					goto fail_unlock_source;
+				}
+
+				if (PageUptodate(page)) {
+					f2fs_put_page(page, 1);
+					continue;
+				}
+				page_addr = kmap(page);
+				if (f2fs_file_read(filp, pos, page_addr, PAGE_SIZE) != PAGE_SIZE) {
+					kunmap(page);
+					flush_dcache_page(page);
+					f2fs_put_page(page, 1);
+					ret = -EIO;
+					goto fail_unlock_source;
+				}
+				kunmap(page);
+				flush_dcache_page(page);
+				SetPageUptodate(page);
+				f2fs_put_page(page, 1);
+
+				if (appboost_should_abort(inode, boost_start, interval)) {
+					ret = -EINTR;
+					f2fs_err(sbi, "f2fs_ioc_preload_file failed timeout!\n");
+					goto fail_unlock_source;
+				}
+
+				if (fatal_signal_pending(current)) {
+					ret = -EINTR;
+					goto fail_unlock_source;
+				}
+
+				if (!f2fs_appboost_enable(sbi)) {
+					ret = -ENOTTY;
+					goto fail_unlock_source;
+				}
+			}
+		}
+		inode_unlock(inode_source);
+		iput(inode_source);
+	}
+fail_unlock_source:
+	if (ret)
+		inode_unlock(inode_source);
+fail_iput_source:
+	if (ret)
+		iput(inode_source);
+fail:
+	atomic_set(&(F2FS_I(inode)->appboost_abort), 0);
+	/* invalidate boostfile clean page */
+	invalidate_mapping_pages(inode->i_mapping, 0, -1);
+	inode_unlock(inode);
+	if (buf)
+		kvfree(buf);
+	if (summary)
+		kfree(summary);
+	if (summary_dinfo)
+		kfree(summary_dinfo);
+	return ret;
+}
+#endif
 
-		f2fs_wait_on_page_writeback(page, DATA, true, true);
+static bool extra_attr_fits_in_inode(struct inode *inode, int field)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_inode *ri;
 
-		set_page_dirty(page);
-		f2fs_put_page(page, 1);
-		f2fs_put_page(page, 0);
+	switch (field) {
+	case F2FS_EXTRA_ATTR_TOTAL_SIZE:
+	case F2FS_EXTRA_ATTR_ISIZE:
+	case F2FS_EXTRA_ATTR_INLINE_XATTR_SIZE:
+		return true;
+	case F2FS_EXTRA_ATTR_PROJID:
+		if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid))
+			return false;
+		return true;
+	case F2FS_EXTRA_ATTR_INODE_CHKSUM:
+		if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_inode_checksum))
+			return false;
+		return true;
+	case F2FS_EXTRA_ATTR_CRTIME:
+		if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime))
+			return false;
+		return true;
+	case F2FS_EXTRA_ATTR_COMPR_BLOCKS:
+	case F2FS_EXTRA_ATTR_COMPR_OPTION:
+		if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_compr_blocks))
+			return false;
+		return true;
+	default:
+		f2fs_bug_on(F2FS_I_SB(inode), 1);
+		return false;
 	}
-
-	return ret;
 }
 
-static int f2fs_ioc_decompress_file(struct file *filp)
+static int f2fs_ioc_get_extra_attr(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct f2fs_inode_info *fi = F2FS_I(inode);
-	pgoff_t page_idx = 0, last_idx, cluster_idx;
-	unsigned int blk_per_seg = sbi->blocks_per_seg;
-	int ret;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_extra_attr attr;
+	u32 chksum;
+	int ret = 0;
 
-	if (!f2fs_sb_has_compression(sbi) ||
-			F2FS_OPTION(sbi).compress_mode != COMPR_MODE_USER)
+	if (!f2fs_has_extra_attr(inode))
 		return -EOPNOTSUPP;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
+		return -EFAULT;
 
-	if (!f2fs_compressed_file(inode))
+	if (attr.field >= F2FS_EXTRA_ATTR_MAX)
 		return -EINVAL;
 
-	f2fs_balance_fs(sbi, true);
-
-	file_start_write(filp);
-	inode_lock(inode);
+	if (!extra_attr_fits_in_inode(inode, attr.field))
+		return -EOPNOTSUPP;
 
-	if (!f2fs_is_compress_backend_ready(inode)) {
-		ret = -EOPNOTSUPP;
-		goto out;
+	switch (attr.field) {
+	case F2FS_EXTRA_ATTR_TOTAL_SIZE:
+		attr.attr = F2FS_TOTAL_EXTRA_ATTR_SIZE;
+		break;
+	case F2FS_EXTRA_ATTR_ISIZE:
+		attr.attr = fi->i_extra_isize;
+		break;
+	case F2FS_EXTRA_ATTR_INLINE_XATTR_SIZE:
+		if (!f2fs_has_inline_xattr(inode))
+			return -EOPNOTSUPP;
+		attr.attr = get_inline_xattr_addrs(inode);
+		break;
+	case F2FS_EXTRA_ATTR_PROJID:
+		if (!f2fs_sb_has_project_quota(F2FS_I_SB(inode)))
+			return -EOPNOTSUPP;
+		attr.attr = from_kprojid(&init_user_ns, fi->i_projid);
+		break;
+	case F2FS_EXTRA_ATTR_INODE_CHKSUM:
+		ret = f2fs_inode_chksum_get(sbi, inode, &chksum);
+		if (ret)
+			return ret;
+		attr.attr = chksum;
+		break;
+	case F2FS_EXTRA_ATTR_CRTIME:
+		if (!f2fs_sb_has_inode_crtime(sbi))
+			return -EOPNOTSUPP;
+		if (attr.attr_size == sizeof(struct timespec64)) {
+			if (put_timespec64(&fi->i_crtime,
+					(void __user *)(uintptr_t)attr.attr))
+				return -EFAULT;
+		} else if (attr.attr_size == sizeof(struct old_timespec32)) {
+			if (put_old_timespec32(&fi->i_crtime,
+					(void __user *)(uintptr_t)attr.attr))
+				return -EFAULT;
+		} else {
+			return -EINVAL;
+		}
+		break;
+	case F2FS_EXTRA_ATTR_COMPR_BLOCKS:
+		if (attr.attr_size != sizeof(__u64))
+			return -EINVAL;
+		ret = f2fs_get_compress_blocks(inode, &attr.attr);
+		break;
+	case F2FS_EXTRA_ATTR_COMPR_OPTION:
+		/* fix coverity error: Untrusted value as argument attr.attr_size*/
+		if (attr.attr_size > sizeof(struct f2fs_comp_option_v2))
+			return -EINVAL;
+		ret = f2fs_get_compress_option_v2(filp, attr.attr,
+						  &attr.attr_size);
+		break;
+	default:
+		return -EINVAL;
 	}
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
-		ret = -EINVAL;
-		goto out;
-	}
+	if (ret < 0)
+		return ret;
 
-	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
-	if (ret)
-		goto out;
+	if (copy_to_user((void __user *)arg, &attr, sizeof(attr)))
+		return -EFAULT;
 
-	if (!atomic_read(&fi->i_compr_blocks))
-		goto out;
+	return 0;
+}
 
-	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
-	last_idx >>= fi->i_log_cluster_size;
+static int f2fs_ioc_set_extra_attr(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_extra_attr attr;
+	struct page *ipage;
+	void *inline_addr;
+	int ret;
 
-	for (cluster_idx = 0; cluster_idx < last_idx; cluster_idx++) {
-		page_idx = cluster_idx << fi->i_log_cluster_size;
+	if (!f2fs_has_extra_attr(inode))
+		return -EOPNOTSUPP;
 
-		if (!f2fs_is_compressed_cluster(inode, page_idx))
-			continue;
+	if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
+		return -EFAULT;
 
-		ret = redirty_blocks(inode, page_idx, fi->i_cluster_size);
-		if (ret < 0)
-			break;
+	if (attr.field >= F2FS_EXTRA_ATTR_MAX)
+		return -EINVAL;
 
-		if (get_dirty_pages(inode) >= blk_per_seg) {
-			ret = filemap_fdatawrite(inode->i_mapping);
-			if (ret < 0)
-				break;
-		}
+	if (!extra_attr_fits_in_inode(inode, attr.field))
+		return -EOPNOTSUPP;
 
-		cond_resched();
-		if (fatal_signal_pending(current)) {
-			ret = -EINTR;
-			break;
+	switch (attr.field) {
+	case F2FS_EXTRA_ATTR_TOTAL_SIZE:
+	case F2FS_EXTRA_ATTR_ISIZE:
+	case F2FS_EXTRA_ATTR_PROJID:
+	case F2FS_EXTRA_ATTR_INODE_CHKSUM:
+	case F2FS_EXTRA_ATTR_CRTIME:
+	case F2FS_EXTRA_ATTR_COMPR_BLOCKS:
+		/* read only attribtues */
+		return -EOPNOTSUPP;
+	case F2FS_EXTRA_ATTR_INLINE_XATTR_SIZE:
+		if (!f2fs_sb_has_flexible_inline_xattr(sbi) ||
+		    !f2fs_has_inline_xattr(inode))
+			return -EOPNOTSUPP;
+		if (attr.attr < MIN_INLINE_XATTR_SIZE ||
+		    attr.attr > MAX_INLINE_XATTR_SIZE)
+			return -EINVAL;
+		inode_lock(inode);
+		f2fs_lock_op(sbi);
+		f2fs_down_write(&F2FS_I(inode)->i_xattr_sem);
+		if (i_size_read(inode) || F2FS_I(inode)->i_xattr_nid) {
+			/*
+			 * it is not allowed to set this field if the inode
+			 * has data or xattr node
+			 */
+			ret = -EFBIG;
+			goto xattr_out_unlock;
+		}
+		ipage = f2fs_get_node_page(sbi, inode->i_ino);
+		if (IS_ERR(ipage)) {
+			ret = PTR_ERR(ipage);
+			goto xattr_out_unlock;
+		}
+		inline_addr = inline_xattr_addr(inode, ipage);
+		if (!IS_XATTR_LAST_ENTRY(XATTR_FIRST_ENTRY(inline_addr))) {
+			ret = -EFBIG;
+		} else {
+			struct f2fs_xattr_header *hdr;
+			struct f2fs_xattr_entry *ent;
+
+			F2FS_I(inode)->i_inline_xattr_size = (int)attr.attr;
+			inline_addr = inline_xattr_addr(inode, ipage);
+			hdr = XATTR_HDR(inline_addr);
+			ent = XATTR_FIRST_ENTRY(inline_addr);
+			hdr->h_magic = cpu_to_le32(F2FS_XATTR_MAGIC);
+			hdr->h_refcount = cpu_to_le32(1);
+			memset(ent, 0, attr.attr - sizeof(*hdr));
+			set_page_dirty(ipage);
+			ret = 0;
 		}
+		f2fs_put_page(ipage, 1);
+xattr_out_unlock:
+		f2fs_up_write(&F2FS_I(inode)->i_xattr_sem);
+		f2fs_unlock_op(sbi);
+		inode_unlock(inode);
+		if (!ret)
+			f2fs_balance_fs(sbi, true);
+		break;
+	case F2FS_EXTRA_ATTR_COMPR_OPTION:
+		/* fix coverity error: Untrusted value as argument attr.attr_size*/
+		if (attr.attr_size > sizeof(struct f2fs_comp_option_v2))
+			return -EINVAL;
+		ret = f2fs_set_compress_option_v2(filp, attr.attr,
+						  &attr.attr_size);
+		break;
+	default:
+		return -EINVAL;
 	}
 
-	if (!ret)
-		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
-							LLONG_MAX);
-
-	if (ret)
-		f2fs_warn(sbi, "%s: The file might be partially decompressed (errno=%d). Please delete the file.",
-			  __func__, ret);
-out:
-	inode_unlock(inode);
-	file_end_write(filp);
-
 	return ret;
 }
 
-static int f2fs_ioc_compress_file(struct file *filp)
+#ifdef CONFIG_F2FS_SEQZONE
+static long f2fs_ioc_set_seqzone_file(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	pgoff_t page_idx = 0, last_idx, cluster_idx;
-	unsigned int blk_per_seg = sbi->blocks_per_seg;
-	int ret;
-
-	if (!f2fs_sb_has_compression(sbi) ||
-			F2FS_OPTION(sbi).compress_mode != COMPR_MODE_USER)
-		return -EOPNOTSUPP;
+	int ret = 0;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (!f2fs_sb_has_seqzone(sbi)) {
+		return -ENOTSUPP;
+	}
 
-	if (!f2fs_compressed_file(inode))
+	if (!S_ISREG(inode->i_mode))
 		return -EINVAL;
 
-	f2fs_balance_fs(sbi, true);
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
 
-	file_start_write(filp);
 	inode_lock(inode);
-
-	if (!f2fs_is_compress_backend_ready(inode)) {
-		ret = -EOPNOTSUPP;
+	if (!IS_ENCRYPTED(inode)) {
+		ret = -EINVAL;
 		goto out;
 	}
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+	if (F2FS_HAS_BLOCKS(inode)) {
 		ret = -EINVAL;
 		goto out;
 	}
 
-	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
-	if (ret)
+	if (is_inode_flag_set(inode, FI_COMPRESSED_FILE)) {
+		ret = -ENOTSUPP;
 		goto out;
-
-	set_inode_flag(inode, FI_ENABLE_COMPRESS);
-
-	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
-	last_idx >>= fi->i_log_cluster_size;
-
-	for (cluster_idx = 0; cluster_idx < last_idx; cluster_idx++) {
-		page_idx = cluster_idx << fi->i_log_cluster_size;
-
-		if (f2fs_is_sparse_cluster(inode, page_idx))
-			continue;
-
-		ret = redirty_blocks(inode, page_idx, fi->i_cluster_size);
-		if (ret < 0)
-			break;
-
-		if (get_dirty_pages(inode) >= blk_per_seg) {
-			ret = filemap_fdatawrite(inode->i_mapping);
-			if (ret < 0)
-				break;
-		}
-
-		cond_resched();
-		if (fatal_signal_pending(current)) {
-			ret = -EINTR;
-			break;
-		}
 	}
 
-	if (!ret)
-		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
-							LLONG_MAX);
-
-	clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+	if (f2fs_inode_support_dedup(sbi, inode)) {
+		set_inode_flag(inode, FI_SEQZONE);
+		f2fs_mark_inode_dirty_sync(inode, true);
+	}
 
-	if (ret)
-		f2fs_warn(sbi, "%s: The file might be partially compressed (errno=%d). Please delete the file.",
-			  __func__, ret);
 out:
 	inode_unlock(inode);
-	file_end_write(filp);
-
+	mnt_drop_write_file(filp);
 	return ret;
 }
+static long f2fs_ioc_get_seqzone_file(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+
+	int seqzone = f2fs_seqzone_file(inode) ? 1 : 0;
+	if (copy_to_user((int __user*)arg, &seqzone, sizeof(seqzone))) {
+		return -1;
+	}
+	return 0;
+}
+#endif
 
 static long __f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 {
+	int ret = 0;
+
 	switch (cmd) {
 	case FS_IOC_GETVERSION:
 		return f2fs_ioc_getversion(filp, arg);
@@ -4437,11 +8155,11 @@ static long __f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 	case FS_IOC_SETFSLABEL:
 		return f2fs_ioc_setfslabel(filp, arg);
 	case F2FS_IOC_GET_COMPRESS_BLOCKS:
-		return f2fs_get_compress_blocks(filp, arg);
+		return f2fs_ioc_get_compress_blocks(filp, arg);
 	case F2FS_IOC_RELEASE_COMPRESS_BLOCKS:
 		return f2fs_release_compress_blocks(filp, arg);
 	case F2FS_IOC_RESERVE_COMPRESS_BLOCKS:
-		return f2fs_reserve_compress_blocks(filp, arg);
+		return f2fs_ioc_reserve_compress_blocks(filp, arg);
 	case F2FS_IOC_SEC_TRIM_FILE:
 		return f2fs_sec_trim_file(filp, arg);
 	case F2FS_IOC_GET_COMPRESS_OPTION:
@@ -4452,9 +8170,62 @@ static long __f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 		return f2fs_ioc_decompress_file(filp);
 	case F2FS_IOC_COMPRESS_FILE:
 		return f2fs_ioc_compress_file(filp);
+#ifdef CONFIG_F2FS_APPBOOST
+	case F2FS_IOC_START_MERGE_FILE:
+		return f2fs_ioc_start_file_merge(filp, arg);
+	case F2FS_IOC_END_MERGE_FILE:
+		return f2fs_ioc_end_file_merge(filp, arg);
+	case F2FS_IOC_PRELOAD_FILE:
+		return f2fs_ioc_preload_file(filp, arg);
+	case F2FS_IOC_ABORT_PRELOAD_FILE:
+		return f2fs_ioc_abort_preload_file(filp, arg);
+#endif
+	case F2FS_IOC_GET_EXTRA_ATTR:
+		return f2fs_ioc_get_extra_attr(filp, arg);
+	case F2FS_IOC_SET_EXTRA_ATTR:
+		return f2fs_ioc_set_extra_attr(filp, arg);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	case F2FS_IOC_DEDUP_CREATE:
+		ret = f2fs_ioc_create_layered_inode(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_FILE:
+		ret = f2fs_ioc_dedup_file(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_REVOKE:
+		ret = f2fs_ioc_dedup_revoke(filp, arg);
+		break;
+	case F2FS_IOC_CLONE_FILE:
+		ret = f2fs_ioc_clone_file(filp, arg);
+		break;
+	case F2FS_IOC_MODIFY_CHECK:
+		ret = f2fs_ioc_modify_check(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_PERM_CHECK:
+		ret = f2fs_ioc_dedup_permission_check(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_GET_FILE_INFO:
+		ret = f2fs_ioc_get_dedupd_file_info(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_GET_SYS_INFO:
+		ret = f2fs_ioc_get_dedup_sysinfo(filp, arg);
+		break;
+	case F2FS_IOC_SNAPSHOT_CREATE:
+		ret = f2fs_ioc_create_snapshot(filp, arg);
+		break;
+	case F2FS_IOC_SNAPSHOT_PREPARE:
+		ret = f2fs_ioc_prepare_snapshot(filp, arg);
+		break;
+#endif
+#ifdef CONFIG_F2FS_SEQZONE
+	case F2FS_IOC_SET_SEQZONE_FILE:
+		return f2fs_ioc_set_seqzone_file(filp, arg);
+	case F2FS_IOC_GET_SEQZONE_FILE:
+		return f2fs_ioc_get_seqzone_file(filp, arg);
+#endif
 	default:
 		return -ENOTTY;
 	}
+	return ret;
 }
 
 long f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
@@ -4482,6 +8253,11 @@ static bool f2fs_should_use_dio(struct inode *inode, struct kiocb *iocb,
 	if (f2fs_force_buffered_io(inode, iov_iter_rw(iter)))
 		return false;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if(is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		return false;
+#endif
+
 	/*
 	 * Direct I/O not aligned to the disk's logical_block_size will be
 	 * attempted, but will fail with -EINVAL.
@@ -4571,10 +8347,15 @@ static void f2fs_trace_rw_file_path(struct kiocb *iocb, size_t count, int rw)
 	struct inode *inode = file_inode(iocb->ki_filp);
 	char *buf, *path;
 
-	buf = f2fs_getname(F2FS_I_SB(inode));
+	buf = f2fs_kmalloc(F2FS_I_SB(inode), PATH_MAX, GFP_KERNEL);
 	if (!buf)
 		return;
+#ifdef CONFIG_F2FS_APPBOOST
+	buf = strcpy(buf, "/data");
+	path = dentry_path_raw(file_dentry(iocb->ki_filp), buf + 5, PATH_MAX - 5);
+#else
 	path = dentry_path_raw(file_dentry(iocb->ki_filp), buf, PATH_MAX);
+#endif
 	if (IS_ERR(path))
 		goto free_buf;
 	if (rw == WRITE)
@@ -4584,7 +8365,7 @@ static void f2fs_trace_rw_file_path(struct kiocb *iocb, size_t count, int rw)
 		trace_f2fs_dataread_start(inode, iocb->ki_pos, count,
 				current->pid, path, current->comm);
 free_buf:
-	f2fs_putname(buf);
+	kfree(buf);
 }
 
 static ssize_t f2fs_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
@@ -4606,6 +8387,15 @@ static ssize_t f2fs_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
 	if (f2fs_should_use_dio(inode, iocb, to)) {
 		ret = f2fs_dio_read_iter(iocb, to);
 	} else {
+		struct backing_dev_info *bdi = inode_to_bdi(inode);
+		struct file *file = iocb->ki_filp;
+
+		if (!(file->f_mode & FMODE_RANDOM) &&
+		    file->f_ra.ra_pages == bdi->ra_pages &&
+		    f2fs_compressed_file(inode)) {
+			file->f_ra.ra_pages = bdi->ra_pages * MIN_RA_MUL;
+		}
+
 		ret = filemap_read(iocb, to, 0);
 		if (ret > 0)
 			f2fs_update_iostat(F2FS_I_SB(inode), inode,
@@ -4626,8 +8416,15 @@ static ssize_t f2fs_write_checks(struct kiocb *iocb, struct iov_iter *from)
 	if (IS_IMMUTABLE(inode))
 		return -EPERM;
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+		err = f2fs_reserve_compress_blocks(inode, NULL);
+		if (err < 0)
+			return err;
+#else
 		return -EPERM;
+#endif
+	}
 
 	count = generic_write_checks(iocb, from);
 	if (count <= 0)
@@ -4658,7 +8455,11 @@ static int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *iter,
 	int ret;
 
 	/* If it will be an out-of-place direct write, don't bother. */
+#ifdef CONFIG_F2FS_SEQZONE
+	if (dio && (f2fs_lfs_mode(sbi) || f2fs_seqzone_file(inode)))
+#else
 	if (dio && f2fs_lfs_mode(sbi))
+#endif
 		return 0;
 	/*
 	 * Don't preallocate holes aligned to DIO_SKIP_HOLES which turns into
@@ -4882,8 +8683,6 @@ static ssize_t f2fs_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 	bool dio;
 	bool may_need_sync = true;
 	int preallocated;
-	const loff_t pos = iocb->ki_pos;
-	const ssize_t count = iov_iter_count(from);
 	ssize_t ret;
 
 	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode)))) {
@@ -4905,11 +8704,21 @@ static ssize_t f2fs_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 		inode_lock(inode);
 	}
 
-	if (f2fs_is_pinned_file(inode) &&
-	    !f2fs_overwrite_io(inode, pos, count)) {
-		ret = -EIO;
-		goto out_unlock;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (!is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		ret = f2fs_revoke_deduped_inode(inode, __func__);
+		if (ret) {
+			inode_unlock(inode);
+			goto out;
+		}
 	}
+#endif
 
 	ret = f2fs_write_checks(iocb, from);
 	if (ret <= 0)
@@ -4987,6 +8796,9 @@ static int f2fs_file_fadvise(struct file *filp, loff_t offset, loff_t len,
 		bdi = inode_to_bdi(mapping->host);
 		filp->f_ra.ra_pages = bdi->ra_pages *
 			F2FS_I_SB(inode)->seq_file_ra_mul;
+		if (f2fs_compressed_file(inode) &&
+		    filp->f_ra.ra_pages < (bdi->ra_pages * COMPR_RA_MUL))
+			filp->f_ra.ra_pages = bdi->ra_pages * COMPR_RA_MUL;
 		spin_lock(&filp->f_lock);
 		filp->f_mode &= ~FMODE_RANDOM;
 		spin_unlock(&filp->f_lock);
@@ -5107,6 +8919,30 @@ long f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
 	case F2FS_IOC_SET_COMPRESS_OPTION:
 	case F2FS_IOC_DECOMPRESS_FILE:
 	case F2FS_IOC_COMPRESS_FILE:
+#ifdef CONFIG_F2FS_APPBOOST
+	case F2FS_IOC_START_MERGE_FILE:
+	case F2FS_IOC_END_MERGE_FILE:
+	case F2FS_IOC_PRELOAD_FILE:
+	case F2FS_IOC_ABORT_PRELOAD_FILE:
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+	case F2FS_IOC_DEDUP_CREATE:
+	case F2FS_IOC_DEDUP_FILE:
+	case F2FS_IOC_DEDUP_REVOKE:
+	case F2FS_IOC_CLONE_FILE:
+	case F2FS_IOC_MODIFY_CHECK:
+	case F2FS_IOC_DEDUP_PERM_CHECK:
+	case F2FS_IOC_DEDUP_GET_FILE_INFO:
+	case F2FS_IOC_DEDUP_GET_SYS_INFO:
+	case F2FS_IOC_SNAPSHOT_CREATE:
+	case F2FS_IOC_SNAPSHOT_PREPARE:
+#endif
+	case F2FS_IOC_GET_EXTRA_ATTR:
+	case F2FS_IOC_SET_EXTRA_ATTR:
+#ifdef CONFIG_F2FS_SEQZONE
+	case F2FS_IOC_SET_SEQZONE_FILE:
+	case F2FS_IOC_GET_SEQZONE_FILE:
+#endif
 		break;
 	default:
 		return -ENOIOCTLCMD;
diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e5e20b7c6..36d1d713b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -228,8 +228,6 @@ static int select_gc_type(struct f2fs_sb_info *sbi, int gc_type)
 
 	switch (sbi->gc_mode) {
 	case GC_IDLE_CB:
-	case GC_URGENT_LOW:
-	case GC_URGENT_MID:
 		gc_mode = GC_CB;
 		break;
 	case GC_IDLE_GREEDY:
@@ -1369,7 +1367,7 @@ static int move_data_block(struct inode *inode, block_t bidx,
 
 	/* allocate block address */
 	err = f2fs_allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &newaddr,
-				&sum, type, NULL);
+				&sum, type, NULL, 0);
 	if (err) {
 		f2fs_put_page(mpage, 1);
 		/* filesystem should shutdown, no need to recovery block */
@@ -1390,7 +1388,7 @@ static int move_data_block(struct inode *inode, block_t bidx,
 				page_address(mpage), PAGE_SIZE);
 	f2fs_put_page(mpage, 1);
 
-	f2fs_invalidate_internal_cache(fio.sbi, fio.old_blkaddr, 1);
+	f2fs_invalidate_internal_cache(fio.sbi, fio.old_blkaddr);
 
 	set_page_dirty(fio.encrypted_page);
 	if (clear_page_dirty_for_io(fio.encrypted_page))
diff --git a/fs/f2fs/inline.c b/fs/f2fs/inline.c
index d34943bc9..dd73fb9f1 100644
--- a/fs/f2fs/inline.c
+++ b/fs/f2fs/inline.c
@@ -335,8 +335,7 @@ int f2fs_recover_inline_data(struct inode *inode, struct page *npage)
 
 struct f2fs_dir_entry *f2fs_find_in_inline_dir(struct inode *dir,
 					const struct f2fs_filename *fname,
-					struct page **res_page,
-					bool use_hash)
+					struct page **res_page)
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(dir->i_sb);
 	struct f2fs_dir_entry *de;
@@ -353,7 +352,7 @@ struct f2fs_dir_entry *f2fs_find_in_inline_dir(struct inode *dir,
 	inline_dentry = inline_data_addr(dir, ipage);
 
 	make_dentry_ptr_inline(dir, &d, inline_dentry);
-	de = f2fs_find_target_dentry(&d, fname, NULL, use_hash);
+	de = f2fs_find_target_dentry(&d, fname, NULL);
 	unlock_page(ipage);
 	if (IS_ERR(de)) {
 		*res_page = ERR_CAST(de);
diff --git a/fs/f2fs/inode.c b/fs/f2fs/inode.c
index a411384a8..7cf41bef8 100644
--- a/fs/f2fs/inode.c
+++ b/fs/f2fs/inode.c
@@ -190,6 +190,27 @@ void f2fs_inode_chksum_set(struct f2fs_sb_info *sbi, struct page *page)
 	ri->i_inode_checksum = cpu_to_le32(f2fs_inode_chksum(sbi, page));
 }
 
+int f2fs_inode_chksum_get(struct f2fs_sb_info *sbi,
+			  struct inode *inode, u32 *chksum)
+{
+	struct page *ipage;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_inode *ri;
+
+	if (!f2fs_sb_has_inode_chksum(sbi) ||
+	    !f2fs_has_extra_attr(inode) ||
+	    !F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_inode_checksum))
+		return -EOPNOTSUPP;
+
+	ipage = f2fs_get_node_page(sbi, inode->i_ino);
+	if (IS_ERR(ipage))
+		return PTR_ERR(ipage);
+
+	*chksum = f2fs_inode_chksum(sbi, ipage);
+	f2fs_put_page(ipage, true);
+	return 0;
+}
+
 static bool sanity_check_inode(struct inode *inode, struct page *node_page)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -213,13 +234,6 @@ static bool sanity_check_inode(struct inode *inode, struct page *node_page)
 		return false;
 	}
 
-	if (ino_of_node(node_page) == fi->i_xattr_nid) {
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		f2fs_warn(sbi, "%s: corrupted inode i_ino=%lx, xnid=%x, run fsck to fix.",
-			  __func__, inode->i_ino, fi->i_xattr_nid);
-		return false;
-	}
-
 	if (f2fs_sb_has_flexible_inline_xattr(sbi)
 			&& !f2fs_has_extra_attr(inode)) {
 		set_sbi_flag(sbi, SBI_NEED_FSCK);
@@ -329,6 +343,44 @@ static void init_idisk_time(struct inode *inode)
 	fi->i_disk_time[2] = inode->i_mtime;
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+/* should init dedup flags before */
+static int init_inner_inode(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct page *node_page = NULL;
+	struct f2fs_inode *ri = NULL;
+	struct inode *inner = NULL;
+	nid_t inner_ino;
+
+	if (!f2fs_is_outer_inode(inode))
+		return 0;
+
+	if (time_to_inject(sbi, FAULT_DEDUP_INIT_INNER))
+		return -EIO;
+
+	node_page = f2fs_get_node_page(sbi, inode->i_ino);
+	if (IS_ERR(node_page))
+		return PTR_ERR(node_page);
+
+	ri = F2FS_INODE(node_page);
+	inner_ino = le32_to_cpu(ri->i_inner_ino);
+
+	inner = f2fs_iget(sbi->sb, inner_ino);
+	if (unlikely(IS_ERR(inner))) {
+		f2fs_err(sbi, "inode[%lu] iget inner ino[%u] fail",
+				inode->i_ino, inner_ino);
+		f2fs_put_page(node_page, 1);
+		return PTR_ERR(inner);
+	}
+
+	fi->inner_inode = inner;
+	f2fs_put_page(node_page, 1);
+	return 0;
+}
+#endif
+
 static int do_read_inode(struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -447,14 +499,19 @@ static int do_read_inode(struct inode *inode)
 			fi->i_log_cluster_size = ri->i_log_cluster_size;
 			compress_flag = le16_to_cpu(ri->i_compress_flag);
 			fi->i_compress_level = compress_flag >>
-						COMPRESS_LEVEL_OFFSET;
+						COMPRESS_LEVEL;
 			fi->i_compress_flag = compress_flag &
-					GENMASK(COMPRESS_LEVEL_OFFSET - 1, 0);
+					GENMASK(COMPRESS_LEVEL - 1, 0);
 			fi->i_cluster_size = BIT(fi->i_log_cluster_size);
 			set_inode_flag(inode, FI_COMPRESSED_FILE);
 		}
 	}
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_inode_support_dedup(sbi, inode))
+		get_dedup_flags_info(inode, ri);
+#endif
+
 	init_idisk_time(inode);
 
 	/* Need all the flag bits */
@@ -467,6 +524,9 @@ static int do_read_inode(struct inode *inode)
 		return -EFSCORRUPTED;
 	}
 
+#ifdef CONFIG_F2FS_APPBOOST
+	atomic_set(&F2FS_I(inode)->appboost_abort, 0);
+#endif
 	f2fs_put_page(node_page, 1);
 
 	stat_inc_inline_xattr(inode);
@@ -557,6 +617,11 @@ struct inode *f2fs_iget(struct super_block *sb, unsigned long ino)
 		ret = -EIO;
 		goto bad_inode;
 	}
+#ifdef CONFIG_F2FS_FS_DEDUP
+	ret = init_inner_inode(inode);
+	if (ret)
+		goto bad_inode;
+#endif
 	f2fs_set_inode_flags(inode);
 
 	unlock_new_inode(inode);
@@ -588,6 +653,9 @@ void f2fs_update_inode(struct inode *inode, struct page *node_page)
 {
 	struct f2fs_inode *ri;
 	struct extent_tree *et = F2FS_I(inode)->extent_tree[EX_READ];
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner;
+#endif
 
 	f2fs_wait_on_page_writeback(node_page, NODE, true, true);
 	set_page_dirty(node_page);
@@ -672,11 +740,24 @@ void f2fs_update_inode(struct inode *inode, struct page *node_page)
 				F2FS_I(inode)->i_compress_algorithm;
 			compress_flag = F2FS_I(inode)->i_compress_flag |
 				F2FS_I(inode)->i_compress_level <<
-						COMPRESS_LEVEL_OFFSET;
+						COMPRESS_LEVEL;
 			ri->i_compress_flag = cpu_to_le16(compress_flag);
 			ri->i_log_cluster_size =
 				F2FS_I(inode)->i_log_cluster_size;
 		}
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_sb_has_dedup(F2FS_I_SB(inode)) &&
+			F2FS_FITS_IN_INODE(ri, F2FS_I(inode)->i_extra_isize,
+				i_dedup_flags)) {
+			set_raw_dedup_flags(inode, ri);
+
+			inner = F2FS_I(inode)->inner_inode;
+			if (inner)
+				ri->i_inner_ino = cpu_to_le32(inner->i_ino);
+			else
+				ri->i_inner_ino = 0;
+		}
+#endif
 	}
 
 	__set_inode_rdev(inode, node_page);
@@ -705,12 +786,8 @@ void f2fs_update_inode_page(struct inode *inode)
 		if (err == -ENOENT)
 			return;
 
-		if (err == -EFSCORRUPTED)
-			goto stop_checkpoint;
-
 		if (err == -ENOMEM || ++count <= DEFAULT_RETRY_IO_COUNT)
 			goto retry;
-stop_checkpoint:
 		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_UPDATE_INODE);
 		return;
 	}
@@ -733,10 +810,8 @@ int f2fs_write_inode(struct inode *inode, struct writeback_control *wbc)
 		!is_inode_flag_set(inode, FI_DIRTY_INODE))
 		return 0;
 
-	if (!f2fs_is_checkpoint_ready(sbi)) {
-		f2fs_mark_inode_dirty_sync(inode, true);
+	if (!f2fs_is_checkpoint_ready(sbi))
 		return -ENOSPC;
-	}
 
 	/*
 	 * We need to balance fs here to prevent from producing dirty node pages
@@ -748,6 +823,33 @@ int f2fs_write_inode(struct inode *inode, struct writeback_control *wbc)
 	return 0;
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+static void f2fs_dec_inner_link(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct inode *inner = NULL;
+	int err = 0;
+
+	inner = get_inner_inode(inode);
+	if (!inner)
+		return;
+
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		err = -ENOSPC;
+	else
+		err = f2fs_acquire_orphan_inode(sbi);
+	if (err) {
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		put_inner_inode(inner);
+		return;
+	}
+	f2fs_drop_deduped_link(inner);
+
+	trace_f2fs_dedup_dec_inner_link(inode, inner);
+	put_inner_inode(inner);
+}
+#endif
+
 /*
  * Called at the last iput() if i_nlink is zero
  */
@@ -812,6 +914,10 @@ void f2fs_evict_inode(struct inode *inode)
 		f2fs_lock_op(sbi);
 		err = f2fs_remove_inode_page(inode);
 		f2fs_unlock_op(sbi);
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_is_outer_inode(inode))
+			f2fs_dec_inner_link(inode);
+#endif
 		if (err == -ENOENT) {
 			err = 0;
 
@@ -840,23 +946,14 @@ void f2fs_evict_inode(struct inode *inode)
 		f2fs_update_inode_page(inode);
 		if (dquot_initialize_needed(inode))
 			set_sbi_flag(sbi, SBI_QUOTA_NEED_REPAIR);
-
-		/*
-		 * If both f2fs_truncate() and f2fs_update_inode_page() failed
-		 * due to fuzzed corrupted inode, call f2fs_inode_synced() to
-		 * avoid triggering later f2fs_bug_on().
-		 */
-		if (is_inode_flag_set(inode, FI_DIRTY_INODE)) {
-			f2fs_warn(sbi,
-				"f2fs_evict_inode: inode is dirty, ino:%lu",
-				inode->i_ino);
-			f2fs_inode_synced(inode);
-			set_sbi_flag(sbi, SBI_NEED_FSCK);
-		}
 	}
 	if (!is_sbi_flag_set(sbi, SBI_IS_FREEZING))
 		sb_end_intwrite(inode->i_sb);
 no_delete:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (F2FS_I(inode)->inner_inode)
+		iput(F2FS_I(inode)->inner_inode);
+#endif
 	dquot_drop(inode);
 
 	stat_dec_inline_xattr(inode);
@@ -869,12 +966,8 @@ void f2fs_evict_inode(struct inode *inode)
 	if (likely(!f2fs_cp_error(sbi) &&
 				!is_sbi_flag_set(sbi, SBI_CP_DISABLED)))
 		f2fs_bug_on(sbi, is_inode_flag_set(inode, FI_DIRTY_INODE));
-
-	/*
-	 * anyway, it needs to remove the inode from sbi->inode_list[DIRTY_META]
-	 * list to avoid UAF in f2fs_sync_inode_meta() during checkpoint.
-	 */
-	f2fs_inode_synced(inode);
+	else
+		f2fs_inode_synced(inode);
 
 	/* for the case f2fs_new_inode() was failed, .i_ino is zero, skip it */
 	if (inode->i_ino)
diff --git a/fs/f2fs/namei.c b/fs/f2fs/namei.c
index d80831037..00df3cbff 100644
--- a/fs/f2fs/namei.c
+++ b/fs/f2fs/namei.c
@@ -23,7 +23,7 @@
 #include <trace/events/f2fs.h>
 
 static inline bool is_extension_exist(const unsigned char *s, const char *sub,
-						bool tmp_ext)
+						bool tmp_ext, bool tmp_dot)
 {
 	size_t slen = strlen(s);
 	size_t sublen = strlen(sub);
@@ -49,13 +49,27 @@ static inline bool is_extension_exist(const unsigned char *s, const char *sub,
 	for (i = 1; i < slen - sublen; i++) {
 		if (s[i] != '.')
 			continue;
-		if (!strncasecmp(s + i + 1, sub, sublen))
-			return true;
+		if (!strncasecmp(s + i + 1, sub, sublen)) {
+			if (!tmp_dot)
+				return true;
+			if (i == slen - sublen - 1 || s[i + 1 + sublen] == '.')
+				return true;
+		}
 	}
 
 	return false;
 }
 
+static inline bool is_temperature_extension(const unsigned char *s, const char *sub)
+{
+	return is_extension_exist(s, sub, true, false);
+}
+
+static inline bool is_compress_extension(const unsigned char *s, const char *sub)
+{
+	return is_extension_exist(s, sub, true, true);
+}
+
 int f2fs_update_extension_list(struct f2fs_sb_info *sbi, const char *name,
 							bool hot, bool set)
 {
@@ -148,7 +162,7 @@ static void set_compress_new_inode(struct f2fs_sb_info *sbi, struct inode *dir,
 	cold_count = le32_to_cpu(sbi->raw_super->extension_count);
 	hot_count = sbi->raw_super->hot_ext_count;
 	for (i = cold_count; i < cold_count + hot_count; i++)
-		if (is_extension_exist(name, extlist[i], false))
+		if (is_temperature_extension(name, extlist[i]))
 			break;
 	f2fs_up_read(&sbi->sb_lock);
 	if (i < (cold_count + hot_count))
@@ -156,12 +170,12 @@ static void set_compress_new_inode(struct f2fs_sb_info *sbi, struct inode *dir,
 
 	/* Don't compress unallowed extension. */
 	for (i = 0; i < noext_cnt; i++)
-		if (is_extension_exist(name, noext[i], false))
+		if (is_compress_extension(name, noext[i]))
 			return;
 
 	/* Compress wanting extension. */
 	for (i = 0; i < ext_cnt; i++) {
-		if (is_extension_exist(name, ext[i], false)) {
+		if (is_compress_extension(name, ext[i])) {
 			set_compress_context(inode);
 			return;
 		}
@@ -189,17 +203,33 @@ static void set_file_temperature(struct f2fs_sb_info *sbi, struct inode *inode,
 	cold_count = le32_to_cpu(sbi->raw_super->extension_count);
 	hot_count = sbi->raw_super->hot_ext_count;
 	for (i = 0; i < cold_count + hot_count; i++)
-		if (is_extension_exist(name, extlist[i], true))
+		if (is_temperature_extension(name, extlist[i]))
 			break;
 	f2fs_up_read(&sbi->sb_lock);
 
 	if (i == cold_count + hot_count)
 		return;
 
-	if (i < cold_count)
+	if (i < cold_count) {
 		file_set_cold(inode);
-	else
+		/* enable seqzone global, except cold files */
+#ifdef CONFIG_F2FS_SEQZONE
+		if (IS_ENCRYPTED(inode) && f2fs_sb_has_seqzone(sbi)) {
+			if (f2fs_inode_support_dedup(sbi, inode))
+				clear_inode_flag(inode, FI_SEQZONE);
+		}
+#endif
+	} else {
 		file_set_hot(inode);
+		/* disable seqzone global, still enable hot db files */
+#ifdef CONFIG_F2FS_SEQZONE
+		if (IS_ENCRYPTED(inode) && sbi->seq_zone == ENABLE_SEQZONE_HOT_FILES &&
+			!f2fs_compressed_file(inode) && f2fs_sb_has_seqzone(sbi)) {
+			if (f2fs_inode_support_dedup(sbi, inode))
+				set_inode_flag(inode, FI_SEQZONE);
+		}
+#endif
+	}
 }
 
 static struct inode *f2fs_new_inode(struct user_namespace *mnt_userns,
@@ -264,7 +294,10 @@ static struct inode *f2fs_new_inode(struct user_namespace *mnt_userns,
 
 	if (f2fs_sb_has_extra_attr(sbi)) {
 		set_inode_flag(inode, FI_EXTRA_ATTR);
-		F2FS_I(inode)->i_extra_isize = F2FS_TOTAL_EXTRA_ATTR_SIZE;
+		if (sbi->oplus_feats & OPLUS_FEAT_DEDUP)
+			F2FS_I(inode)->i_extra_isize = F2FS_TOTAL_EXTRA_ATTR_SIZE;
+		else
+			F2FS_I(inode)->i_extra_isize = F2FS_LEGACY_EXTRA_ATTR_SIZE;
 	}
 
 	if (test_opt(sbi, INLINE_XATTR))
@@ -296,6 +329,16 @@ static struct inode *f2fs_new_inode(struct user_namespace *mnt_userns,
 	/* Check compression first. */
 	set_compress_new_inode(sbi, dir, inode, name);
 
+#ifdef CONFIG_F2FS_SEQZONE
+	/* enable seqzone global */
+	if (f2fs_sb_has_seqzone(sbi) && sbi->seq_zone == ENABLE_SEQZONE_EXCEPT_COLD
+			&& S_ISREG(inode->i_mode) &&
+			IS_ENCRYPTED(inode) && !f2fs_compressed_file(inode)) {
+		if (f2fs_inode_support_dedup(sbi, inode))
+			set_inode_flag(inode, FI_SEQZONE);
+	}
+#endif
+
 	/* Should enable inline_data after compression set */
 	if (test_opt(sbi, INLINE_DATA) && f2fs_may_inline_data(inode))
 		set_inode_flag(inode, FI_INLINE_DATA);
@@ -325,7 +368,6 @@ static struct inode *f2fs_new_inode(struct user_namespace *mnt_userns,
 	trace_f2fs_new_inode(inode, err);
 	dquot_drop(inode);
 	inode->i_flags |= S_NOQUOTA;
-	make_bad_inode(inode);
 	if (nid_free)
 		set_inode_flag(inode, FI_FREE_NID);
 	clear_nlink(inode);
@@ -398,7 +440,7 @@ static int f2fs_link(struct dentry *old_dentry, struct inode *dir,
 
 	if (is_inode_flag_set(dir, FI_PROJ_INHERIT) &&
 			(!projid_eq(F2FS_I(dir)->i_projid,
-			F2FS_I(inode)->i_projid)))
+			F2FS_I(old_dentry->d_inode)->i_projid)))
 		return -EXDEV;
 
 	err = f2fs_dquot_initialize(dir);
@@ -547,15 +589,6 @@ static int f2fs_unlink(struct inode *dir, struct dentry *dentry)
 		goto fail;
 	}
 
-	if (unlikely(inode->i_nlink == 0)) {
-		f2fs_warn(F2FS_I_SB(inode), "%s: inode (ino=%lx) has zero i_nlink",
-			  __func__, inode->i_ino);
-		err = -EFSCORRUPTED;
-		set_sbi_flag(F2FS_I_SB(inode), SBI_NEED_FSCK);
-		f2fs_put_page(page, 0);
-		goto fail;
-	}
-
 	f2fs_balance_fs(sbi, true);
 
 	f2fs_lock_op(sbi);
@@ -578,6 +611,7 @@ static int f2fs_unlink(struct inode *dir, struct dentry *dentry)
 	if (IS_CASEFOLDED(dir))
 		d_invalidate(dentry);
 #endif
+
 	if (IS_DIRSYNC(dir))
 		f2fs_sync_fs(sbi->sb, 1);
 fail:
@@ -899,7 +933,7 @@ static int f2fs_rename(struct user_namespace *mnt_userns, struct inode *old_dir,
 
 	if (is_inode_flag_set(new_dir, FI_PROJ_INHERIT) &&
 			(!projid_eq(F2FS_I(new_dir)->i_projid,
-			F2FS_I(old_inode)->i_projid)))
+			F2FS_I(old_dentry->d_inode)->i_projid)))
 		return -EXDEV;
 
 	/*
@@ -1033,7 +1067,7 @@ static int f2fs_rename(struct user_namespace *mnt_userns, struct inode *old_dir,
 	}
 
 	if (old_dir_entry) {
-		if (old_dir != new_dir)
+		if (old_dir != new_dir && !whiteout)
 			f2fs_set_link(old_inode, old_dir_entry,
 						old_dir_page, new_dir);
 		else
@@ -1088,10 +1122,10 @@ static int f2fs_cross_rename(struct inode *old_dir, struct dentry *old_dentry,
 
 	if ((is_inode_flag_set(new_dir, FI_PROJ_INHERIT) &&
 			!projid_eq(F2FS_I(new_dir)->i_projid,
-			F2FS_I(old_inode)->i_projid)) ||
-	    (is_inode_flag_set(old_dir, FI_PROJ_INHERIT) &&
+			F2FS_I(old_dentry->d_inode)->i_projid)) ||
+	    (is_inode_flag_set(new_dir, FI_PROJ_INHERIT) &&
 			!projid_eq(F2FS_I(old_dir)->i_projid,
-			F2FS_I(new_inode)->i_projid)))
+			F2FS_I(new_dentry->d_inode)->i_projid)))
 		return -EXDEV;
 
 	err = f2fs_dquot_initialize(old_dir);
@@ -1332,3 +1366,36 @@ const struct inode_operations f2fs_special_inode_operations = {
 	.set_acl	= f2fs_set_acl,
 	.listxattr	= f2fs_listxattr,
 };
+
+void f2fs_update_atime(struct inode *inode, bool oneshot)
+{
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+
+	if (!f2fs_compressed_file(inode))
+		return;
+
+	if (!sb_start_write_trylock(inode->i_sb))
+		return;
+
+	if (!inode_trylock(inode))
+		goto out_unlock_sb;
+
+	if (IS_RDONLY(inode))
+		goto out_unlock_inode;
+
+	if (fi->i_compress_flag & COMPRESS_ATIME_MASK) {
+		struct timespec64 now;
+
+		if (oneshot)
+			fi->i_compress_flag &= ~COMPRESS_ATIME_MASK;
+		now = current_time(inode);
+		inode_update_time(inode, &now, S_ATIME);
+	}
+
+out_unlock_inode:
+	inode_unlock(inode);
+out_unlock_sb:
+	sb_end_write(inode->i_sb);
+#endif
+}
diff --git a/fs/f2fs/node.c b/fs/f2fs/node.c
index b0620e377..89af08bdc 100644
--- a/fs/f2fs/node.c
+++ b/fs/f2fs/node.c
@@ -753,6 +753,38 @@ static int get_node_path(struct inode *inode, long block,
 	return level;
 }
 
+#ifdef CONFIG_F2FS_SEQZONE
+u32 seqzone_index(struct inode *inode,
+			struct page *node_page, unsigned int offset)
+{
+	struct f2fs_node *raw_node;
+	__le32 *addr_array;
+	int base = 0;
+	bool is_inode = IS_INODE(node_page);
+	int addrs = DEF_ADDRS_PER_BLOCK / 2;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	raw_node = F2FS_NODE(node_page);
+
+	if (is_inode) {
+		int xattr_addrs = 0;
+		if (f2fs_sb_has_flexible_inline_xattr(sbi))
+			xattr_addrs = le16_to_cpu(raw_node->i.i_inline_xattr_size);
+		else if (raw_node->i.i_inline & F2FS_INLINE_XATTR)
+			xattr_addrs = DEFAULT_INLINE_XATTR_ADDRS;
+
+		if (!inode)
+			/* from GC path only */
+			base = offset_in_addr(&raw_node->i);
+		else if (f2fs_has_extra_attr(inode))
+			base = get_extra_isize(inode);
+		addrs = (DEF_ADDRS_PER_INODE - base - xattr_addrs) / 2;
+	}
+
+	addr_array = blkaddr_in_node(raw_node);
+	return le32_to_cpu(addr_array[base + offset + addrs]);
+}
+#endif
+
 /*
  * Caller should call f2fs_put_dnode(dn).
  * Also, it should grab and release a rwsem by calling f2fs_lock_op() and
@@ -799,16 +831,6 @@ int f2fs_get_dnode_of_data(struct dnode_of_data *dn, pgoff_t index, int mode)
 	for (i = 1; i <= level; i++) {
 		bool done = false;
 
-		if (nids[i] && nids[i] == dn->inode->i_ino) {
-			err = -EFSCORRUPTED;
-			f2fs_err(sbi,
-				"inode mapping table is corrupted, run fsck to fix it, "
-				"ino:%lu, nid:%u, level:%d, offset:%d",
-				dn->inode->i_ino, nids[i], level, offset[level]);
-			set_sbi_flag(sbi, SBI_NEED_FSCK);
-			goto release_pages;
-		}
-
 		if (!nids[i] && mode == ALLOC_NODE) {
 			/* alloc new node */
 			if (!f2fs_alloc_nid(sbi, &(nids[i]))) {
@@ -886,6 +908,11 @@ int f2fs_get_dnode_of_data(struct dnode_of_data *dn, pgoff_t index, int mode)
 		f2fs_update_read_extent_tree_range_compressed(dn->inode,
 					fofs, blkaddr, cluster_size, c_len);
 	}
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(dn->inode))
+		dn->seqzone_index = seqzone_index(dn->inode,
+					dn->node_page, dn->ofs_in_node);
+#endif
 out:
 	return 0;
 
@@ -916,7 +943,7 @@ static int truncate_node(struct dnode_of_data *dn)
 		return err;
 
 	/* Deallocate node address */
-	f2fs_invalidate_blocks(sbi, ni.blk_addr, 1);
+	f2fs_invalidate_blocks(sbi, ni.blk_addr);
 	dec_valid_node_count(sbi, dn->inode, dn->nid == dn->inode->i_ino);
 	set_node_addr(sbi, &ni, NULL_ADDR, false);
 
@@ -1124,14 +1151,7 @@ int f2fs_truncate_inode_blocks(struct inode *inode, pgoff_t from)
 	trace_f2fs_truncate_inode_blocks_enter(inode, from);
 
 	level = get_node_path(inode, from, offset, noffset);
-	if (level <= 0) {
-		if (!level) {
-			level = -EFSCORRUPTED;
-			f2fs_err(sbi, "%s: inode ino=%lx has corrupted node block, from:%lu addrs:%u",
-					__func__, inode->i_ino,
-					from, ADDRS_PER_INODE(inode));
-			set_sbi_flag(sbi, SBI_NEED_FSCK);
-		}
+	if (level < 0) {
 		trace_f2fs_truncate_inode_blocks_exit(inode, level);
 		return level;
 	}
@@ -2740,7 +2760,7 @@ int f2fs_recover_xattr_data(struct inode *inode, struct page *page)
 	if (err)
 		return err;
 
-	f2fs_invalidate_blocks(sbi, ni.blk_addr, 1);
+	f2fs_invalidate_blocks(sbi, ni.blk_addr);
 	dec_valid_node_count(sbi, inode, false);
 	set_node_addr(sbi, &ni, NULL_ADDR, false);
 
@@ -2760,11 +2780,11 @@ int f2fs_recover_xattr_data(struct inode *inode, struct page *page)
 	f2fs_update_inode_page(inode);
 
 	/* 3: update and set xattr node page dirty */
-	if (page) {
+	if (page)
 		memcpy(F2FS_NODE(xpage), F2FS_NODE(page),
 				VALID_XATTR_BLOCK_SIZE);
-		set_page_dirty(xpage);
-	}
+
+	set_page_dirty(xpage);
 	f2fs_put_page(xpage, 1);
 
 	return 0;
diff --git a/fs/f2fs/node.h b/fs/f2fs/node.h
index 906fb67a9..c5d23403a 100644
--- a/fs/f2fs/node.h
+++ b/fs/f2fs/node.h
@@ -428,3 +428,7 @@ static inline void set_mark(struct page *page, int mark, int type)
 }
 #define set_dentry_mark(page, mark)	set_mark(page, mark, DENT_BIT_SHIFT)
 #define set_fsync_mark(page, mark)	set_mark(page, mark, FSYNC_BIT_SHIFT)
+#ifdef CONFIG_F2FS_SEQZONE
+u32 seqzone_index(struct inode *inode,
+			struct page *node_page, unsigned int offset);
+#endif
\ No newline at end of file
diff --git a/fs/f2fs/recovery.c b/fs/f2fs/recovery.c
index 1b6c41f86..03a4f615b 100644
--- a/fs/f2fs/recovery.c
+++ b/fs/f2fs/recovery.c
@@ -287,6 +287,9 @@ static void recover_inline_flags(struct inode *inode, struct f2fs_inode *ri)
 static int recover_inode(struct inode *inode, struct page *page)
 {
 	struct f2fs_inode *raw = F2FS_INODE(page);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+#endif
 	char *name;
 	int err;
 
@@ -334,6 +337,10 @@ static int recover_inode(struct inode *inode, struct page *page)
 				le16_to_cpu(raw->i_gc_failures);
 
 	recover_inline_flags(inode, raw);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_inode_support_dedup(sbi, inode))
+		get_dedup_flags_info(inode, raw);
+#endif
 
 	f2fs_mark_inode_dirty_sync(inode, true);
 
@@ -730,7 +737,10 @@ static int do_recover_data(struct f2fs_sb_info *sbi, struct inode *inode,
 						ERROR_INVALID_BLKADDR);
 				goto err;
 			}
-
+#ifdef CONFIG_F2FS_SEQZONE
+			if (f2fs_seqzone_file(dn.inode))
+				dn.seqzone_index = seqzone_index(dn.inode, page, dn.ofs_in_node);
+#endif
 			/* write dummy data page */
 			f2fs_replace_block(sbi, &dn, src, dest,
 						ni.version, false, false);
diff --git a/fs/f2fs/segment.c b/fs/f2fs/segment.c
index bd0fbc88e..7e185dcba 100644
--- a/fs/f2fs/segment.c
+++ b/fs/f2fs/segment.c
@@ -201,12 +201,6 @@ void f2fs_abort_atomic_write(struct inode *inode, bool clean)
 	clear_inode_flag(inode, FI_ATOMIC_FILE);
 	if (is_inode_flag_set(inode, FI_ATOMIC_DIRTIED)) {
 		clear_inode_flag(inode, FI_ATOMIC_DIRTIED);
-		/*
-		 * The vfs inode keeps clean during commit, but the f2fs inode
-		 * doesn't. So clear the dirty state after commit and let
-		 * f2fs_mark_inode_dirty_sync ensure a consistent dirty state.
-		 */
-		f2fs_inode_synced(inode);
 		f2fs_mark_inode_dirty_sync(inode, true);
 	}
 	stat_dec_atomic_inode(inode);
@@ -222,7 +216,7 @@ void f2fs_abort_atomic_write(struct inode *inode, bool clean)
 }
 
 static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
-			block_t new_addr, block_t *old_addr, bool recover)
+			block_t new_addr, struct revoke_entry *re, bool recover)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct dnode_of_data dn;
@@ -247,11 +241,15 @@ static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
 	}
 
 	if (recover) {
+#ifdef CONFIG_F2FS_SEQZONE
+		if (f2fs_seqzone_file(dn.inode))
+			dn.seqzone_index = re->old_seqzone_index;
+#endif
 		/* dn.data_blkaddr is always valid */
 		if (!__is_valid_data_blkaddr(new_addr)) {
 			if (new_addr == NULL_ADDR)
 				dec_valid_block_count(sbi, inode, 1);
-			f2fs_invalidate_blocks(sbi, dn.data_blkaddr, 1);
+			f2fs_invalidate_blocks(sbi, dn.data_blkaddr);
 			f2fs_update_data_blkaddr(&dn, new_addr);
 		} else {
 			f2fs_replace_block(sbi, &dn, dn.data_blkaddr,
@@ -259,6 +257,11 @@ static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
 		}
 	} else {
 		blkcnt_t count = 1;
+#ifdef CONFIG_F2FS_SEQZONE
+		pgoff_t new_seqzone_index;
+		if (f2fs_seqzone_file(dn.inode))
+			new_seqzone_index = re->old_seqzone_index;
+#endif
 
 		err = inc_valid_block_count(sbi, inode, &count, true);
 		if (err) {
@@ -266,7 +269,13 @@ static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
 			return err;
 		}
 
-		*old_addr = dn.data_blkaddr;
+		re->old_addr = dn.data_blkaddr;
+#ifdef CONFIG_F2FS_SEQZONE
+		if (f2fs_seqzone_file(dn.inode)) {
+			re->old_seqzone_index = dn.seqzone_index;
+			dn.seqzone_index = new_seqzone_index;
+		}
+#endif
 		f2fs_truncate_data_blocks_range(&dn, 1);
 		dec_valid_block_count(sbi, F2FS_I(inode)->cow_inode, count);
 
@@ -277,7 +286,7 @@ static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
 	f2fs_put_dnode(&dn);
 
 	trace_f2fs_replace_atomic_write_block(inode, F2FS_I(inode)->cow_inode,
-			index, old_addr ? *old_addr : 0, new_addr, recover);
+			index, re ? re->old_addr : 0, new_addr, recover);
 	return 0;
 }
 
@@ -291,7 +300,7 @@ static void __complete_revoke_list(struct inode *inode, struct list_head *head,
 	list_for_each_entry_safe(cur, tmp, head, list) {
 		if (revoke) {
 			__replace_atomic_write_block(inode, cur->index,
-						cur->old_addr, NULL, true);
+						cur->old_addr, cur, true);
 		} else if (truncate) {
 			f2fs_truncate_hole(inode, start_index, cur->index);
 			start_index = cur->index + 1;
@@ -353,9 +362,14 @@ static int __f2fs_commit_atomic_write(struct inode *inode)
 
 			new = f2fs_kmem_cache_alloc(revoke_entry_slab, GFP_NOFS,
 							true, NULL);
+#ifdef CONFIG_F2FS_SEQZONE
+			if (f2fs_seqzone_file(dn.inode))
+				new->old_seqzone_index =
+					seqzone_index(dn.inode, dn.node_page, dn.ofs_in_node);
+#endif
 
 			ret = __replace_atomic_write_block(inode, index, blkaddr,
-							&new->old_addr, false);
+							new, false);
 			if (ret) {
 				f2fs_put_dnode(&dn);
 				kmem_cache_free(revoke_entry_slab, new);
@@ -1190,7 +1204,10 @@ static void __init_discard_policy(struct f2fs_sb_info *sbi,
 		dpolicy->min_interval = dcc->min_discard_issue_time;
 		dpolicy->mid_interval = dcc->mid_discard_issue_time;
 		dpolicy->max_interval = dcc->max_discard_issue_time;
-		dpolicy->io_aware = true;
+		if (dcc->discard_io_aware == DPOLICY_IO_AWARE_ENABLE)
+			dpolicy->io_aware = true;
+		else if (dcc->discard_io_aware == DPOLICY_IO_AWARE_DISABLE)
+			dpolicy->io_aware = false;
 		dpolicy->sync = false;
 		dpolicy->ordered = true;
 		if (utilization(sbi) > dcc->discard_urgent_util) {
@@ -2197,6 +2214,7 @@ static int create_discard_cmd_control(struct f2fs_sb_info *sbi)
 
 	dcc->discard_io_aware_gran = MAX_PLIST_NUM;
 	dcc->discard_granularity = DEFAULT_DISCARD_GRANULARITY;
+	dcc->discard_io_aware = DPOLICY_IO_AWARE_ENABLE;
 	dcc->max_ordered_discard = DEFAULT_MAX_ORDERED_DISCARD_GRANULARITY;
 	if (F2FS_OPTION(sbi).discard_unit == DISCARD_UNIT_SEGMENT)
 		dcc->discard_granularity = sbi->blocks_per_seg;
@@ -2310,38 +2328,78 @@ static void update_segment_mtime(struct f2fs_sb_info *sbi, block_t blkaddr,
 		SIT_I(sbi)->max_mtime = ctime;
 }
 
-/*
- * NOTE: when updating multiple blocks at the same time, please ensure
- * that the consecutive input blocks belong to the same segment.
- */
-static int update_sit_entry_for_release(struct f2fs_sb_info *sbi, struct seg_entry *se,
-				block_t blkaddr, unsigned int offset, int del)
+static void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)
 {
+	struct seg_entry *se;
+	unsigned int segno, offset;
+	long int new_vblocks;
 	bool exist;
 #ifdef CONFIG_F2FS_CHECK_FS
 	bool mir_exist;
 #endif
-	int i;
-	int del_count = -del;
 
-	f2fs_bug_on(sbi, GET_SEGNO(sbi, blkaddr) != GET_SEGNO(sbi, blkaddr + del_count - 1));
+	segno = GET_SEGNO(sbi, blkaddr);
+	if (segno == NULL_SEGNO)
+		return;
 
-	for (i = 0; i < del_count; i++) {
-		exist = f2fs_test_and_clear_bit(offset + i, se->cur_valid_map);
+	se = get_seg_entry(sbi, segno);
+	new_vblocks = se->valid_blocks + del;
+	offset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);
+
+	f2fs_bug_on(sbi, (new_vblocks < 0 ||
+			(new_vblocks > f2fs_usable_blks_in_seg(sbi, segno))));
+
+	se->valid_blocks = new_vblocks;
+
+	/* Update valid block bitmap */
+	if (del > 0) {
+		exist = f2fs_test_and_set_bit(offset, se->cur_valid_map);
 #ifdef CONFIG_F2FS_CHECK_FS
-		mir_exist = f2fs_test_and_clear_bit(offset + i,
+		mir_exist = f2fs_test_and_set_bit(offset,
+						se->cur_valid_map_mir);
+		if (unlikely(exist != mir_exist)) {
+			f2fs_err(sbi, "Inconsistent error when setting bitmap, blk:%u, old bit:%d",
+				 blkaddr, exist);
+			f2fs_bug_on(sbi, 1);
+		}
+#endif
+		if (unlikely(exist)) {
+			f2fs_err(sbi, "Bitmap was wrongly set, blk:%u",
+				 blkaddr);
+			f2fs_bug_on(sbi, 1);
+			se->valid_blocks--;
+			del = 0;
+		}
+
+		if (f2fs_block_unit_discard(sbi) &&
+				!f2fs_test_and_set_bit(offset, se->discard_map))
+			sbi->discard_blks--;
+
+		/*
+		 * SSR should never reuse block which is checkpointed
+		 * or newly invalidated.
+		 */
+		if (!is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {
+			if (!f2fs_test_and_set_bit(offset, se->ckpt_valid_map))
+				se->ckpt_valid_blocks++;
+		}
+	} else {
+		exist = f2fs_test_and_clear_bit(offset, se->cur_valid_map);
+#ifdef CONFIG_F2FS_CHECK_FS
+		mir_exist = f2fs_test_and_clear_bit(offset,
 						se->cur_valid_map_mir);
 		if (unlikely(exist != mir_exist)) {
 			f2fs_err(sbi, "Inconsistent error when clearing bitmap, blk:%u, old bit:%d",
-				blkaddr + i, exist);
+				 blkaddr, exist);
 			f2fs_bug_on(sbi, 1);
 		}
 #endif
 		if (unlikely(!exist)) {
-			f2fs_err(sbi, "Bitmap was wrongly cleared, blk:%u", blkaddr + i);
+			f2fs_err(sbi, "Bitmap was wrongly cleared, blk:%u",
+				 blkaddr);
 			f2fs_bug_on(sbi, 1);
 			se->valid_blocks++;
-			del += 1;
+			del = 0;
 		} else if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {
 			/*
 			 * If checkpoints are off, we must not reuse data that
@@ -2349,7 +2407,7 @@ static int update_sit_entry_for_release(struct f2fs_sb_info *sbi, struct seg_ent
 			 * before, we must track that to know how much space we
 			 * really have.
 			 */
-			if (f2fs_test_bit(offset + i, se->ckpt_valid_map)) {
+			if (f2fs_test_bit(offset, se->ckpt_valid_map)) {
 				spin_lock(&sbi->stat_lock);
 				sbi->unusable_block_count++;
 				spin_unlock(&sbi->stat_lock);
@@ -2357,91 +2415,12 @@ static int update_sit_entry_for_release(struct f2fs_sb_info *sbi, struct seg_ent
 		}
 
 		if (f2fs_block_unit_discard(sbi) &&
-				f2fs_test_and_clear_bit(offset + i, se->discard_map))
+			f2fs_test_and_clear_bit(offset, se->discard_map))
 			sbi->discard_blks++;
-
-		if (!f2fs_test_bit(offset + i, se->ckpt_valid_map))
-			se->ckpt_valid_blocks -= 1;
-	}
-
-	return del;
-}
-
-static int update_sit_entry_for_alloc(struct f2fs_sb_info *sbi, struct seg_entry *se,
-				block_t blkaddr, unsigned int offset, int del)
-{
-	bool exist;
-#ifdef CONFIG_F2FS_CHECK_FS
-	bool mir_exist;
-#endif
-
-	exist = f2fs_test_and_set_bit(offset, se->cur_valid_map);
-#ifdef CONFIG_F2FS_CHECK_FS
-	mir_exist = f2fs_test_and_set_bit(offset,
-					se->cur_valid_map_mir);
-	if (unlikely(exist != mir_exist)) {
-		f2fs_err(sbi, "Inconsistent error when setting bitmap, blk:%u, old bit:%d",
-			blkaddr, exist);
-		f2fs_bug_on(sbi, 1);
-	}
-#endif
-	if (unlikely(exist)) {
-		f2fs_err(sbi, "Bitmap was wrongly set, blk:%u", blkaddr);
-		f2fs_bug_on(sbi, 1);
-		se->valid_blocks--;
-		del = 0;
-	}
-
-	if (f2fs_block_unit_discard(sbi) &&
-			!f2fs_test_and_set_bit(offset, se->discard_map))
-		sbi->discard_blks--;
-
-	/*
-	 * SSR should never reuse block which is checkpointed
-	 * or newly invalidated.
-	 */
-	if (!is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {
-		if (!f2fs_test_and_set_bit(offset, se->ckpt_valid_map))
-			se->ckpt_valid_blocks++;
 	}
-
 	if (!f2fs_test_bit(offset, se->ckpt_valid_map))
 		se->ckpt_valid_blocks += del;
 
-	return del;
-}
-
-/*
- * If releasing blocks, this function supports updating multiple consecutive blocks
- * at one time, but please note that these consecutive blocks need to belong to the
- * same segment.
- */
-static void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)
-{
-	struct seg_entry *se;
-	unsigned int segno, offset;
-	long int new_vblocks;
-
-	segno = GET_SEGNO(sbi, blkaddr);
-	if (segno == NULL_SEGNO)
-		return;
-
-	se = get_seg_entry(sbi, segno);
-	new_vblocks = se->valid_blocks + del;
-	offset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);
-
-	f2fs_bug_on(sbi, (new_vblocks < 0 ||
-			(new_vblocks > f2fs_usable_blks_in_seg(sbi, segno))));
-
-	se->valid_blocks = new_vblocks;
-
-	/* Update valid block bitmap */
-	if (del > 0) {
-		del = update_sit_entry_for_alloc(sbi, se, blkaddr, offset, del);
-	} else {
-		del = update_sit_entry_for_release(sbi, se, blkaddr, offset, del);
-	}
-
 	__mark_sit_entry_dirty(sbi, segno);
 
 	/* update total number of valid blocks to be written in ckpt area */
@@ -2451,43 +2430,25 @@ static void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)
 		get_sec_entry(sbi, segno)->valid_blocks += del;
 }
 
-void f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr,
-				unsigned int len)
+void f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr)
 {
 	unsigned int segno = GET_SEGNO(sbi, addr);
 	struct sit_info *sit_i = SIT_I(sbi);
-	block_t addr_start = addr, addr_end = addr + len - 1;
-	unsigned int seg_num = GET_SEGNO(sbi, addr_end) - segno + 1;
-	unsigned int i = 1, max_blocks = sbi->blocks_per_seg, cnt;
 
 	f2fs_bug_on(sbi, addr == NULL_ADDR);
 	if (addr == NEW_ADDR || addr == COMPRESS_ADDR)
 		return;
 
-	f2fs_invalidate_internal_cache(sbi, addr, len);
+	f2fs_invalidate_internal_cache(sbi, addr);
 
 	/* add it into sit main buffer */
 	down_write(&sit_i->sentry_lock);
 
-	if (seg_num == 1)
-		cnt = len;
-	else
-		cnt = max_blocks - GET_BLKOFF_FROM_SEG0(sbi, addr);
-
-	do {
-		update_segment_mtime(sbi, addr_start, 0);
-		update_sit_entry(sbi, addr_start, -cnt);
+	update_segment_mtime(sbi, addr, 0);
+	update_sit_entry(sbi, addr, -1);
 
-		/* add it into dirty seglist */
-		locate_dirty_segment(sbi, segno);
-
-		/* update @addr_start and @cnt and @segno */
-		addr_start = START_BLOCK(sbi, ++segno);
-		if (++i == seg_num)
-			cnt = GET_BLKOFF_FROM_SEG0(sbi, addr_end) + 1;
-		else
-			cnt = max_blocks;
-	} while (i <= seg_num);
+	/* add it into dirty seglist */
+	locate_dirty_segment(sbi, segno);
 
 	up_write(&sit_i->sentry_lock);
 }
@@ -2741,7 +2702,7 @@ static unsigned int __get_next_segno(struct f2fs_sb_info *sbi, int type)
 		return SIT_I(sbi)->last_victim[ALLOC_NEXT];
 
 	/* find segments from 0 to reuse freed segments */
-	if (f2fs_get_alloc_mode(sbi) == ALLOC_MODE_REUSE)
+	if (F2FS_OPTION(sbi).alloc_mode == ALLOC_MODE_REUSE)
 		return 0;
 
 	return curseg->segno;
@@ -2879,9 +2840,33 @@ static void __f2fs_init_atgc_curseg(struct f2fs_sb_info *sbi)
 	f2fs_up_read(&SM_I(sbi)->curseg_lock);
 
 }
+
+#ifdef CONFIG_F2FS_SEQZONE
+void __f2fs_init_seqzone_curseg(struct f2fs_sb_info *sbi)
+{
+	int i;
+	struct curseg_info *curseg;
+	for (i = CURSEG_HOT_DATA_0; i <= CURSEG_WARM_DATA_7; i++) {
+		curseg = CURSEG_I(sbi, i);
+		f2fs_down_read(&SM_I(sbi)->curseg_lock);
+		mutex_lock(&curseg->curseg_mutex);
+		down_write(&SIT_I(sbi)->sentry_lock);
+		new_curseg(sbi, i, true);
+		stat_inc_seg_type(sbi, curseg);
+		up_write(&SIT_I(sbi)->sentry_lock);
+		mutex_unlock(&curseg->curseg_mutex);
+		f2fs_up_read(&SM_I(sbi)->curseg_lock);
+	}
+}
+#endif
+
 void f2fs_init_inmem_curseg(struct f2fs_sb_info *sbi)
 {
 	__f2fs_init_atgc_curseg(sbi);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi))
+		__f2fs_init_seqzone_curseg(sbi);
+#endif
 }
 
 static void __f2fs_save_inmem_curseg(struct f2fs_sb_info *sbi, int type)
@@ -2907,6 +2892,14 @@ static void __f2fs_save_inmem_curseg(struct f2fs_sb_info *sbi, int type)
 void f2fs_save_inmem_curseg(struct f2fs_sb_info *sbi)
 {
 	__f2fs_save_inmem_curseg(sbi, CURSEG_COLD_DATA_PINNED);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi)) {
+		int i;
+		for (i = CURSEG_HOT_DATA_0; i <= CURSEG_WARM_DATA_7; i++) {
+			__f2fs_save_inmem_curseg(sbi, i);
+		}
+	}
+#endif
 
 	if (sbi->am.atgc_enabled)
 		__f2fs_save_inmem_curseg(sbi, CURSEG_ALL_DATA_ATGC);
@@ -2932,6 +2925,14 @@ static void __f2fs_restore_inmem_curseg(struct f2fs_sb_info *sbi, int type)
 void f2fs_restore_inmem_curseg(struct f2fs_sb_info *sbi)
 {
 	__f2fs_restore_inmem_curseg(sbi, CURSEG_COLD_DATA_PINNED);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi)) {
+		int i;
+		for (i = CURSEG_HOT_DATA_0; i <= CURSEG_WARM_DATA_7; i++) {
+			__f2fs_restore_inmem_curseg(sbi, i);
+		}
+	}
+#endif
 
 	if (sbi->am.atgc_enabled)
 		__f2fs_restore_inmem_curseg(sbi, CURSEG_ALL_DATA_ATGC);
@@ -3374,14 +3375,33 @@ static void f2fs_randomize_chunk(struct f2fs_sb_info *sbi,
 int f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,
 		block_t old_blkaddr, block_t *new_blkaddr,
 		struct f2fs_summary *sum, int type,
-		struct f2fs_io_info *fio)
+		struct f2fs_io_info *fio, bool use_seqzone)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
-	struct curseg_info *curseg = CURSEG_I(sbi, type);
+	struct curseg_info *curseg;
 	unsigned long long old_mtime;
 	bool from_gc = (type == CURSEG_ALL_DATA_ATGC);
 	struct seg_entry *se = NULL;
 	bool segment_full = false;
+	int real_type = type;
+#ifdef CONFIG_F2FS_SEQZONE
+	bool is_seqzone_file = false;
+	if (fio && fio->use_seqzone) {
+		f2fs_seqzone_fio_check(fio);
+		is_seqzone_file = true;
+	}
+
+	if ((is_seqzone_file || use_seqzone) &&
+		IS_DATASEG(type) && (CURSEG_COLD_DATA != type)) {
+		real_type = (type - CURSEG_HOT_DATA) * F2FS_NR_CPUS +
+					smp_processor_id() + CURSEG_HOT_DATA_0;
+		curseg = CURSEG_I(sbi, real_type);
+		// inmem curseg has not been inited, rollback to origin curseg.
+		if (!curseg->inited)
+			real_type = type;
+	}
+#endif
+	curseg = CURSEG_I(sbi, real_type);
 
 	f2fs_down_read(&SM_I(sbi)->curseg_lock);
 
@@ -3439,10 +3459,10 @@ int f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,
 			get_atssr_segment(sbi, type, se->type,
 						AT_SSR, se->mtime);
 		} else {
-			if (need_new_seg(sbi, type))
-				new_curseg(sbi, type, false);
+			if (need_new_seg(sbi, real_type))
+				new_curseg(sbi, real_type, false);
 			else
-				change_curseg(sbi, type);
+				change_curseg(sbi, real_type);
 			stat_inc_seg_type(sbi, curseg);
 		}
 
@@ -3470,13 +3490,19 @@ int f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,
 
 	if (fio) {
 		struct f2fs_bio_info *io;
+		int real_temp = fio->temp;
+#ifdef CONFIG_F2FS_SEQZONE
+		real_temp = (fio->use_seqzone && fio->type == DATA && fio->temp != COLD) ?
+					fio->temp * F2FS_NR_CPUS + smp_processor_id() : fio->temp;
+		fio->real_temp = real_temp;
+#endif
 
 		if (F2FS_IO_ALIGNED(sbi))
 			fio->retry = 0;
 
 		INIT_LIST_HEAD(&fio->list);
 		fio->in_list = 1;
-		io = sbi->write_io[fio->type] + fio->temp;
+		io = sbi->write_io[fio->type] + real_temp;
 		spin_lock(&io->io_lock);
 		list_add_tail(&fio->list, &io->io_list);
 		spin_unlock(&io->io_lock);
@@ -3530,7 +3556,7 @@ static void do_write_page(struct f2fs_summary *sum, struct f2fs_io_info *fio)
 		f2fs_down_read(&fio->sbi->io_order_lock);
 reallocate:
 	if (f2fs_allocate_data_block(fio->sbi, fio->page, fio->old_blkaddr,
-			&fio->new_blkaddr, sum, type, fio)) {
+			&fio->new_blkaddr, sum, type, fio, 0)) {
 		if (fscrypt_inode_uses_fs_layer_crypto(fio->page->mapping->host))
 			fscrypt_finalize_bounce_page(&fio->encrypted_page);
 		if (PageWriteback(fio->page))
@@ -3540,8 +3566,11 @@ static void do_write_page(struct f2fs_summary *sum, struct f2fs_io_info *fio)
 		goto out;
 	}
 
-	if (GET_SEGNO(fio->sbi, fio->old_blkaddr) != NULL_SEGNO)
-		f2fs_invalidate_internal_cache(fio->sbi, fio->old_blkaddr, 1);
+	if (GET_SEGNO(fio->sbi, fio->old_blkaddr) != NULL_SEGNO) {
+		invalidate_mapping_pages(META_MAPPING(fio->sbi),
+					fio->old_blkaddr, fio->old_blkaddr);
+		f2fs_invalidate_compress_page(fio->sbi, fio->old_blkaddr);
+	}
 
 	/* writeout dirty page into bdev */
 	f2fs_submit_page_write(fio);
@@ -3603,6 +3632,10 @@ void f2fs_outplace_write_data(struct dnode_of_data *dn,
 		f2fs_update_age_extent_cache(dn);
 	set_summary(&sum, dn->nid, dn->ofs_in_node, fio->version);
 	do_write_page(&sum, fio);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone)
+		dn->seqzone_index = fio->seqzone_index;
+#endif
 	f2fs_update_data_blkaddr(dn, fio->new_blkaddr);
 
 	f2fs_update_iostat(sbi, dn->inode, fio->io_type, F2FS_BLKSIZE);
@@ -3736,7 +3769,7 @@ void f2fs_do_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		update_sit_entry(sbi, new_blkaddr, 1);
 	}
 	if (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO) {
-		f2fs_invalidate_internal_cache(sbi, old_blkaddr, 1);
+		f2fs_invalidate_internal_cache(sbi, old_blkaddr);
 		if (!from_gc)
 			update_segment_mtime(sbi, old_blkaddr, 0);
 		update_sit_entry(sbi, old_blkaddr, -1);
@@ -4514,6 +4547,10 @@ static int build_curseg(struct f2fs_sb_info *sbi)
 			array[i].seg_type = CURSEG_COLD_DATA;
 		else if (i == CURSEG_ALL_DATA_ATGC)
 			array[i].seg_type = CURSEG_COLD_DATA;
+#ifdef CONFIG_F2FS_SEQZONE
+		else if (i >= CURSEG_HOT_DATA_0 && i <= CURSEG_WARM_DATA_7)
+			array[i].seg_type = (i - CURSEG_HOT_DATA_0) / 8 + CURSEG_HOT_DATA;
+#endif
 		array[i].segno = NULL_SEGNO;
 		array[i].next_blkoff = 0;
 		array[i].inited = false;
@@ -5428,8 +5465,16 @@ int __init f2fs_create_segment_manager_caches(void)
 			sizeof(struct revoke_entry));
 	if (!revoke_entry_slab)
 		goto destroy_sit_entry_set;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (create_page_info_slab())
+		goto destroy_revoke_entry;
+#endif
 	return 0;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+destroy_revoke_entry:
+	kmem_cache_destroy(revoke_entry_slab);
+#endif
 destroy_sit_entry_set:
 	kmem_cache_destroy(sit_entry_set_slab);
 destroy_discard_cmd:
@@ -5446,4 +5491,7 @@ void f2fs_destroy_segment_manager_caches(void)
 	kmem_cache_destroy(discard_cmd_slab);
 	kmem_cache_destroy(discard_entry_slab);
 	kmem_cache_destroy(revoke_entry_slab);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	destroy_page_info_slab();
+#endif
 }
diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 904625112..7c5c8148a 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -36,6 +36,28 @@ static inline void sanity_check_seg_type(struct f2fs_sb_info *sbi,
 #define IS_WARM(t)	((t) == CURSEG_WARM_NODE || (t) == CURSEG_WARM_DATA)
 #define IS_COLD(t)	((t) == CURSEG_COLD_NODE || (t) == CURSEG_COLD_DATA)
 
+#ifdef CONFIG_F2FS_SEQZONE
+#define IS_CURSEG_SEQZONE(sbi, seg)	(\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_0)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_1)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_2)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_3)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_4)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_5)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_6)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_7)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_0)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_1)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_2)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_3)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_4)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_5)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_6)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_7)->segno))
+#else
+#define IS_CURSEG_SEQZONE(sbi, seg)	(false)
+#endif
+
 #define IS_CURSEG(sbi, seg)						\
 	(((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno) ||	\
 	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno) ||	\
@@ -44,7 +66,46 @@ static inline void sanity_check_seg_type(struct f2fs_sb_info *sbi,
 	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno) ||	\
 	 ((seg) == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno) ||	\
 	 ((seg) == CURSEG_I(sbi, CURSEG_COLD_DATA_PINNED)->segno) ||	\
-	 ((seg) == CURSEG_I(sbi, CURSEG_ALL_DATA_ATGC)->segno))
+	 ((seg) == CURSEG_I(sbi, CURSEG_ALL_DATA_ATGC)->segno) ||	\
+	IS_CURSEG_SEQZONE(sbi, (seg)))
+
+#ifdef CONFIG_F2FS_SEQZONE
+#define IS_CURSEC_SEQZONE(sbi, secno)	(	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_0)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_1)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_2)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_3)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_4)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_5)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_6)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_7)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_0)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_1)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_2)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_3)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_4)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_5)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_6)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_7)->segno /		\
+	  (sbi)->segs_per_sec))
+#else
+#define IS_CURSEC_SEQZONE(sbi, secno)	(false)
+#endif
 
 #define IS_CURSEC(sbi, secno)						\
 	(((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno /		\
@@ -62,7 +123,9 @@ static inline void sanity_check_seg_type(struct f2fs_sb_info *sbi,
 	 ((secno) == CURSEG_I(sbi, CURSEG_COLD_DATA_PINNED)->segno /	\
 	  (sbi)->segs_per_sec) ||	\
 	 ((secno) == CURSEG_I(sbi, CURSEG_ALL_DATA_ATGC)->segno /	\
-	  (sbi)->segs_per_sec))
+	  (sbi)->segs_per_sec) ||	\
+	 IS_CURSEC_SEQZONE(sbi, secno))
+
 
 #define MAIN_BLKADDR(sbi)						\
 	(SM_I(sbi) ? SM_I(sbi)->main_blkaddr : 				\
@@ -220,6 +283,9 @@ struct sec_entry {
 struct revoke_entry {
 	struct list_head list;
 	block_t old_addr;		/* for revoking when fail to commit */
+#ifdef CONFIG_F2FS_SEQZONE
+	pgoff_t old_seqzone_index;	/* for revoking when fail to commit */
+#endif
 	pgoff_t index;
 };
 
@@ -608,7 +674,8 @@ static inline void __get_secs_required(struct f2fs_sb_info *sbi,
 	unsigned int dent_blocks = total_dent_blocks % CAP_BLKS_PER_SEC(sbi);
 	unsigned int data_blocks = 0;
 
-	if (f2fs_lfs_mode(sbi)) {
+	if (f2fs_lfs_mode(sbi) &&
+		unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {
 		total_data_blocks = get_pages(sbi, F2FS_DIRTY_DATA);
 		data_secs = total_data_blocks / CAP_BLKS_PER_SEC(sbi);
 		data_blocks = total_data_blocks % CAP_BLKS_PER_SEC(sbi);
@@ -617,7 +684,7 @@ static inline void __get_secs_required(struct f2fs_sb_info *sbi,
 	if (lower_p)
 		*lower_p = node_secs + dent_secs + data_secs;
 	if (upper_p)
-		*upper_p = node_secs + dent_secs + data_secs +
+		*upper_p = node_secs + dent_secs +
 			(node_blocks ? 1 : 0) + (dent_blocks ? 1 : 0) +
 			(data_blocks ? 1 : 0);
 	if (curseg_p)
diff --git a/fs/f2fs/super.c b/fs/f2fs/super.c
index c76808055..d9e90a821 100644
--- a/fs/f2fs/super.c
+++ b/fs/f2fs/super.c
@@ -42,32 +42,56 @@
 
 static struct kmem_cache *f2fs_inode_cachep;
 
+#ifdef CONFIG_F2FS_SEQZONE
+#include <linux/of.h>
+const char *seqzone_switch = NULL;
+#endif
+
 #ifdef CONFIG_F2FS_FAULT_INJECTION
 
 const char *f2fs_fault_name[FAULT_MAX] = {
-	[FAULT_KMALLOC]			= "kmalloc",
-	[FAULT_KVMALLOC]		= "kvmalloc",
-	[FAULT_PAGE_ALLOC]		= "page alloc",
-	[FAULT_PAGE_GET]		= "page get",
-	[FAULT_ALLOC_NID]		= "alloc nid",
-	[FAULT_ORPHAN]			= "orphan",
-	[FAULT_BLOCK]			= "no more block",
-	[FAULT_DIR_DEPTH]		= "too big dir depth",
-	[FAULT_EVICT_INODE]		= "evict_inode fail",
-	[FAULT_TRUNCATE]		= "truncate fail",
-	[FAULT_READ_IO]			= "read IO error",
-	[FAULT_CHECKPOINT]		= "checkpoint error",
-	[FAULT_DISCARD]			= "discard error",
-	[FAULT_WRITE_IO]		= "write IO error",
-	[FAULT_SLAB_ALLOC]		= "slab alloc",
-	[FAULT_DQUOT_INIT]		= "dquot initialize",
-	[FAULT_LOCK_OP]			= "lock_op",
-	[FAULT_BLKADDR_VALIDITY]	= "invalid blkaddr",
-	[FAULT_BLKADDR_CONSISTENCE]	= "inconsistent blkaddr",
+	[FAULT_KMALLOC]		= "kmalloc",
+	[FAULT_KVMALLOC]	= "kvmalloc",
+	[FAULT_PAGE_ALLOC]	= "page alloc",
+	[FAULT_PAGE_GET]	= "page get",
+	[FAULT_ALLOC_NID]	= "alloc nid",
+	[FAULT_ORPHAN]		= "orphan",
+	[FAULT_BLOCK]		= "no more block",
+	[FAULT_DIR_DEPTH]	= "too big dir depth",
+	[FAULT_EVICT_INODE]	= "evict_inode fail",
+	[FAULT_TRUNCATE]	= "truncate fail",
+	[FAULT_READ_IO]		= "read IO error",
+	[FAULT_CHECKPOINT]	= "checkpoint error",
+	[FAULT_DISCARD]		= "discard error",
+	[FAULT_WRITE_IO]	= "write IO error",
+	[FAULT_SLAB_ALLOC]	= "slab alloc",
+	[FAULT_DQUOT_INIT]	= "dquot initialize",
+	[FAULT_LOCK_OP]		= "lock_op",
+	[FAULT_BLKADDR]		= "invalid blkaddr",
+#ifdef CONFIG_F2FS_APPBOOST
+	[FAULT_READ_ERROR]    = "appboost read error",
+	[FAULT_WRITE_ERROR]   = "appboost write error",
+	[FAULT_PAGE_ERROR]      = "appboost page error",
+	[FAULT_FSYNC_ERROR]     = "appboost fsync error",
+	[FAULT_FLUSH_ERROR]     = "appboost flush error",
+	[FAULT_WRITE_TAIL_ERROR]= "appboost write tail error",
+#endif
+	[FAULT_COMPRESS_REDIRTY] = "compress ioc redirty",
+	[FAULT_COMPRESS_WRITEBACK] = "compress ioc writeback",
+	[FAULT_COMPRESS_RESERVE_NOSPC] = "compress reserve nospc",
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	[FAULT_COMPRESS_VMAP] = "compress vmap",
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	[FAULT_COMPRESS_INIT_CTX] = "compress init ctx",
+	[FAULT_COMPRESS_PAGE_ARRAY] = "compress page array",
+	[FAULT_COMPRESS_LOW_RATIO] = "compress low ratio",
+	[FAULT_COMPRESS_GET_DNODE] = "compress get dnode",
+#endif
+#endif
 };
 
 int f2fs_build_fault_attr(struct f2fs_sb_info *sbi, unsigned long rate,
-							unsigned long type)
+							unsigned long long type)
 {
 	struct f2fs_fault_info *ffi = &F2FS_OPTION(sbi).fault_info;
 
@@ -170,13 +194,13 @@ enum {
 	Opt_compress_chksum,
 	Opt_compress_mode,
 	Opt_compress_cache,
+	Opt_compress_layout,
 	Opt_atgc,
 	Opt_gc_merge,
 	Opt_nogc_merge,
 	Opt_discard_unit,
 	Opt_memory_mode,
 	Opt_age_extent_cache,
-	Opt_lookup_mode,
 	Opt_err,
 };
 
@@ -250,13 +274,13 @@ static match_table_t f2fs_tokens = {
 	{Opt_compress_chksum, "compress_chksum"},
 	{Opt_compress_mode, "compress_mode=%s"},
 	{Opt_compress_cache, "compress_cache"},
+	{Opt_compress_layout, "compress_layout=%s"},
 	{Opt_atgc, "atgc"},
 	{Opt_gc_merge, "gc_merge"},
 	{Opt_nogc_merge, "nogc_merge"},
 	{Opt_discard_unit, "discard_unit=%s"},
 	{Opt_memory_mode, "memory=%s"},
 	{Opt_age_extent_cache, "age_extent_cache"},
-	{Opt_lookup_mode, "lookup_mode=%s"},
 	{Opt_err, NULL},
 };
 
@@ -624,12 +648,14 @@ static int f2fs_set_lz4hc_level(struct f2fs_sb_info *sbi, const char *str)
 {
 #ifdef CONFIG_F2FS_FS_LZ4HC
 	unsigned int level;
+#endif
 
 	if (strlen(str) == 3) {
 		F2FS_OPTION(sbi).compress_level = 0;
 		return 0;
 	}
 
+#ifdef CONFIG_F2FS_FS_LZ4HC
 	str += 3;
 
 	if (str[0] != ':') {
@@ -647,10 +673,6 @@ static int f2fs_set_lz4hc_level(struct f2fs_sb_info *sbi, const char *str)
 	F2FS_OPTION(sbi).compress_level = level;
 	return 0;
 #else
-	if (strlen(str) == 3) {
-		F2FS_OPTION(sbi).compress_level = 0;
-		return 0;
-	}
 	f2fs_info(sbi, "kernel doesn't support lz4hc compression");
 	return -EINVAL;
 #endif
@@ -664,7 +686,7 @@ static int f2fs_set_zstd_level(struct f2fs_sb_info *sbi, const char *str)
 	int len = 4;
 
 	if (strlen(str) == len) {
-		F2FS_OPTION(sbi).compress_level = F2FS_ZSTD_DEFAULT_CLEVEL;
+		F2FS_OPTION(sbi).compress_level = 0;
 		return 0;
 	}
 
@@ -692,6 +714,7 @@ static int f2fs_set_zstd_level(struct f2fs_sb_info *sbi, const char *str)
 	return 0;
 }
 #endif
+
 #endif
 
 static int parse_options(struct super_block *sb, char *options, bool is_remount)
@@ -705,6 +728,9 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 #endif
 	char *p, *name;
 	int arg = 0;
+#ifdef CONFIG_F2FS_FAULT_INJECTION
+	unsigned long long larg;
+#endif
 	kuid_t uid;
 	kgid_t gid;
 	int ret;
@@ -935,9 +961,9 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			break;
 
 		case Opt_fault_type:
-			if (args->from && match_int(args, &arg))
+			if (args->from && match_u64(args, &larg))
 				return -EINVAL;
-			if (f2fs_build_fault_attr(sbi, 0, arg))
+			if (f2fs_build_fault_attr(sbi, 0, larg))
 				return -EINVAL;
 			set_opt(sbi, FAULT_INJECTION);
 			break;
@@ -1036,9 +1062,9 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 				return -ENOMEM;
 
 			if (!strcmp(name, "default")) {
-				f2fs_set_alloc_mode(sbi, ALLOC_MODE_DEFAULT);
+				F2FS_OPTION(sbi).alloc_mode = ALLOC_MODE_DEFAULT;
 			} else if (!strcmp(name, "reuse")) {
-				f2fs_set_alloc_mode(sbi, ALLOC_MODE_REUSE);
+				F2FS_OPTION(sbi).alloc_mode = ALLOC_MODE_REUSE;
 			} else {
 				kfree(name);
 				return -EINVAL;
@@ -1154,6 +1180,15 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 				kfree(name);
 				return -EINVAL;
 			}
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+			if (F2FS_OPTION(sbi).compress_layout == COMPRESS_FIXED_OUTPUT &&
+			    (F2FS_OPTION(sbi).compress_algorithm != COMPRESS_LZ4 ||
+			     F2FS_OPTION(sbi).compress_level != 0)) {
+				f2fs_err(sbi, "fixed-output compress layout can only work on lz4");
+				kfree(name);
+				return -EINVAL;
+			}
+#endif
 			kfree(name);
 			break;
 		case Opt_compress_log_size:
@@ -1183,14 +1218,6 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			ext = F2FS_OPTION(sbi).extensions;
 			ext_cnt = F2FS_OPTION(sbi).compress_ext_cnt;
 
-			if (strlen(name) >= F2FS_EXTENSION_LEN ||
-				ext_cnt >= COMPRESS_EXT_NUM) {
-				f2fs_err(sbi,
-					"invalid extension length/number");
-				kfree(name);
-				return -EINVAL;
-			}
-
 			if (is_compress_extension_exist(sbi, name, true)) {
 				kfree(name);
 				break;
@@ -1212,14 +1239,6 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			noext = F2FS_OPTION(sbi).noextensions;
 			noext_cnt = F2FS_OPTION(sbi).nocompress_ext_cnt;
 
-			if (strlen(name) >= F2FS_EXTENSION_LEN ||
-				noext_cnt >= COMPRESS_EXT_NUM) {
-				f2fs_err(sbi,
-					"invalid extension length/number");
-				kfree(name);
-				return -EINVAL;
-			}
-
 			if (is_compress_extension_exist(sbi, name, false)) {
 				kfree(name);
 				break;
@@ -1261,6 +1280,32 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			}
 			set_opt(sbi, COMPRESS_CACHE);
 			break;
+		case Opt_compress_layout:
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+			if (!f2fs_sb_has_compression(sbi)) {
+				f2fs_info(sbi, "Image doesn't support compression");
+				break;
+			}
+			name = match_strdup(&args[0]);
+			/* fix coverity error: Dereference null return value name*/
+			if (name && !strcmp(name, "fixed-input")) {
+				F2FS_OPTION(sbi).compress_layout = COMPRESS_FIXED_INPUT;
+			} else if (name && !strcmp(name, "fixed-output")) {
+				if (F2FS_OPTION(sbi).compress_algorithm != COMPRESS_LZ4 ||
+				    F2FS_OPTION(sbi).compress_level != 0) {
+					f2fs_err(sbi, "fixed-output compress layout can only work on lz4");
+					kfree(name);
+					return -EINVAL;
+				}
+				F2FS_OPTION(sbi).compress_layout = COMPRESS_FIXED_OUTPUT;
+			} else {
+				f2fs_err(sbi, "Unknown compress layout");
+				kfree(name);
+				return -EINVAL;
+			}
+			kfree(name);
+#endif
+			break;
 #else
 		case Opt_compress_algorithm:
 		case Opt_compress_log_size:
@@ -1269,6 +1314,7 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 		case Opt_compress_chksum:
 		case Opt_compress_mode:
 		case Opt_compress_cache:
+		case Opt_compress_layout:
 			f2fs_info(sbi, "compression options not supported");
 			break;
 #endif
@@ -1319,22 +1365,6 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 		case Opt_age_extent_cache:
 			set_opt(sbi, AGE_EXTENT_CACHE);
 			break;
-		case Opt_lookup_mode:
-			name = match_strdup(&args[0]);
-			if (!name)
-				return -ENOMEM;
-			if (!strcmp(name, "perf")) {
-				f2fs_set_lookup_mode(sbi, LOOKUP_PERF);
-			} else if (!strcmp(name, "compat")) {
-				f2fs_set_lookup_mode(sbi, LOOKUP_COMPAT);
-			} else if (!strcmp(name, "auto")) {
-				f2fs_set_lookup_mode(sbi, LOOKUP_AUTO);
-			} else {
-				kfree(name);
-				return -EINVAL;
-			}
-			kfree(name);
-			break;
 		default:
 			f2fs_err(sbi, "Unrecognized mount option \"%s\" or missing value",
 				 p);
@@ -1412,7 +1442,7 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			return -EINVAL;
 		}
 
-		min_size = sizeof(struct f2fs_xattr_header) / sizeof(__le32);
+		min_size = MIN_INLINE_XATTR_SIZE;
 		max_size = MAX_INLINE_XATTR_SIZE;
 
 		if (F2FS_OPTION(sbi).inline_xattr_size < min_size ||
@@ -1468,10 +1498,16 @@ static struct inode *f2fs_alloc_inode(struct super_block *sb)
 	init_f2fs_rwsem(&fi->i_gc_rwsem[READ]);
 	init_f2fs_rwsem(&fi->i_gc_rwsem[WRITE]);
 	init_f2fs_rwsem(&fi->i_xattr_sem);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	fi->dio_cur_seqindex = 1;
+#endif
 	/* Will be used by directory only */
 	fi->i_dir_level = F2FS_SB(sb)->dir_level;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	atomic_set(&fi->inflight_read_io, 0);
+	init_waitqueue_head(&fi->dedup_wq);
+#endif
 	return &fi->vfs_inode;
 }
 
@@ -1599,6 +1635,9 @@ static void f2fs_dirty_inode(struct inode *inode, int flags)
 static void f2fs_free_inode(struct inode *inode)
 {
 	fscrypt_free_inode(inode);
+#ifdef CONFIG_F2FS_APPBOOST
+	f2fs_boostfile_free(inode);
+#endif
 	kmem_cache_free(f2fs_inode_cachep, F2FS_I(inode));
 }
 
@@ -1791,32 +1830,26 @@ static int f2fs_statfs_project(struct super_block *sb,
 
 	limit = min_not_zero(dquot->dq_dqb.dqb_bsoftlimit,
 					dquot->dq_dqb.dqb_bhardlimit);
-	limit >>= sb->s_blocksize_bits;
-
-	if (limit) {
-		uint64_t remaining = 0;
+	if (limit)
+		limit >>= sb->s_blocksize_bits;
 
+	if (limit && buf->f_blocks > limit) {
 		curblock = (dquot->dq_dqb.dqb_curspace +
 			    dquot->dq_dqb.dqb_rsvspace) >> sb->s_blocksize_bits;
-		if (limit > curblock)
-			remaining = limit - curblock;
-
-		buf->f_blocks = min(buf->f_blocks, limit);
-		buf->f_bfree = min(buf->f_bfree, remaining);
-		buf->f_bavail = min(buf->f_bavail, remaining);
+		buf->f_blocks = limit;
+		buf->f_bfree = buf->f_bavail =
+			(buf->f_blocks > curblock) ?
+			 (buf->f_blocks - curblock) : 0;
 	}
 
 	limit = min_not_zero(dquot->dq_dqb.dqb_isoftlimit,
 					dquot->dq_dqb.dqb_ihardlimit);
 
-	if (limit) {
-		uint64_t remaining = 0;
-
-		if (limit > dquot->dq_dqb.dqb_curinodes)
-			remaining = limit - dquot->dq_dqb.dqb_curinodes;
-
-		buf->f_files = min(buf->f_files, limit);
-		buf->f_ffree = min(buf->f_ffree, remaining);
+	if (limit && buf->f_files > limit) {
+		buf->f_files = limit;
+		buf->f_ffree =
+			(buf->f_files > dquot->dq_dqb.dqb_curinodes) ?
+			 (buf->f_files - dquot->dq_dqb.dqb_curinodes) : 0;
 	}
 
 	spin_unlock(&dquot->dq_dqb_lock);
@@ -1874,9 +1907,9 @@ static int f2fs_statfs(struct dentry *dentry, struct kstatfs *buf)
 	buf->f_fsid    = u64_to_fsid(id);
 
 #ifdef CONFIG_QUOTA
-	if (is_inode_flag_set(d_inode(dentry), FI_PROJ_INHERIT) &&
+	if (is_inode_flag_set(dentry->d_inode, FI_PROJ_INHERIT) &&
 			sb_has_quota_limits_enabled(sb, PRJQUOTA)) {
-		f2fs_statfs_project(sb, F2FS_I(d_inode(dentry))->i_projid, buf);
+		f2fs_statfs_project(sb, F2FS_I(dentry->d_inode)->i_projid, buf);
 	}
 #endif
 	return 0;
@@ -1925,7 +1958,9 @@ static inline void f2fs_show_compress_options(struct seq_file *seq,
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(sb);
 	char *algtype = "";
+#ifndef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
 	int i;
+#endif
 
 	if (!f2fs_sb_has_compression(sbi))
 		return;
@@ -1952,6 +1987,7 @@ static inline void f2fs_show_compress_options(struct seq_file *seq,
 	seq_printf(seq, ",compress_log_size=%u",
 			F2FS_OPTION(sbi).compress_log_size);
 
+#ifndef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
 	for (i = 0; i < F2FS_OPTION(sbi).compress_ext_cnt; i++) {
 		seq_printf(seq, ",compress_extension=%s",
 			F2FS_OPTION(sbi).extensions[i]);
@@ -1961,6 +1997,7 @@ static inline void f2fs_show_compress_options(struct seq_file *seq,
 		seq_printf(seq, ",nocompress_extension=%s",
 			F2FS_OPTION(sbi).noextensions[i]);
 	}
+#endif
 
 	if (F2FS_OPTION(sbi).compress_chksum)
 		seq_puts(seq, ",compress_chksum");
@@ -2078,7 +2115,7 @@ static int f2fs_show_options(struct seq_file *seq, struct dentry *root)
 	if (test_opt(sbi, FAULT_INJECTION)) {
 		seq_printf(seq, ",fault_injection=%u",
 				F2FS_OPTION(sbi).fault_info.inject_rate);
-		seq_printf(seq, ",fault_type=%u",
+		seq_printf(seq, ",fault_type=%llu",
 				F2FS_OPTION(sbi).fault_info.inject_type);
 	}
 #endif
@@ -2099,9 +2136,9 @@ static int f2fs_show_options(struct seq_file *seq, struct dentry *root)
 	if (sbi->sb->s_flags & SB_INLINECRYPT)
 		seq_puts(seq, ",inlinecrypt");
 
-	if (f2fs_get_alloc_mode(sbi) == ALLOC_MODE_DEFAULT)
+	if (F2FS_OPTION(sbi).alloc_mode == ALLOC_MODE_DEFAULT)
 		seq_printf(seq, ",alloc_mode=%s", "default");
-	else if (f2fs_get_alloc_mode(sbi) == ALLOC_MODE_REUSE)
+	else if (F2FS_OPTION(sbi).alloc_mode == ALLOC_MODE_REUSE)
 		seq_printf(seq, ",alloc_mode=%s", "reuse");
 
 	if (test_opt(sbi, DISABLE_CHECKPOINT))
@@ -2130,18 +2167,29 @@ static int f2fs_show_options(struct seq_file *seq, struct dentry *root)
 	else if (F2FS_OPTION(sbi).memory_mode == MEMORY_MODE_LOW)
 		seq_printf(seq, ",memory=%s", "low");
 
-	if (f2fs_get_lookup_mode(sbi) == LOOKUP_PERF)
-		seq_show_option(seq, "lookup_mode", "perf");
-	else if (f2fs_get_lookup_mode(sbi) == LOOKUP_COMPAT)
-		seq_show_option(seq, "lookup_mode", "compat");
-	else if (f2fs_get_lookup_mode(sbi) == LOOKUP_AUTO)
-		seq_show_option(seq, "lookup_mode", "auto");
-
 	return 0;
 }
 
+static bool is_data_partition(struct f2fs_sb_info *sbi)
+{
+	uint64_t min_data_partition_blocks = 0x800000; // 32GB
+
+	if (le64_to_cpu(sbi->raw_super->block_count) > min_data_partition_blocks)
+		return true;
+	return false;
+}
+
 static void default_options(struct f2fs_sb_info *sbi)
 {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	int i = 0;
+#endif
+
+	if (is_data_partition(sbi)) {
+		sbi->oplus_feats = OPLUS_FEAT_COMPR;
+		sbi->oplus_feats |= OPLUS_FEAT_DEDUP;
+	}
+
 	/* init some FS parameters */
 	if (f2fs_sb_has_readonly(sbi))
 		F2FS_OPTION(sbi).active_logs = NR_CURSEG_RO_TYPE;
@@ -2151,9 +2199,9 @@ static void default_options(struct f2fs_sb_info *sbi)
 	F2FS_OPTION(sbi).inline_xattr_size = DEFAULT_INLINE_XATTR_ADDRS;
 	if (le32_to_cpu(F2FS_RAW_SUPER(sbi)->segment_count_main) <=
 							SMALL_VOLUME_SEGMENTS)
-		f2fs_set_alloc_mode(sbi, ALLOC_MODE_REUSE);
+		F2FS_OPTION(sbi).alloc_mode = ALLOC_MODE_REUSE;
 	else
-		f2fs_set_alloc_mode(sbi, ALLOC_MODE_DEFAULT);
+		F2FS_OPTION(sbi).alloc_mode = ALLOC_MODE_DEFAULT;
 	F2FS_OPTION(sbi).fsync_mode = FSYNC_MODE_POSIX;
 	F2FS_OPTION(sbi).s_resuid = make_kuid(&init_user_ns, F2FS_DEF_RESUID);
 	F2FS_OPTION(sbi).s_resgid = make_kgid(&init_user_ns, F2FS_DEF_RESGID);
@@ -2161,7 +2209,20 @@ static void default_options(struct f2fs_sb_info *sbi)
 		F2FS_OPTION(sbi).compress_algorithm = COMPRESS_LZ4;
 		F2FS_OPTION(sbi).compress_log_size = MIN_COMPRESS_LOG_SIZE;
 		F2FS_OPTION(sbi).compress_ext_cnt = 0;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		F2FS_OPTION(sbi).compress_mode = COMPR_MODE_USER;
+		F2FS_OPTION(sbi).compress_layout = COMPRESS_FIXED_OUTPUT;
+		strcpy(F2FS_OPTION(sbi).extensions[i++], "odex");
+		strcpy(F2FS_OPTION(sbi).extensions[i++], "vdex");
+		//strcpy(F2FS_OPTION(sbi).extensions[i++], "so");
+		//strcpy(F2FS_OPTION(sbi).extensions[i++], "dex");
+		//strcpy(F2FS_OPTION(sbi).extensions[i++], "wxapkg");
+		//strcpy(F2FS_OPTION(sbi).extensions[i++], "js");
+		F2FS_OPTION(sbi).compress_ext_cnt = (unsigned char)i;
+#else
 		F2FS_OPTION(sbi).compress_mode = COMPR_MODE_FS;
+#endif
+
 	}
 	F2FS_OPTION(sbi).bggc_mode = BGGC_MODE_ON;
 	F2FS_OPTION(sbi).memory_mode = MEMORY_MODE_NORMAL;
@@ -2194,8 +2255,6 @@ static void default_options(struct f2fs_sb_info *sbi)
 #endif
 
 	f2fs_build_fault_attr(sbi, 0, 0);
-
-	f2fs_set_lookup_mode(sbi, LOOKUP_PERF);
 }
 
 #ifdef CONFIG_QUOTA
@@ -3627,7 +3686,6 @@ int f2fs_sanity_check_ckpt(struct f2fs_sb_info *sbi)
 	block_t user_block_count, valid_user_blocks;
 	block_t avail_node_count, valid_node_count;
 	unsigned int nat_blocks, nat_bits_bytes, nat_bits_blocks;
-	unsigned int sit_blk_cnt;
 	int i, j;
 
 	total = le32_to_cpu(raw_super->segment_count);
@@ -3739,13 +3797,6 @@ int f2fs_sanity_check_ckpt(struct f2fs_sb_info *sbi)
 		return 1;
 	}
 
-	sit_blk_cnt = DIV_ROUND_UP(main_segs, SIT_ENTRY_PER_BLOCK);
-	if (sit_bitmap_size * 8 < sit_blk_cnt) {
-		f2fs_err(sbi, "Wrong bitmap size: sit: %u, sit_blk_cnt:%u",
-			 sit_bitmap_size, sit_blk_cnt);
-		return 1;
-	}
-
 	cp_pack_start_sum = __start_sum_addr(sbi);
 	cp_payload = __cp_payload(sbi);
 	if (cp_pack_start_sum < cp_payload + 1 ||
@@ -3841,6 +3892,9 @@ static void init_sb_info(struct f2fs_sb_info *sbi)
 
 	init_f2fs_rwsem(&sbi->sb_lock);
 	init_f2fs_rwsem(&sbi->pin_sem);
+#ifdef CONFIG_F2FS_SEQZONE
+	sbi->seq_zone = 0;
+#endif
 }
 
 static int init_percpu_info(struct f2fs_sb_info *sbi)
@@ -4369,7 +4423,11 @@ static int f2fs_fill_super(struct super_block *sb, void *data, int silent)
 		}
 	}
 #endif
-
+#ifdef CONFIG_F2FS_APPBOOST
+	sbi->appboost = 0;
+#define APPBOOST_MAX_BLOCKS 51200
+	sbi->appboost_max_blocks = APPBOOST_MAX_BLOCKS;
+#endif
 	sb->s_op = &f2fs_sops;
 #ifdef CONFIG_FS_ENCRYPTION
 	sb->s_cop = &f2fs_cryptops;
@@ -4423,6 +4481,7 @@ static int f2fs_fill_super(struct super_block *sb, void *data, int silent)
 	if (err)
 		goto free_xattr_cache;
 
+
 	/* get an inode for meta space */
 	sbi->meta_inode = f2fs_iget(sb, F2FS_META_INO(sbi));
 	if (IS_ERR(sbi->meta_inode)) {
@@ -4843,6 +4902,17 @@ static void destroy_inodecache(void)
 	kmem_cache_destroy(f2fs_inode_cachep);
 }
 
+#ifdef CONFIG_F2FS_SEQZONE
+#define SEQZONE_CONFIG_PATH "/soc/oplus,f2fs/seqzone"
+static void update_seqzone_switch(void)
+{
+	struct device_node *np = of_find_node_by_path(SEQZONE_CONFIG_PATH);
+	if (np) {
+		of_property_read_string(np, "switch", &seqzone_switch);
+	}
+}
+#endif
+
 static int __init init_f2fs_fs(void)
 {
 	int err;
@@ -4853,6 +4923,10 @@ static int __init init_f2fs_fs(void)
 		return -EINVAL;
 	}
 
+#ifdef CONFIG_F2FS_SEQZONE
+	update_seqzone_switch();
+#endif
+
 	err = init_inodecache();
 	if (err)
 		goto fail;
diff --git a/fs/f2fs/sysfs.c b/fs/f2fs/sysfs.c
index dab418a50..3436451d5 100644
--- a/fs/f2fs/sysfs.c
+++ b/fs/f2fs/sysfs.c
@@ -39,6 +39,9 @@ enum {
 	RESERVED_BLOCKS,	/* struct f2fs_sb_info */
 	CPRC_INFO,	/* struct ckpt_req_control */
 	ATGC_INFO,	/* struct atgc_management */
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	COMP_EXT,
+#endif
 };
 
 static const char *gc_mode_names[MAX_GC_MODE] = {
@@ -89,6 +92,10 @@ static unsigned char *__struct_ptr(struct f2fs_sb_info *sbi, int struct_type)
 		return (unsigned char *)&sbi->cprc_info;
 	else if (struct_type == ATGC_INFO)
 		return (unsigned char *)&sbi->am;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	else if (struct_type == COMP_EXT)
+		return (unsigned char *)&sbi->mount_opt;
+#endif
 	return NULL;
 }
 
@@ -196,6 +203,17 @@ static ssize_t features_show(struct f2fs_attr *a,
 	if (f2fs_sb_has_compression(sbi))
 		len += scnprintf(buf + len, PAGE_SIZE - len, "%s%s",
 				len ? ", " : "", "compression");
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_sb_has_dedup(sbi))
+		len += snprintf(buf + len, PAGE_SIZE - len, "%s%s",
+				len ? ", " : "", "dedup");
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi))
+		len += snprintf(buf + len, PAGE_SIZE - len, "%s%s",
+				len ? ", " : "", "seqzone");
+#endif
 	len += scnprintf(buf + len, PAGE_SIZE - len, "%s%s",
 				len ? ", " : "", "pin_file");
 	len += scnprintf(buf + len, PAGE_SIZE - len, "\n");
@@ -235,29 +253,6 @@ static ssize_t encoding_show(struct f2fs_attr *a,
 	return sysfs_emit(buf, "(none)\n");
 }
 
-static ssize_t encoding_flags_show(struct f2fs_attr *a,
-		struct f2fs_sb_info *sbi, char *buf)
-{
-	return sysfs_emit(buf, "%x\n",
-		le16_to_cpu(F2FS_RAW_SUPER(sbi)->s_encoding_flags));
-}
-
-static ssize_t effective_lookup_mode_show(struct f2fs_attr *a,
-		struct f2fs_sb_info *sbi, char *buf)
-{
-	switch (f2fs_get_lookup_mode(sbi)) {
-	case LOOKUP_PERF:
-		return sysfs_emit(buf, "perf\n");
-	case LOOKUP_COMPAT:
-		return sysfs_emit(buf, "compat\n");
-	case LOOKUP_AUTO:
-		if (sb_no_casefold_compat_fallback(sbi->sb))
-			return sysfs_emit(buf, "auto:perf\n");
-		return sysfs_emit(buf, "auto:compat\n");
-	}
-	return 0;
-}
-
 static ssize_t mounted_time_sec_show(struct f2fs_attr *a,
 		struct f2fs_sb_info *sbi, char *buf)
 {
@@ -354,6 +349,51 @@ static ssize_t f2fs_sbi_show(struct f2fs_attr *a,
 
 	if (!strcmp(a->attr.name, "compr_new_inode"))
 		return sysfs_emit(buf, "%u\n", sbi->compr_new_inode);
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (!strcmp(a->attr.name, "compress_layout")) {
+		if (F2FS_OPTION(sbi).compress_layout == COMPRESS_FIXED_OUTPUT)
+			return sysfs_emit(buf, "fixed-output");
+		return sysfs_emit(buf, "fixed-input");
+	}
+
+	if (!strcmp(a->attr.name, "compress_extension")) {
+		int len = 0, i;
+
+		f2fs_down_read(&sbi->sb_lock);
+		for (i = 0; i < F2FS_OPTION(sbi).compress_ext_cnt; i++) {
+			if (!strlen(F2FS_OPTION(sbi).extensions[i]))
+				continue;
+			len += scnprintf(buf + len, PAGE_SIZE - len, "%s\n",
+					 F2FS_OPTION(sbi).extensions[i]);
+		}
+		f2fs_up_read(&sbi->sb_lock);
+		return len;
+	}
+
+	if (!strcmp(a->attr.name, "compress_noextension")) {
+		int len = 0, i;
+
+		f2fs_down_read(&sbi->sb_lock);
+		for (i = 0; i < F2FS_OPTION(sbi).nocompress_ext_cnt; i++) {
+			if (!strlen(F2FS_OPTION(sbi).noextensions[i]))
+				continue;
+			len += scnprintf(buf + len, PAGE_SIZE - len, "%s\n",
+					 F2FS_OPTION(sbi).noextensions[i]);
+		}
+		f2fs_up_read(&sbi->sb_lock);
+		return len;
+	}
+
+	if (!strcmp(a->attr.name, "compress_log_size")) {
+		int log_size;
+
+		f2fs_down_read(&sbi->sb_lock);
+		log_size = F2FS_OPTION(sbi).compress_log_size;
+		f2fs_up_read(&sbi->sb_lock);
+		return sysfs_emit(buf, "%d\n", log_size);
+	}
+#endif
 #endif
 
 	if (!strcmp(a->attr.name, "gc_segment_mode"))
@@ -379,6 +419,16 @@ static ssize_t f2fs_sbi_show(struct f2fs_attr *a,
 	if (!strcmp(a->attr.name, "revoked_atomic_block"))
 		return sysfs_emit(buf, "%llu\n", sbi->revoked_atomic_block);
 
+#ifdef CONFIG_F2FS_APPBOOST
+	if (!strcmp(a->attr.name, "appboost")) {
+		return sysfs_emit(buf, "%u\n",
+			sbi->appboost);
+	}
+	if (!strcmp(a->attr.name, "appboost_max_blocks")) {
+		return sysfs_emit(buf, "%u\n",
+			sbi->appboost_max_blocks);
+	}
+#endif
 	ui = (unsigned int *)(ptr + a->offset);
 
 	return sysfs_emit(buf, "%u\n", *ui);
@@ -389,7 +439,7 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 			const char *buf, size_t count)
 {
 	unsigned char *ptr;
-	unsigned long t;
+	unsigned long long t;
 	unsigned int *ui;
 	ssize_t ret;
 
@@ -432,6 +482,83 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 		return ret ? ret : count;
 	}
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (!strcmp(a->attr.name, "compress_extension")) {
+		char *token = NULL, *name = strim((char *)buf);
+		unsigned char (*ext)[F2FS_EXTENSION_LEN];
+		unsigned char extensions[COMPRESS_EXT_NUM][F2FS_EXTENSION_LEN];
+		unsigned char ext_cnt = 0;
+		ssize_t sz;
+
+		memset(extensions, 0, sizeof(extensions));
+		while (1) {
+			token = strsep(&name, ",");
+			if (!token)
+				break;
+			sz = strlen(token);
+			if (sz == 0)
+				continue;
+			if (sz >= F2FS_EXTENSION_LEN)
+				return -EINVAL;
+			if (ext_cnt >= COMPRESS_EXT_NUM)
+				return -EINVAL;
+			if (!strcmp(token, "disable")) {
+				memset(extensions, 0, sizeof(extensions));
+				ext_cnt = 0;
+				break;
+			}
+			memcpy(extensions[ext_cnt++], token, sz);
+		}
+
+		ext = F2FS_OPTION(sbi).extensions;
+		f2fs_down_write(&sbi->sb_lock);
+		memset(ext, 0, sizeof(F2FS_OPTION(sbi).extensions));
+		memcpy(ext, extensions, ext_cnt * F2FS_EXTENSION_LEN);
+		F2FS_OPTION(sbi).compress_ext_cnt = ext_cnt;
+		/*
+		 * no need to persist the new extensions since RUS could
+		 * update it manually
+		 */
+		f2fs_up_write(&sbi->sb_lock);
+		return count;
+	}
+
+	if (!strcmp(a->attr.name, "compress_noextension")) {
+		char *token = NULL, *name = strim((char *)buf);
+		unsigned char (*noext)[F2FS_EXTENSION_LEN];
+		unsigned char noextensions[COMPRESS_EXT_NUM][F2FS_EXTENSION_LEN];
+		unsigned char noext_cnt = 0;
+		ssize_t sz;
+
+		memset(noextensions, 0, sizeof(noextensions));
+		while (1) {
+			token = strsep(&name, ",");
+			if (!token)
+				break;
+			sz = strlen(token);
+			if (sz == 0)
+				continue;
+			if (sz >= F2FS_EXTENSION_LEN)
+				return -EINVAL;
+			if (noext_cnt >= COMPRESS_EXT_NUM)
+				return -EINVAL;
+			memcpy(noextensions[noext_cnt++], token, sz);
+		}
+
+		noext = F2FS_OPTION(sbi).noextensions;
+		f2fs_down_write(&sbi->sb_lock);
+		memset(noext, 0, sizeof(F2FS_OPTION(sbi).noextensions));
+		memcpy(noext, noextensions, noext_cnt * F2FS_EXTENSION_LEN);
+		F2FS_OPTION(sbi).nocompress_ext_cnt = noext_cnt;
+		/*
+		 * no need to persist the new noextensions since RUS could
+		 * update it manually
+		 */
+		f2fs_up_write(&sbi->sb_lock);
+		return count;
+	}
+#endif
+
 	if (!strcmp(a->attr.name, "ckpt_thread_ioprio")) {
 		const char *name = strim((char *)buf);
 		struct ckpt_req_control *cprc = &sbi->cprc_info;
@@ -466,7 +593,7 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 
 	ui = (unsigned int *)(ptr + a->offset);
 
-	ret = kstrtoul(skip_spaces(buf), 0, &t);
+	ret = kstrtoull(skip_spaces(buf), 0, &t);
 	if (ret < 0)
 		return ret;
 #ifdef CONFIG_F2FS_FAULT_INJECTION
@@ -483,7 +610,7 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 #endif
 	if (a->struct_type == RESERVED_BLOCKS) {
 		spin_lock(&sbi->stat_lock);
-		if (t > (unsigned long)(sbi->user_block_count -
+		if (t > (unsigned long long)(sbi->user_block_count -
 				F2FS_OPTION(sbi).root_reserved_blocks -
 				sbi->blocks_per_seg *
 				SM_I(sbi)->additional_reserved_segments)) {
@@ -519,6 +646,13 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 		return count;
 	}
 
+	if (!strcmp(a->attr.name, "discard_io_aware")) {
+		if (t >= DPOLICY_IO_AWARE_MAX)
+			return -EINVAL;
+		*ui = t;
+		return count;
+	}
+
 	if (!strcmp(a->attr.name, "max_ordered_discard")) {
 		if (t == 0 || t > MAX_PLIST_NUM)
 			return -EINVAL;
@@ -636,6 +770,17 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 		*ui = t;
 		return count;
 	}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (!strcmp(a->attr.name, "compress_log_size")) {
+		if (t < MIN_COMPRESS_LOG_SIZE || t > MAX_COMPRESS_LOG_SIZE)
+			return -EINVAL;
+		f2fs_down_write(&sbi->sb_lock);
+		F2FS_OPTION(sbi).compress_log_size = (unsigned char)t;
+		f2fs_up_write(&sbi->sb_lock);
+		return count;
+	}
+#endif
 #endif
 
 	if (!strcmp(a->attr.name, "atgc_candidate_ratio")) {
@@ -682,6 +827,19 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 			return -EINVAL;
 		return count;
 	}
+#ifdef CONFIG_F2FS_APPBOOST
+	if (!strcmp(a->attr.name, "appboost")) {
+		if (t)
+			sbi->appboost = 1;
+		else
+			sbi->appboost = 0;
+		return count;
+	}
+	if (!strcmp(a->attr.name, "appboost_max_blocks")) {
+		sbi->appboost_max_blocks = t;
+		return count;
+	}
+#endif
 
 	if (!strcmp(a->attr.name, "max_fragment_hole")) {
 		if (t >= MIN_FRAGMENT_SIZE && t <= MAX_FRAGMENT_SIZE)
@@ -825,15 +983,54 @@ static void f2fs_sb_release(struct kobject *kobj)
 static ssize_t f2fs_feature_show(struct f2fs_attr *a,
 		struct f2fs_sb_info *sbi, char *buf)
 {
+	if (!strcmp(a->attr.name, "seqzone")) {
+		if (!seqzone_switch)
+			return sysfs_emit(buf, "unsupported\n");
+		else
+			return sysfs_emit(buf, seqzone_switch);
+	}
+
 	return sysfs_emit(buf, "supported\n");
 }
 
+static ssize_t f2fs_may_compr_show(struct f2fs_attr *a,
+		struct f2fs_sb_info *sbi, char *buf)
+{
+	if (!strcmp(a->attr.name, "may_compress"))
+		return sprintf(buf, "%d", may_compress ? 1 : 0);
+	else if (!strcmp(a->attr.name, "may_set_compr_fl"))
+		return sprintf(buf, "%d", may_set_compr_fl ? 1 : 0);
+	return -EINVAL;
+}
+
+static ssize_t f2fs_may_compr_store(struct f2fs_attr *a,
+			struct f2fs_sb_info *sbi, const char *buf, size_t count)
+{
+	int val, ret;
+
+	ret = kstrtoint(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	if (!strcmp(a->attr.name, "may_compress"))
+		may_compress = val == 0 ? false : true;
+	else if (!strcmp(a->attr.name, "may_set_compr_fl"))
+		may_set_compr_fl = val == 0 ? false : true;
+	return count;
+}
+
 #define F2FS_FEATURE_RO_ATTR(_name)				\
 static struct f2fs_attr f2fs_attr_##_name = {			\
 	.attr = {.name = __stringify(_name), .mode = 0444 },	\
 	.show	= f2fs_feature_show,				\
 }
 
+#define F2FS_FEATURE_RW_ATTR(_name)				\
+static struct f2fs_attr f2fs_attr_##_name = {			\
+	.attr = {.name = __stringify(_name), .mode = 0644 },	\
+	.show	= f2fs_may_compr_show,				\
+	.store	= f2fs_may_compr_store,				\
+}
+
 static ssize_t f2fs_sb_feature_show(struct f2fs_attr *a,
 		struct f2fs_sb_info *sbi, char *buf)
 {
@@ -895,6 +1092,7 @@ F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, max_discard_issue_time, max_discard_
 F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, discard_io_aware_gran, discard_io_aware_gran);
 F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, discard_urgent_util, discard_urgent_util);
 F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, discard_granularity, discard_granularity);
+F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, discard_io_aware, discard_io_aware);
 F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, max_ordered_discard, max_ordered_discard);
 F2FS_RW_ATTR(RESERVED_BLOCKS, f2fs_sb_info, reserved_blocks, reserved_blocks);
 F2FS_RW_ATTR(SM_INFO, f2fs_sm_info, ipu_policy, ipu_policy);
@@ -907,6 +1105,13 @@ F2FS_RW_ATTR(NM_INFO, f2fs_nm_info, ram_thresh, ram_thresh);
 F2FS_RW_ATTR(NM_INFO, f2fs_nm_info, ra_nid_pages, ra_nid_pages);
 F2FS_RW_ATTR(NM_INFO, f2fs_nm_info, dirty_nats_ratio, dirty_nats_ratio);
 F2FS_RW_ATTR(NM_INFO, f2fs_nm_info, max_roll_forward_node_blocks, max_rf_node_blocks);
+#ifdef CONFIG_F2FS_APPBOOST
+F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, appboost, appboost);
+F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, appboost_max_blocks, appboost_max_blocks);
+#endif
+#ifdef CONFIG_F2FS_SEQZONE
+F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, seq_zone, seq_zone);
+#endif
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, max_victim_search, max_victim_search);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, migration_granularity, migration_granularity);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, dir_level, dir_level);
@@ -941,8 +1146,6 @@ F2FS_GENERAL_RO_ATTR(features);
 F2FS_GENERAL_RO_ATTR(current_reserved_blocks);
 F2FS_GENERAL_RO_ATTR(unusable);
 F2FS_GENERAL_RO_ATTR(encoding);
-F2FS_GENERAL_RO_ATTR(encoding_flags);
-F2FS_GENERAL_RO_ATTR(effective_lookup_mode);
 F2FS_GENERAL_RO_ATTR(mounted_time_sec);
 F2FS_GENERAL_RO_ATTR(main_blkaddr);
 F2FS_GENERAL_RO_ATTR(pending_discard);
@@ -992,10 +1195,23 @@ F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, compr_saved_block, compr_saved_block);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, compr_new_inode, compr_new_inode);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, compress_percent, compress_percent);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, compress_watermark, compress_watermark);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+F2FS_RW_ATTR(COMP_EXT, f2fs_mount_info, compress_layout, compress_layout);
+F2FS_RW_ATTR(COMP_EXT, f2fs_mount_info, compress_extension, extensions);
+F2FS_RW_ATTR(COMP_EXT, f2fs_mount_info, compress_noextension, noextensions);
+F2FS_RW_ATTR(COMP_EXT, f2fs_mount_info, compress_log_size, compress_log_size);
+#endif
 #endif
 F2FS_FEATURE_RO_ATTR(pin_file);
-#ifdef CONFIG_UNICODE
-F2FS_FEATURE_RO_ATTR(linear_lookup);
+#ifdef CONFIG_F2FS_FS_DEDUP
+F2FS_FEATURE_RO_ATTR(dedup);
+F2FS_FEATURE_RO_ATTR(snapshot);
+#endif
+F2FS_FEATURE_RW_ATTR(may_compress);
+F2FS_FEATURE_RW_ATTR(may_set_compr_fl);
+
+#ifdef CONFIG_F2FS_SEQZONE
+F2FS_FEATURE_RO_ATTR(seqzone);
 #endif
 
 /* For ATGC */
@@ -1039,6 +1255,7 @@ static struct attribute *f2fs_attrs[] = {
 	ATTR_LIST(discard_io_aware_gran),
 	ATTR_LIST(discard_urgent_util),
 	ATTR_LIST(discard_granularity),
+	ATTR_LIST(discard_io_aware),
 	ATTR_LIST(max_ordered_discard),
 	ATTR_LIST(pending_discard),
 	ATTR_LIST(gc_mode),
@@ -1049,6 +1266,13 @@ static struct attribute *f2fs_attrs[] = {
 	ATTR_LIST(min_hot_blocks),
 	ATTR_LIST(min_ssr_sections),
 	ATTR_LIST(max_victim_search),
+#ifdef CONFIG_F2FS_APPBOOST
+	ATTR_LIST(appboost),
+	ATTR_LIST(appboost_max_blocks),
+#endif
+#ifdef CONFIG_F2FS_SEQZONE
+	ATTR_LIST(seq_zone),
+#endif
 	ATTR_LIST(migration_granularity),
 	ATTR_LIST(dir_level),
 	ATTR_LIST(ram_thresh),
@@ -1085,8 +1309,6 @@ static struct attribute *f2fs_attrs[] = {
 	ATTR_LIST(reserved_blocks),
 	ATTR_LIST(current_reserved_blocks),
 	ATTR_LIST(encoding),
-	ATTR_LIST(encoding_flags),
-	ATTR_LIST(effective_lookup_mode),
 	ATTR_LIST(mounted_time_sec),
 #ifdef CONFIG_F2FS_STAT_FS
 	ATTR_LIST(cp_foreground_calls),
@@ -1106,6 +1328,12 @@ static struct attribute *f2fs_attrs[] = {
 	ATTR_LIST(compr_new_inode),
 	ATTR_LIST(compress_percent),
 	ATTR_LIST(compress_watermark),
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	ATTR_LIST(compress_layout),
+	ATTR_LIST(compress_extension),
+	ATTR_LIST(compress_noextension),
+	ATTR_LIST(compress_log_size),
+#endif
 #endif
 	/* For ATGC */
 	ATTR_LIST(atgc_candidate_ratio),
@@ -1159,9 +1387,16 @@ static struct attribute *f2fs_feat_attrs[] = {
 	ATTR_LIST(compression),
 #endif
 	ATTR_LIST(pin_file),
-#ifdef CONFIG_UNICODE
-	ATTR_LIST(linear_lookup),
+#ifdef CONFIG_F2FS_FS_DEDUP
+	ATTR_LIST(dedup),
+	ATTR_LIST(snapshot),
 #endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+	ATTR_LIST(seqzone),
+#endif
+	ATTR_LIST(may_compress),
+	ATTR_LIST(may_set_compr_fl),
 	NULL,
 };
 ATTRIBUTE_GROUPS(f2fs_feat);
@@ -1188,8 +1423,13 @@ F2FS_SB_FEATURE_RO_ATTR(verity, VERITY);
 F2FS_SB_FEATURE_RO_ATTR(sb_checksum, SB_CHKSUM);
 F2FS_SB_FEATURE_RO_ATTR(casefold, CASEFOLD);
 F2FS_SB_FEATURE_RO_ATTR(compression, COMPRESSION);
+#ifdef CONFIG_F2FS_DEDUP
+F2FS_SB_FEATURE_RO_ATTR(dedup, DEDUP);
+#endif
 F2FS_SB_FEATURE_RO_ATTR(readonly, RO);
-
+#ifdef CONFIG_F2FS_SEQZONE
+F2FS_SB_FEATURE_RO_ATTR(seqzone, SEQZONE);
+#endif
 static struct attribute *f2fs_sb_feat_attrs[] = {
 	ATTR_LIST(sb_encryption),
 	ATTR_LIST(sb_block_zoned),
@@ -1204,7 +1444,13 @@ static struct attribute *f2fs_sb_feat_attrs[] = {
 	ATTR_LIST(sb_sb_checksum),
 	ATTR_LIST(sb_casefold),
 	ATTR_LIST(sb_compression),
+#ifdef CONFIG_F2FS_DEDUP
+	ATTR_LIST(sb_dedup),
+#endif
 	ATTR_LIST(sb_readonly),
+#ifdef CONFIG_F2FS_SEQZONE
+	ATTR_LIST(sb_seqzone),
+#endif
 	NULL,
 };
 ATTRIBUTE_GROUPS(f2fs_sb_feat);
diff --git a/fs/f2fs/verity.c b/fs/f2fs/verity.c
index 4fc95f353..36830b062 100644
--- a/fs/f2fs/verity.c
+++ b/fs/f2fs/verity.c
@@ -139,6 +139,15 @@ static int f2fs_begin_enable_verity(struct file *filp)
 	if (err)
 		return err;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		err = f2fs_revoke_deduped_inode(inode, __func__);
+		if (err)
+			return err;
+	}
+#endif
+
 	set_inode_flag(inode, FI_VERITY_IN_PROGRESS);
 	return 0;
 }
diff --git a/fs/f2fs/xattr.c b/fs/f2fs/xattr.c
index 65437c18e..9f1fb1818 100644
--- a/fs/f2fs/xattr.c
+++ b/fs/f2fs/xattr.c
@@ -661,14 +661,11 @@ static int __f2fs_setxattr(struct inode *inode, int index,
 	here = __find_xattr(base_addr, last_base_addr, NULL, index, len, name);
 	if (!here) {
 		if (!F2FS_I(inode)->i_xattr_nid) {
-			error = f2fs_recover_xattr_data(inode, NULL);
 			f2fs_notice(F2FS_I_SB(inode),
-				"recover xattr in inode (%lu), error(%d)",
-					inode->i_ino, error);
-			if (!error) {
-				kfree(base_addr);
-				goto retry;
-			}
+				"recover xattr in inode (%lu)", inode->i_ino);
+			f2fs_recover_xattr_data(inode, NULL);
+			kfree(base_addr);
+			goto retry;
 		}
 		f2fs_err(F2FS_I_SB(inode), "set inode (%lu) has corrupted xattr",
 								inode->i_ino);
@@ -758,12 +755,6 @@ static int __f2fs_setxattr(struct inode *inode, int index,
 		memcpy(pval, value, size);
 		last->e_value_size = cpu_to_le16(size);
 		new_hsize += newsize;
-		/*
-		 * Explicitly add the null terminator.  The unused xattr space
-		 * is supposed to always be zeroed, which would make this
-		 * unnecessary, but don't depend on that.
-		 */
-		*(u32 *)((u8 *)last + newsize) = 0;
 	}
 
 	error = write_all_xattrs(inode, new_hsize, base_addr, ipage);
diff --git a/fs/f2fs/xattr.h b/fs/f2fs/xattr.h
index 416d65277..b1811c392 100644
--- a/fs/f2fs/xattr.h
+++ b/fs/f2fs/xattr.h
@@ -83,6 +83,7 @@ struct f2fs_xattr_entry {
 				sizeof(struct f2fs_xattr_header) -	\
 				sizeof(struct f2fs_xattr_entry))
 
+#define MIN_INLINE_XATTR_SIZE (sizeof(struct f2fs_xattr_header) / sizeof(__le32))
 #define MAX_INLINE_XATTR_SIZE						\
 			(DEF_ADDRS_PER_INODE -				\
 			F2FS_TOTAL_EXTRA_ATTR_SIZE / sizeof(__le32) -	\
diff --git a/include/linux/f2fs_fs.h b/include/linux/f2fs_fs.h
index e5951002e..1cea5aeea 100644
--- a/include/linux/f2fs_fs.h
+++ b/include/linux/f2fs_fs.h
@@ -24,6 +24,8 @@
 #define NULL_ADDR		((block_t)0)	/* used as block_t addresses */
 #define NEW_ADDR		((block_t)-1)	/* used as block_t addresses */
 #define COMPRESS_ADDR		((block_t)-2)	/* used as compressed data flag */
+#define DEDUP_ADDR		((block_t)-1024)/* used as block_t addresses */
+#define SEQZONE_ADDR		((block_t)-1025)/* used as seqzone addresses */
 
 #define F2FS_BYTES_TO_BLK(bytes)	((bytes) >> F2FS_BLKSIZE_BITS)
 #define F2FS_BLK_TO_BYTES(blk)		((blk) << F2FS_BLKSIZE_BITS)
@@ -332,9 +334,15 @@ struct f2fs_inode {
 			__u8 i_compress_algorithm;	/* compress algorithm */
 			__u8 i_log_cluster_size;	/* log of cluster size */
 			__le16 i_compress_flag;		/* compress flag */
-						/* 0 bit: chksum flag
-						 * [8,15] bits: compress level
-						 */
+							/* 0 bit: chksum flag
+							 * [1,2] bits: compress layout
+							 * 3 bit: readpage updates atime
+							 * [4,7] bits: reserved
+							 * [8,15] bits: compress level
+							 */
+			__le32 i_inner_ino;     /* for layered inode */
+			__le32 i_dedup_flags;   /* dedup file attributes */
+			__le32 i_dedup_rsvd;    /* reserved for dedup */
 			__le32 i_extra_end[0];	/* for attribute size calculation */
 		} __packed;
 		__le32 i_addr[DEF_ADDRS_PER_INODE];	/* Pointers to data blocks */
diff --git a/include/trace/events/f2fs.h b/include/trace/events/f2fs.h
index 222324551..cb6be14f8 100644
--- a/include/trace/events/f2fs.h
+++ b/include/trace/events/f2fs.h
@@ -2209,8 +2209,11 @@ DECLARE_EVENT_CLASS(f2fs__rw_start,
 		__string(cmdline, command)
 		__field(pid_t, pid)
 		__field(ino_t, ino)
+#ifdef CONFIG_F2FS_APPBOOST
+		__field(u64, mtime)
+		__field(u32, i_generation)
+#endif
 	),
-
 	TP_fast_assign(
 		/*
 		 * Replace the spaces in filenames and cmdlines
@@ -2226,13 +2229,24 @@ DECLARE_EVENT_CLASS(f2fs__rw_start,
 		(void)strreplace(__get_str(cmdline), ' ', '_');
 		__entry->pid = pid;
 		__entry->ino = inode->i_ino;
+#ifdef CONFIG_F2FS_APPBOOST
+		__entry->mtime = timespec64_to_ns(&inode->i_mtime);
+		__entry->i_generation = inode->i_generation;
+#endif
 	),
-
+#ifdef CONFIG_F2FS_APPBOOST
+	TP_printk("entry_name %s, offset %llu, bytes %d, cmdline %s,"
+		" pid %d, i_size %llu, ino %lu, mtime %llu, i_generation %u",
+		__get_str(pathbuf), __entry->offset, __entry->bytes,
+		__get_str(cmdline), __entry->pid, __entry->i_size,
+		(unsigned long) __entry->ino, __entry->mtime,  __entry->i_generation)
+#else
 	TP_printk("entry_name %s, offset %llu, bytes %d, cmdline %s,"
 		" pid %d, i_size %llu, ino %lu",
 		__get_str(pathbuf), __entry->offset, __entry->bytes,
 		__get_str(cmdline), __entry->pid, __entry->i_size,
 		(unsigned long) __entry->ino)
+#endif
 );
 
 DECLARE_EVENT_CLASS(f2fs__rw_end,
@@ -2282,12 +2296,128 @@ DEFINE_EVENT(f2fs__rw_start, f2fs_datawrite_start,
 );
 
 DEFINE_EVENT(f2fs__rw_end, f2fs_datawrite_end,
-
 	TP_PROTO(struct inode *inode, loff_t offset, int bytes),
 
 	TP_ARGS(inode, offset, bytes)
 );
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+DECLARE_EVENT_CLASS(f2fs__dedup_inode,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner),
+
+	TP_STRUCT__entry(
+		__field(dev_t,  dev)
+		__field(ino_t,  ino)
+		__field(ino_t,  pino)
+		__field(loff_t, size)
+		__field(unsigned int, nlink)
+		__field(ino_t,  inner_ino)
+		__field(unsigned int, inner_nlink)
+	),
+
+	TP_fast_assign(
+		__entry->dev    = inode->i_sb->s_dev;
+		__entry->ino    = inode->i_ino;
+		__entry->pino   = F2FS_I(inode)->i_pino;
+		__entry->nlink  = inode->i_nlink;
+		__entry->size   = inode->i_size;
+		__entry->inner_ino      = inner->i_ino;
+		__entry->inner_nlink    = inner->i_nlink;
+	),
+
+	TP_printk("dev = (%d,%d), ino = %lu, pino = %lu, "
+		"i_size = %lld, i_nlink = %u, "
+		"inner = %lu, inner i_nlink = %u",
+		show_dev_ino(__entry),
+		(unsigned long)__entry->pino,
+		__entry->size,
+		(unsigned int)__entry->nlink,
+		(unsigned long)__entry->inner_ino,
+		(unsigned int)__entry->inner_nlink)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_ioc_create_layered_inode,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_ioc_dedup_inode,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_revoke_inode,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_revoke_fail,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_dec_inner_link,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DECLARE_EVENT_CLASS(f2fs__dedup_map,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner),
+
+	TP_STRUCT__entry(
+		__field(dev_t,  dev)
+		__field(ino_t,  ino)
+		__field(loff_t, size)
+		__field(ino_t,  inner_ino)
+		__field(unsigned int, inner_nlink)
+	),
+
+	TP_fast_assign(
+		__entry->dev    = inode->i_sb->s_dev;
+		__entry->ino    = inode->i_ino;
+		__entry->size   = inode->i_size;
+		__entry->inner_ino      = inner->i_ino;
+		__entry->inner_nlink    = inner->i_nlink;
+	),
+
+	TP_printk("dev = (%d,%d), outer ino = %lu, i_size = %lld, "
+		"map to inner ino = %lu, inner i_nlink = %u",
+		show_dev_ino(__entry),
+		__entry->size,
+		(unsigned long)__entry->inner_ino,
+		(unsigned int)__entry->inner_nlink)
+);
+
+DEFINE_EVENT(f2fs__dedup_map, f2fs_dedup_map_readpage,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_map, f2fs_dedup_map_blocks,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+#endif /* CONFIG_F2FS_FS_DEDUP */
 #endif /* _TRACE_F2FS_H */
 
  /* This part must be outside protection */
diff --git a/include/uapi/linux/f2fs.h b/include/uapi/linux/f2fs.h
index 955d440be..481ab7360 100644
--- a/include/uapi/linux/f2fs.h
+++ b/include/uapi/linux/f2fs.h
@@ -43,6 +43,50 @@
 #define F2FS_IOC_DECOMPRESS_FILE	_IO(F2FS_IOCTL_MAGIC, 23)
 #define F2FS_IOC_COMPRESS_FILE		_IO(F2FS_IOCTL_MAGIC, 24)
 #define F2FS_IOC_START_ATOMIC_REPLACE	_IO(F2FS_IOCTL_MAGIC, 25)
+#define F2FS_IOC_GET_EXTRA_ATTR		_IOR(F2FS_IOCTL_MAGIC, 26,	\
+						struct f2fs_extra_attr)
+#define F2FS_IOC_SET_EXTRA_ATTR		_IOW(F2FS_IOCTL_MAGIC, 27,	\
+						struct f2fs_extra_attr)
+#define F2FS_APPBOOST_IOC_BASE (50)
+enum {
+	APPBOOST_PRELOAD = F2FS_APPBOOST_IOC_BASE,
+	APPBOOST_START_MERGE,
+	APPBOOST_END_MERGE,
+	APPBOOST_ABORT_PRELOAD,
+	SET_SEQZONE,
+	GET_SEQZONE,
+};
+#define F2FS_IOC_PRELOAD_FILE		_IOW(F2FS_IOCTL_MAGIC,		\
+						APPBOOST_PRELOAD, __u32)
+#define F2FS_IOC_START_MERGE_FILE	_IOW(F2FS_IOCTL_MAGIC,		\
+						APPBOOST_START_MERGE, struct merge_file_user)
+#define F2FS_IOC_END_MERGE_FILE		_IO(F2FS_IOCTL_MAGIC, APPBOOST_END_MERGE)
+#define F2FS_IOC_ABORT_PRELOAD_FILE	_IO(F2FS_IOCTL_MAGIC, APPBOOST_ABORT_PRELOAD)
+#define F2FS_IOC_SET_SEQZONE_FILE	_IOW(F2FS_IOCTL_MAGIC, SET_SEQZONE, __u32)
+#define F2FS_IOC_GET_SEQZONE_FILE	_IOR(F2FS_IOCTL_MAGIC, GET_SEQZONE, __u32)
+
+#define F2FS_DEDUP_IOC_BASE	(100)
+#define F2FS_IOC_DEDUP_CREATE		_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 0, struct f2fs_dedup_src)
+#define F2FS_IOC_DEDUP_FILE		_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 1, struct f2fs_dedup_dst)
+#define F2FS_IOC_DEDUP_REVOKE		_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 2, struct f2fs_dedup_revoke)
+#define F2FS_IOC_DEDUP_GET_FILE_INFO	_IOR(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 3, struct f2fs_dedup_file_info)
+#define F2FS_IOC_DEDUP_GET_SYS_INFO	_IOR(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 4, struct f2fs_dedup_sys_info)
+
+#define F2FS_IOC_CLONE_FILE		_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 5, struct f2fs_clone_info)
+#define F2FS_IOC_MODIFY_CHECK	_IOWR(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 6, struct f2fs_modify_check_info)
+#define F2FS_IOC_DEDUP_PERM_CHECK \
+			_IO(F2FS_IOCTL_MAGIC, F2FS_DEDUP_IOC_BASE + 7)
+#define F2FS_IOC_SNAPSHOT_CREATE	_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 8, struct f2fs_dedup_dst)
+#define F2FS_IOC_SNAPSHOT_PREPARE	_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 9, struct f2fs_dedup_src)
 
 /*
  * should be same as XFS_IOC_GOINGDOWN.
@@ -96,4 +140,84 @@ struct f2fs_comp_option {
 	__u8 log_cluster_size;
 };
 
+struct merge_file_user {
+	unsigned ino;
+	unsigned extent_count;
+	unsigned int i_generation;
+	unsigned int REV;
+	__u64 mtime;
+	__u8 __user *extents;
+};
+
+struct f2fs_comp_option_v2 {
+	__u8 algorithm;
+	__u8 log_cluster_size;
+	__u8 level;
+	__u8 flag;
+};
+
+enum {
+	F2FS_EXTRA_ATTR_TOTAL_SIZE,		/* ro, size of extra attr area */
+	F2FS_EXTRA_ATTR_ISIZE,			/* ro, i_extra_isize */
+	F2FS_EXTRA_ATTR_INLINE_XATTR_SIZE,	/* rw, i_inline_xattr_size */
+	F2FS_EXTRA_ATTR_PROJID,			/* ro, i_projid */
+	F2FS_EXTRA_ATTR_INODE_CHKSUM,		/* ro, i_inode_chksum */
+	F2FS_EXTRA_ATTR_CRTIME,			/* ro, i_crtime, i_crtime_nsec */
+	F2FS_EXTRA_ATTR_COMPR_BLOCKS,		/* ro, i_compr_blocks */
+	F2FS_EXTRA_ATTR_COMPR_OPTION,		/* rw, i_compress_algorithm,
+						 *     i_log_cluster_size,
+						 *     i_compress_flag
+						 */
+	F2FS_EXTRA_ATTR_MAX,
+};
+
+struct f2fs_extra_attr {
+	__u8 field;		/* F2FS_EXTRA_ATTR_* */
+	__u8 rsvd1;
+	__u16 attr_size;	/* size of @attr */
+	__u32 rsvd2;
+	__u64 attr;		/* attr value or pointer */
+};
+
+/* F2FS_IOC_DEDUP_CREATE */
+struct f2fs_dedup_src {
+	int inner_fd;
+};
+
+/* F2FS_IOC_DEDUP_FILE */
+struct f2fs_dedup_dst {
+	int base_fd;
+	int tmp_fd;
+};
+
+/* F2FS_IOC_DEDUP_REVOKE */
+struct f2fs_dedup_revoke {
+	int revoke_fd;
+};
+
+/* F2FS_IOC_DEDUP_GET_FILE_INFO */
+struct f2fs_dedup_file_info {
+	short is_deduped;
+	short is_layered;
+	int group;
+};
+
+/* F2FS_IOC_DEDUP_GET_SYS_INFO */
+struct f2fs_dedup_sys_info {
+	__u64 file_count;
+	__u64 file_space;		/* MB */
+};
+
+struct f2fs_clone_info {
+	int src_fd;
+	int flags;	/* meta/data index */
+};
+
+struct f2fs_modify_check_info {
+	int flag;	/* data/meta */
+	int mode;	/* set/get/clear */
+};
+
+#define F2FS_KBYTE_SHIFT	10
+
 #endif /* _UAPI_LINUX_F2FS_H */
diff --git a/include/uapi/linux/stat.h b/include/uapi/linux/stat.h
index 7cab2c65d..47d79daed 100644
--- a/include/uapi/linux/stat.h
+++ b/include/uapi/linux/stat.h
@@ -184,6 +184,7 @@ struct statx {
 #define STATX_ATTR_IMMUTABLE		0x00000010 /* [I] File is marked immutable */
 #define STATX_ATTR_APPEND		0x00000020 /* [I] File is append-only */
 #define STATX_ATTR_NODUMP		0x00000040 /* [I] File is not to be dumped */
+#define STATX_ATTR_NOCOMPR		0x00000400 /* [I] File is worthless to compress */
 #define STATX_ATTR_ENCRYPTED		0x00000800 /* [I] File requires key to decrypt in fs */
 #define STATX_ATTR_AUTOMOUNT		0x00001000 /* Dir: Automount trigger */
 #define STATX_ATTR_MOUNT_ROOT		0x00002000 /* Root of a mount */
diff --git a/kernel/locking/Makefile b/kernel/locking/Makefile
index ea925731f..afd151ae1 100644
--- a/kernel/locking/Makefile
+++ b/kernel/locking/Makefile
@@ -3,7 +3,7 @@
 # and is generally not a function of system call inputs.
 KCOV_INSTRUMENT		:= n
 
-obj-y += mutex.o semaphore.o rwsem.o percpu-rwsem.o
+obj-y += mutex.o semaphore.o rwsem.o percpu-rwsem.o oplus_locking.o
 
 # Avoid recursion lockdep -> sanitizer -> ... -> lockdep.
 KCSAN_SANITIZE_lockdep.o := n
diff --git a/kernel/locking/locking_main.h b/kernel/locking/locking_main.h
new file mode 100644
index 000000000..809e98b17
--- /dev/null
+++ b/kernel/locking/locking_main.h
@@ -0,0 +1,165 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Copyright (C) 2020-2022 Oplus. All rights reserved.
+ */
+
+#ifndef _OPLUS_LOCKING_MAIN_H_
+#define _OPLUS_LOCKING_MAIN_H_
+
+#include "sa_common_struct.h"
+
+#define cond_trace_printk(cond, fmt, ...)	\
+do {										\
+	if (cond)								\
+		trace_printk(fmt, ##__VA_ARGS__);	\
+} while (0)
+
+
+
+#define MAGIC_NUM       (0xdead0000)
+#define MAGIC_MASK      (0xffff0000)
+#define MAGIC_SHIFT     (16)
+#define OWNER_BIT       (1 << 0)
+#define THREAD_INFO_BIT (1 << 1)
+#define TYPE_BIT        (1 << 2)
+
+#define UX_FLAG_BIT       (1<<0)
+#define SS_FLAG_BIT       (1<<1)
+#define GRP_SHIFT         (2)
+#define GRP_FLAG_MASK     (7 << GRP_SHIFT)
+#define U_GRP_OTHER       (1 << GRP_SHIFT)
+#define U_GRP_BACKGROUND  (2 << GRP_SHIFT)
+#define U_GRP_FRONDGROUD  (3 << GRP_SHIFT)
+#define U_GRP_TOP_APP     (4 << GRP_SHIFT)
+
+#define LOCK_TYPE_SHIFT (30)
+#define INVALID_TYPE    (0)
+#define LOCK_ART        (1)
+#define LOCK_JUC        (2)
+
+#define lk_err(fmt, ...) \
+		pr_err("[oplus_locking][%s]"fmt, __func__, ##__VA_ARGS__)
+#define lk_warn(fmt, ...) \
+		pr_warn("[oplus_locking][%s]"fmt, __func__, ##__VA_ARGS__)
+#define lk_info(fmt, ...) \
+		pr_info("[oplus_locking][%s]"fmt, __func__, ##__VA_ARGS__)
+
+#define OTS_IDX			0
+
+
+struct futex_uinfo {
+	u32 cmd;
+	u32 owner_tid;
+	u32 type;
+	u64 inform_user;
+};
+
+enum {
+	CGROUP_RESV = 0,
+	CGROUP_DEFAULT,
+	CGROUP_FOREGROUND,
+	CGROUP_BACKGROUND,
+	CGROUP_TOP_APP,
+
+	CGROUP_NRS,
+};
+
+enum rwsem_waiter_type {
+	RWSEM_WAITING_FOR_WRITE,
+	RWSEM_WAITING_FOR_READ
+};
+
+struct rwsem_waiter {
+	struct list_head list;
+	struct task_struct *task;
+	enum rwsem_waiter_type type;
+	unsigned long timeout;
+	bool handoff_set;
+};
+
+#define LK_MUTEX_ENABLE (1 << 0)
+#define LK_RWSEM_ENABLE (1 << 1)
+#define LK_FUTEX_ENABLE (1 << 2)
+#define LK_OSQ_ENABLE   (1 << 3)
+
+#ifdef CONFIG_OPLUS_LOCKING_MONITOR
+/*
+ * The bit definitions of the g_opt_enable:
+ * bit 0-7: reserved bits for other locking optimation.
+ * bit8 ~ bit10(each monitor version is exclusive):
+ * 1 : monitor control, level-0(internal version).
+ * 2 : monitor control, level-1(trial version).
+ * 3 : monitor control, level-2(official version).
+ */
+#define LK_MONITOR_SHIFT  (8)
+#define LK_MONITOR_MASK   (7 << LK_MONITOR_SHIFT)
+#define LK_MONITOR_LEVEL0 (1 << LK_MONITOR_SHIFT)
+#define LK_MONITOR_LEVEL1 (2 << LK_MONITOR_SHIFT)
+#define LK_MONITOR_LEVEL2 (3 << LK_MONITOR_SHIFT)
+#endif
+
+#define LK_DEBUG_PRINTK (1 << 0)
+#define LK_DEBUG_FTRACE (1 << 1)
+
+extern unsigned int g_opt_enable;
+extern unsigned int g_opt_debug;
+
+extern atomic64_t futex_inherit_set_times;
+extern atomic64_t futex_inherit_unset_times;
+extern atomic64_t futex_inherit_useless_times;
+extern atomic64_t futex_low_count;
+extern atomic64_t futex_high_count;
+
+static inline bool locking_opt_enable(unsigned int enable)
+{
+	return g_opt_enable & enable;
+}
+
+#ifdef CONFIG_OPLUS_LOCKING_MONITOR
+static inline bool lock_supp_level(int level)
+{
+	return (g_opt_enable & LK_MONITOR_MASK) == level;
+}
+#endif
+
+static inline bool locking_opt_debug(int debug)
+{
+	return g_opt_debug & debug;
+}
+
+void register_rwsem_vendor_hooks(void);
+void register_mutex_vendor_hooks(void);
+void register_futex_vendor_hooks(void);
+void register_monitor_vendor_hooks(void);
+void lk_sysfs_init(void);
+#ifdef CONFIG_OPLUS_LOCKING_MONITOR
+int kern_lstat_init(void);
+#endif
+
+void unregister_rwsem_vendor_hooks(void);
+void unregister_mutex_vendor_hooks(void);
+void unregister_futex_vendor_hooks(void);
+void unregister_monitor_vendor_hooks(void);
+void lk_sysfs_exit(void);
+#ifdef CONFIG_OPLUS_LOCKING_MONITOR
+void kern_lstat_exit(void);
+#endif
+#endif /* _OPLUS_LOCKING_MAIN_H_ */
+
+static inline struct oplus_task_struct *get_oplus_task_struct(struct task_struct *t)
+{
+	struct oplus_task_struct *ots = NULL;
+
+	/* not Skip idle thread */
+	if (!t)
+		return NULL;
+
+	ots = (struct oplus_task_struct *) READ_ONCE(t->android_oem_data1[OTS_IDX]);
+	if (IS_ERR_OR_NULL(ots))
+		return NULL;
+
+	return ots;
+}
+
+void locking_record_switch_in_cs(struct task_struct *tsk);
+
diff --git a/kernel/locking/oplus_locking.c b/kernel/locking/oplus_locking.c
new file mode 100644
index 000000000..338097ebc
--- /dev/null
+++ b/kernel/locking/oplus_locking.c
@@ -0,0 +1,885 @@
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/version.h>
+#include <linux/sched/clock.h>
+#include <linux/cgroup.h>
+#include <linux/ftrace.h>
+#include <linux/futex.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/rwsem.h>
+#include <linux/rtmutex.h>
+#include <linux/mutex.h>
+#include <trace/hooks/dtask.h>
+#include <trace/hooks/futex.h>
+#include <trace/hooks/sched.h>
+#include <trace/events/sched.h>
+#include <trace/events/task.h>
+#include <linux/sched.h>
+
+
+//#include <../kernel/oplus_cpu/sched/sched_assist/sa_common.h>
+
+#include "locking_main.h"
+static inline struct oplus_task_struct *get_oplus_task_struct(struct task_struct *t);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0))
+#define PDE_DATA pde_data
+#endif
+
+static int debug_enable_flag;
+static bool init_done = false;
+atomic64_t switch_in_cs_cnts;
+
+#define SAVE_TRACE_NUMS	4
+struct lock_record {
+	int type;
+	u64 lock_id;
+	u64 start_time;
+
+	unsigned long traces[SAVE_TRACE_NUMS];
+
+	struct list_head node;
+};
+
+static const char *skip_str[] = {"f2fs_quota_sync", "f2fs_dquot_commit", "aio_write", "aio_complete_rw", "mcp_wait",
+                                 "session_gp_invoke_command", "ksys_read", "devkmsg_read", "n_tty_read", "__driver_attach",
+                                 "btmtk_uart_cif_mutex_unlock", "wfsys_unlock", "connv3_core_pre_cal_start", "regulator_unlock_recursive"};
+
+#define MUTEX		0
+#define RWSEM		1
+#define RTMUTEX		2
+#define PCP_RWSEM	3
+#define LOCK_TYPES	4
+
+char *lock_str[LOCK_TYPES] = {"mutex", "rwsem", "rtmutex", "pcp-rwsem"};
+
+static atomic64_t wait_cnt[LOCK_TYPES];
+static atomic64_t lock_cnts[LOCK_TYPES];
+static atomic64_t max_wait_cnts[LOCK_TYPES];
+static atomic_t max_depth;
+
+#define CS_STATS_LVLS	6
+static atomic64_t cs_duration[LOCK_TYPES][CS_STATS_LVLS];
+
+static unsigned long scopes[CS_STATS_LVLS][2] = {
+	{0, 2 * NSEC_PER_MSEC},
+	{2 * NSEC_PER_MSEC, 10 * NSEC_PER_MSEC},
+	{10 * NSEC_PER_MSEC, 50 * NSEC_PER_MSEC},
+	{50 * NSEC_PER_MSEC, 200 * NSEC_PER_MSEC},
+	{200 * NSEC_PER_MSEC, 1 * NSEC_PER_SEC},
+	{1 * NSEC_PER_SEC, ULONG_MAX}
+};
+
+#define MAX_TRACE_DEPTH	10
+
+static bool is_match_skip_str(unsigned long *entries, int nentries)
+{
+	char trace_str[KSYM_SYMBOL_LEN];
+	char buf[1024];
+	int idx = 0;
+	int i;
+
+	for (i = 0; i < nentries; i++) {
+		sprint_symbol_no_offset(trace_str, entries[i]);
+		idx += sprintf(&buf[idx], "%s-", trace_str);
+	}
+
+	for (i = 0; i < sizeof(skip_str) / sizeof(skip_str[0]); i++) {
+		if (strstr(buf, skip_str[i])) {
+			//pr_err("krn_reliab : match string = %s\n", strstr(buf, skip_str[i]));
+			return true;
+		}
+	}
+
+	return false;
+}
+
+#define WALK_BACKTRACE_LEVELS	4
+static bool is_skip_report_error(void)
+{
+	unsigned long entries[WALK_BACKTRACE_LEVELS];
+
+	stack_trace_save(entries, WALK_BACKTRACE_LEVELS, 3);
+
+	return is_match_skip_str(entries, WALK_BACKTRACE_LEVELS);
+}
+
+static void output_backtrace(unsigned long *entries, int nentrie)
+{
+	char trace_str[KSYM_SYMBOL_LEN];
+	char *buf;
+	int idx = 0;
+	int i;
+
+	buf = kmalloc(KSYM_SYMBOL_LEN * MAX_TRACE_DEPTH, GFP_ATOMIC);
+	if (!buf)
+		return;
+
+	for (i = 0; i < nentrie; i++) {
+		sprint_symbol(trace_str, entries[i]);
+		idx += sprintf(&buf[idx], "%s - ", trace_str);
+	}
+	idx += sprintf(&buf[idx], "\n");
+
+	pr_err("krn_reliab : trace = %s\n", buf);
+
+	kfree(buf);
+}
+
+static void print_backtrace(int skip, int depth)
+{
+	unsigned long entries[MAX_TRACE_DEPTH];
+
+	if (depth >= MAX_TRACE_DEPTH)
+		return;
+
+	stack_trace_save(entries, depth, skip);
+	output_backtrace(entries, depth);
+}
+
+#define CLEAR_PERIOD_MSEC	(5000)
+#define HOLD_EXPIRE_TIME	(10 * NSEC_PER_SEC)
+/* Timer to clear hold lock info which exceed timeout. */
+static atomic64_t clear_seq;
+static struct timer_list clh_timer;
+
+static void clear_hold_timer(struct timer_list *unused)
+{
+	atomic64_inc_return(&clear_seq);
+	mod_timer(&clh_timer, jiffies + CLEAR_PERIOD_MSEC);
+}
+
+static void init_clear_lock_hold_timer(void)
+{
+	timer_setup(&clh_timer, clear_hold_timer, 0);
+	clh_timer.expires = jiffies + msecs_to_jiffies(CLEAR_PERIOD_MSEC);
+	add_timer(&clh_timer);
+}
+
+static void exit_clear_lock_hold_timer(void)
+{
+	del_timer_sync(&clh_timer);
+}
+
+static void stats_cs_duration(int type, u64 duration)
+{
+	int i;
+
+	if (duration > 10 * NSEC_PER_SEC) {
+		pr_err("krn_reliab : lock %s own the lock exceed threshold, duration = %llu s\n", lock_str[type], duration / NSEC_PER_SEC);
+		print_backtrace(2, 7);
+	}
+
+	if (duration > NSEC_PER_SEC) {
+		if (is_skip_report_error()) {
+			//pr_err("match the skip str, do not report the error\n");
+		}
+		else {
+			pr_err("krn_reliab : lock %s own the lock exceed 1s, duration = %llu s\n", lock_str[type], duration / NSEC_PER_SEC);
+			print_backtrace(2, 7);
+			atomic64_inc(&cs_duration[type][CS_STATS_LVLS-1]);
+		}
+	}
+
+	for (i = 0; i < CS_STATS_LVLS-1; i++) {
+		if (duration < scopes[i][1] && duration >= scopes[i][0]) {
+			atomic64_inc(&cs_duration[type][i]);
+			return;
+		}
+	}
+}
+
+void locking_record_switch_in_cs(struct task_struct *tsk)
+{
+	struct oplus_task_struct *ots;
+
+	ots = get_oplus_task_struct(tsk);
+	if (NULL == ots)
+		return;
+	if (atomic_read(&ots->lkinfo.lock_depth)) {
+		atomic64_inc(&switch_in_cs_cnts);
+	}
+}
+EXPORT_SYMBOL(locking_record_switch_in_cs);
+
+static void get_lock_cnts(bool is_lock, int type)
+{
+	if (is_lock)
+		atomic64_inc(&lock_cnts[type]);
+}
+
+void depth_check(struct oplus_task_struct *ots, int flag)
+{
+	int length = 0;
+	struct list_head *list_node;
+
+	list_for_each(list_node, &ots->lkinfo.lock_head) {
+		length++;
+	}
+	
+	if(length != atomic_read(&ots->lkinfo.lock_depth)) {
+		//pr_err("depth:%d, length:%d, flag:%d", atomic_read(&ots->lkinfo.lock_depth), length, flag);
+		atomic_set(&ots->lkinfo.lock_depth, length);
+	}
+}
+
+static void lock_handler(struct task_struct *tsk, u64 lock_id, int type)
+{
+	struct oplus_task_struct *ots;
+	struct lock_record *node, *tmp;
+
+	u64 g_clear_seq;
+	
+	ots = get_oplus_task_struct(tsk);
+	if (IS_ERR_OR_NULL(ots)) {
+		//pr_err("krn_reliab : ots == NULL \n");
+		return;
+	}
+	if (NULL == ots->lkinfo.lock_head.next) {
+		INIT_LIST_HEAD(&ots->lkinfo.lock_head);
+	}
+
+	g_clear_seq = atomic64_read_acquire(&clear_seq);
+	if (ots->lkinfo.clear_seq < g_clear_seq) {
+		ots->lkinfo.clear_seq = g_clear_seq;
+		list_for_each_entry_safe_reverse(node, tmp, &ots->lkinfo.lock_head, node) {
+			if (sched_clock() - node->start_time >= HOLD_EXPIRE_TIME) {
+				if (is_match_skip_str(node->traces, SAVE_TRACE_NUMS)) {
+					pr_err("krn_reliab : free skip expire held lock info, type = %s\n", lock_str[type]);
+				} else {
+					pr_err("krn_reliab : free unskip expire held lock info, type = %s, lock_id = 0x%llx, lock_stamp = %llu\n", lock_str[node->type], node->lock_id, node->start_time);
+					output_backtrace(node->traces, SAVE_TRACE_NUMS);
+					print_backtrace(2, 7);
+				}
+				if (type == PCP_RWSEM) {
+					if (atomic_read(&ots->lkinfo.lock_depth) < 0) {
+						//pr_err("krn_reliab : why lock depth less than 0?\n");
+					}
+				}
+				atomic_dec(&ots->lkinfo.lock_depth);
+				list_del(&node->node);
+				depth_check(ots, 1);
+				kfree(node);
+			} else {
+				break;
+			}
+		}
+	}
+
+	node = kmalloc(sizeof(*node), GFP_ATOMIC);
+	if (!node) {
+		pr_err("krn_reliab : Failed to alloc record node\n");
+		return;
+	}
+	node->lock_id = lock_id;
+	node->type = type;
+	node->start_time = sched_clock();
+	stack_trace_save(node->traces, SAVE_TRACE_NUMS, 2);
+	list_add(&node->node, &ots->lkinfo.lock_head);
+
+	atomic_inc(&ots->lkinfo.lock_depth);
+	depth_check(ots, 2);
+	if (atomic_read(&max_depth) < atomic_read(&ots->lkinfo.lock_depth)) {
+		atomic_set(&max_depth, atomic_read(&ots->lkinfo.lock_depth));
+		pr_err("krn_reliab : update the max depth: %d, type = %s, lock_id = 0x%llx\n", atomic_read(&ots->lkinfo.lock_depth), lock_str[node->type], node->lock_id);
+		print_backtrace(2, 7);
+	}
+
+}
+
+static void unlock_handler(struct task_struct *tsk, u64 lock_id, int type)
+{
+	struct oplus_task_struct *ots;
+	struct lock_record *node, *tmp;
+	u64 duration;
+	int found = 0;
+
+	ots = get_oplus_task_struct(tsk);
+
+	if (IS_ERR_OR_NULL(ots)) {
+		pr_err("krn_reliab : ots == NULL \n");
+		return;
+	}
+	if (NULL == ots->lkinfo.lock_head.next)
+		INIT_LIST_HEAD(&ots->lkinfo.lock_head);
+
+	list_for_each_entry_safe(node, tmp, &ots->lkinfo.lock_head, node) {
+		if (lock_id == node->lock_id) {
+			duration = sched_clock() - node->start_time;
+			stats_cs_duration(type, duration);
+
+			/* Find corresponding lock !*/
+			found++;
+			/*
+			if (found >= 2) {
+				pr_err("krn_reliab : why found 2 unlocks, type = %s\n", lock_str[type]);
+				print_backtrace(3, 4);
+			} */
+			atomic_dec(&ots->lkinfo.lock_depth);
+			depth_check(ots, 3);
+			if (atomic_read(&ots->lkinfo.lock_depth) < 0) {
+				pr_err("krn_reliab : why lock depth less than 0?\n");
+			}
+
+			list_del(&node->node);
+			kfree(node);
+
+			/* f2fs_dquot_commit/f2fs_quota_sync will lock recursively rwsem(reader) with trylock. */
+			break;
+		}
+	}
+	if (0 == found) {
+		if (is_skip_report_error()) {
+			//pr_err("krn_reliab : skip report unlocked error, type =  %s\n", lock_str[type]);
+		} else {
+			pr_err("krn_reliab : can't find corresponding lock, type = %s, lock_id = 0x%llx, lock_stamp = %llu\n", lock_str[type], lock_id, sched_clock());
+			print_backtrace(2, 7);
+		}
+	}
+}
+
+void mutex_lock_handler(u64 lock, struct task_struct *tsk, unsigned long jiffies)
+{
+	if(!debug_enable_flag) return;
+
+	get_lock_cnts(jiffies, MUTEX);
+
+	if (jiffies) {
+		lock_handler(tsk, lock, MUTEX);
+	}
+	else {
+		unlock_handler(tsk, lock, MUTEX);
+	}
+
+}
+
+void rwsem_lock_handler(u64 sem, struct task_struct *tsk, unsigned long jiffies)
+{
+	if(!debug_enable_flag) return;
+	get_lock_cnts(jiffies, RWSEM);
+
+	if (jiffies) {	
+		lock_handler(tsk, (u64)sem, RWSEM);
+	}
+	else {
+		unlock_handler(tsk, (u64)sem, RWSEM);
+	}
+}
+
+void rtmutex_lock_handler(u64 lock, struct task_struct *tsk, unsigned long jiffies)
+{
+	if(!debug_enable_flag) return;
+	get_lock_cnts(jiffies, RTMUTEX);
+
+	if (jiffies)
+		lock_handler(tsk, (u64)lock, RTMUTEX);
+	else
+		unlock_handler(tsk, (u64)lock, RTMUTEX);
+}
+
+void android_vh_pcpu_rwsem_handler(u64 sem, struct task_struct *tsk, unsigned long jiffies)
+{
+	if(!debug_enable_flag) return;
+	get_lock_cnts(jiffies, PCP_RWSEM);
+
+/*
+	if (jiffies)
+		lock_handler(tsk, (u64)sem, PCP_RWSEM);
+	else
+		unlock_handler(tsk, (u64)sem, PCP_RWSEM);
+*/
+}
+EXPORT_SYMBOL(android_vh_pcpu_rwsem_handler);
+
+static int iter_rbtree_for_elem_nums(struct rb_root *root)
+{
+	int cnts = 0;
+	struct rb_node *node;
+
+	for (node = rb_first(root); node; node = rb_next(node))
+		cnts++;
+
+	return cnts;
+}
+
+unsigned long flags;
+static noinline int walk_list_for_elem_nums(struct list_head *head)
+{
+	atomic_t cnts;
+	struct list_head *node;
+
+	atomic_set(&cnts, 0);
+
+	list_for_each(node, head) {
+		atomic_inc(&cnts);
+	}
+	return atomic_read(&cnts);
+}
+
+/* We don't want to add a cnt field in lock struct.
+ * Just walk the list/rbtree to get wait list cnt;
+ */
+static void record_waiters_cnts(void *lock, int type)
+{
+	struct mutex *mlock;
+	struct rw_semaphore *rwsem;
+	struct rt_mutex_base *rtlock;
+	struct percpu_rw_semaphore *pcp_sem;
+
+	int max;
+	atomic_t cnt;
+	
+	atomic_set(&cnt, 0);
+	if (NULL == lock) {
+		printk("krn_reliab : lock is NULL\n");
+		return;
+	}
+
+	/* Only read, no need to acquire spinlock.*/
+	max = atomic64_read(&max_wait_cnts[type]);
+	switch (type) {
+	case MUTEX:
+		mlock = (struct mutex*)lock;
+		atomic_set(&cnt, walk_list_for_elem_nums(&mlock->wait_list));
+		break;
+	case RWSEM:
+		rwsem = (struct rw_semaphore *)lock;
+		atomic_set(&cnt, walk_list_for_elem_nums(&rwsem->wait_list));
+		break;
+	case RTMUTEX:
+		rtlock = (struct rt_mutex_base *)lock;
+		atomic_set(&cnt, iter_rbtree_for_elem_nums(&rtlock->waiters.rb_root));
+		break;
+	case PCP_RWSEM:
+		pcp_sem = (struct percpu_rw_semaphore *)lock;
+		spin_lock_irqsave(&pcp_sem->waiters.lock, flags);
+		atomic_set(&cnt, walk_list_for_elem_nums(&pcp_sem->waiters.head));
+		spin_unlock_irqrestore(&pcp_sem->waiters.lock, flags);
+		break;
+	default:
+		atomic_set(&cnt, 0);
+		break;
+	}
+
+	if (atomic_read(&cnt) > max) {
+		atomic64_set(&max_wait_cnts[type], atomic_read(&cnt));
+		pr_err("krn_reliab : update the max waiter_cnts: %d, type = %s, lock_id = 0x%lx\n", atomic_read(&cnt), lock_str[type], (unsigned long)lock);
+		print_backtrace(2, 7);
+	}
+}
+
+void mutex_wait_handler(struct mutex *lock)
+{
+	if(!debug_enable_flag) return;
+	/* Contend stats. */
+	atomic64_inc(&wait_cnt[MUTEX]);
+
+	/* Wait list nums stats. */
+	record_waiters_cnts(lock, MUTEX);
+}
+
+void rwsem_read_wait_handler(struct rw_semaphore *sem)
+{
+	if(!debug_enable_flag) return;
+	/* Contend stats. */
+	atomic64_inc(&wait_cnt[RWSEM]);
+
+	/* Wait list nums stats. */
+	raw_spin_lock(&sem->wait_lock);
+	record_waiters_cnts(sem, RWSEM);
+	// walk_list_for_elem_nums(&sem->wait_list);
+	raw_spin_unlock(&sem->wait_lock);
+}
+
+void rwsem_write_wait_handler(struct rw_semaphore *sem)
+{
+	/* Contend stats. */
+	atomic64_inc(&wait_cnt[RWSEM]);
+
+	/* Wait list nums stats. */
+	record_waiters_cnts(sem, RWSEM);
+}
+
+void rtmutex_wait_handler(struct rt_mutex_base *lock)
+{
+	if(!debug_enable_flag) return;
+	/* Contend stats. */
+	atomic64_inc(&wait_cnt[RTMUTEX]);
+
+	/* Wait list nums stats. */
+	record_waiters_cnts(lock, RTMUTEX);
+}
+
+void pcp_wait_handler(struct percpu_rw_semaphore *sem, bool is_reader, int phase)
+{
+	if(!debug_enable_flag) return;
+	/* Contend stats. */
+	atomic64_inc(&wait_cnt[PCP_RWSEM]);
+
+	/* Wait list nums stats. */
+	record_waiters_cnts(sem, PCP_RWSEM);
+}
+
+/*
+static void schedule_handler(void *unused, struct task_struct *prev, struct task_struct *next, struct rq *rq)
+{
+
+}*/
+
+
+/***************************** switch in cs *******************************/
+static int switch_in_cs_show(struct seq_file *m, void *v)
+{
+	char buf[1024];
+	int idx = 0;
+	long long int total_lock_cnts = 0;
+	int i;
+
+	idx += sprintf(&buf[idx], "%-16s%-16s\n", "total", "switch_incs");
+
+	for (i = 0; i < LOCK_TYPES; i++) {
+		total_lock_cnts += atomic64_read(&lock_cnts[i]);
+	}
+
+	idx += sprintf(&buf[idx], "%-16lld%-16lld\n", total_lock_cnts, atomic64_read(&switch_in_cs_cnts));
+
+	sprintf(&buf[idx], "\n");
+
+	seq_printf(m, "%s\n", buf);
+
+	return 0;
+}
+
+static int switch_in_cs_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, switch_in_cs_show, PDE_DATA(inode));
+}
+
+static const struct proc_ops switch_in_cs_fops = {
+	.proc_open		= switch_in_cs_open,
+	.proc_read		= seq_read,
+	.proc_lseek		= seq_lseek,
+	.proc_release		= single_release,
+};
+
+/*****************************Contend Info *******************************/
+
+/***************************** max depth *******************************/
+static int max_depth_show(struct seq_file *m, void *v)
+{
+	char buf[1024];
+	int idx = 0;
+	// int i;
+
+	idx += sprintf(&buf[idx], "%-16s%-16d\n", "max_depth:", atomic_read(&max_depth));
+
+	/*
+	for (i = 0; i < LOCK_TYPES; i++) {
+		idx += sprintf(&buf[idx], "%-16s%-16d\n", lock_str[i], atomic_read(&max_depth[i]));
+	}
+	*/
+
+	seq_printf(m, "%s\n", buf);
+
+	return 0;
+}
+
+static int max_depth_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, max_depth_show, PDE_DATA(inode));
+}
+
+static const struct proc_ops max_depth_fops = {
+	.proc_open		= max_depth_open,
+	.proc_read		= seq_read,
+	.proc_lseek		= seq_lseek,
+	.proc_release		= single_release,
+};
+
+/***************************** max depth *******************************/
+
+
+/***************************** CS duration *******************************/
+#define STAMP_TO_UNIT_NUM(x)	(x / 1000000000 >= 1 ? (x / 1000000000) : (x / 1000000 >= 1 ? (x / 1000000) : (x / 1000)))
+#define STAMP_TO_NUM_UNIT(x)	(x / 1000000000 >= 1 ? "s" : (x / 1000000 >= 1 ? "ms" : "us"))
+
+static int cs_duration_show(struct seq_file *m, void *v)
+{
+	char *buf;
+	char tmp[50];
+	int i, j, idx = 0;
+
+	buf = kmalloc(8192, GFP_ATOMIC);
+	if (!buf)
+		return -ENOMEM;
+
+	idx += sprintf(&buf[idx], "%-16s", "");
+
+	for (i = 0; i < CS_STATS_LVLS; i++) {
+		if(i < CS_STATS_LVLS -1) {
+			sprintf(tmp, "%lu%s~%lu%s",
+					STAMP_TO_UNIT_NUM(scopes[i][0]),
+					STAMP_TO_NUM_UNIT(scopes[i][0]),
+					STAMP_TO_UNIT_NUM(scopes[i][1]),
+					STAMP_TO_NUM_UNIT(scopes[i][1]));
+		} else {
+			sprintf(tmp, "%lu%s~%s",
+					STAMP_TO_UNIT_NUM(scopes[i][0]),
+					STAMP_TO_NUM_UNIT(scopes[i][0]),
+					"UL_MAX");
+		}
+		idx += sprintf(&buf[idx], "%-12s", tmp);
+	}
+	idx += sprintf(&buf[idx], "\n");
+
+	for (i = 0; i < LOCK_TYPES; i++) {
+		idx += sprintf(&buf[idx], "%-16s", lock_str[i]);
+		for (j = 0; j < CS_STATS_LVLS; j++) {
+			idx += sprintf(&buf[idx], "%-12lld", atomic64_read(&cs_duration[i][j]));
+		}
+		idx += sprintf(&buf[idx], "\n");
+	}
+	seq_printf(m, "%s\n", buf);
+
+	kfree(buf);
+	return 0;
+}
+
+static int cs_duration_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, cs_duration_show, PDE_DATA(inode));
+}
+
+static const struct proc_ops cs_duration_fops = {
+	.proc_open		= cs_duration_open,
+	.proc_read		= seq_read,
+	.proc_lseek		= seq_lseek,
+	.proc_release		= single_release,
+};
+
+/***************************** CS duration *******************************/
+
+/***************************** Waiter cnts *******************************/
+static int waiter_cnts_show(struct seq_file *m, void *v)
+{
+	char waiter_cnts[1024];
+	int idx = 0;
+	int i;
+
+	idx += sprintf(&waiter_cnts[idx], "%-16s%-16s\n", "", "max_cnts");
+
+	for (i = 0; i < LOCK_TYPES; i++) {
+		idx += sprintf(&waiter_cnts[idx], "%-16s%-16lld\n", lock_str[i], atomic64_read(&max_wait_cnts[i]));
+	}
+
+	sprintf(&waiter_cnts[idx], "\n");
+
+	seq_printf(m, "%s\n", waiter_cnts);
+
+	return 0;
+}
+
+static int waiter_cnts_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, waiter_cnts_show, PDE_DATA(inode));
+}
+
+static const struct proc_ops waiter_cnts_fops = {
+	.proc_open		= waiter_cnts_open,
+	.proc_read		= seq_read,
+	.proc_lseek		= seq_lseek,
+	.proc_release		= single_release,
+};
+
+/***************************** Waiter cnts *******************************/
+
+/*****************************Contend Info *******************************/
+static int contend_infos_show(struct seq_file *m, void *v)
+{
+	char contend_info[1024];
+	int idx = 0;
+	int i;
+
+	idx += sprintf(&contend_info[idx], "%-16s%-16s%-16s\n", "", "total", "contend");
+
+	for (i = 0; i < LOCK_TYPES; i++) {
+		idx += sprintf(&contend_info[idx], "%-16s%-16lld%-16lld\n", lock_str[i], atomic64_read(&lock_cnts[i]), atomic64_read(&wait_cnt[i]));	
+	}
+
+	sprintf(&contend_info[idx], "\n");
+
+	seq_printf(m, "%s\n", contend_info);
+
+	return 0;
+}
+
+static int contend_infos_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, contend_infos_show, PDE_DATA(inode));
+}
+
+static const struct proc_ops contend_info_fops = {
+	.proc_open		= contend_infos_open,
+	.proc_read		= seq_read,
+	.proc_lseek		= seq_lseek,
+	.proc_release		= single_release,
+};
+
+/*****************************Contend Info *******************************/
+
+
+#define CONTEND_INFO_FILE	"contend_info"
+#define WAITER_CNTS		"waiter_cnts"
+#define CS_DURATION		"cs_duration"
+#define MAX_DEPTH		"max_depth"
+#define KRN_RELIAB_DIR		"boot_reliab_dir"
+#define SWITCH_IN_CS		"switch_in_cs"
+
+static struct proc_dir_entry *d_krn_reliab;
+
+static int create_proc_files(void)
+{
+	struct proc_dir_entry *p;
+
+	p = proc_create(CONTEND_INFO_FILE, S_IRUGO | S_IWUGO,
+			d_krn_reliab, &contend_info_fops);
+	if (!p)
+		goto err;
+
+	p = proc_create(WAITER_CNTS, S_IRUGO | S_IWUGO,
+			d_krn_reliab, &waiter_cnts_fops);
+	if (!p)
+		goto err1;
+
+	p = proc_create(CS_DURATION, S_IRUGO | S_IWUGO,
+			d_krn_reliab, &cs_duration_fops);
+	if (!p)
+		goto err2;
+
+	p = proc_create(MAX_DEPTH, S_IRUGO | S_IWUGO,
+			d_krn_reliab, &max_depth_fops);
+	if (!p)
+		goto err3;
+
+	p = proc_create(SWITCH_IN_CS, S_IRUGO | S_IWUGO,
+			d_krn_reliab, &switch_in_cs_fops);
+	if (!p)
+		goto err4;
+
+	return 0;
+err4:
+	remove_proc_entry(MAX_DEPTH, d_krn_reliab);
+err3:
+	remove_proc_entry(CS_DURATION, d_krn_reliab);
+err2:
+	remove_proc_entry(WAITER_CNTS, d_krn_reliab);
+err1:
+	remove_proc_entry(CONTEND_INFO_FILE, d_krn_reliab);
+err:
+	remove_proc_entry(KRN_RELIAB_DIR, NULL);
+	return -ENOMEM;
+}
+
+
+static void remove_proc_files(void)
+{
+	remove_proc_entry(SWITCH_IN_CS, d_krn_reliab);
+	remove_proc_entry(MAX_DEPTH, d_krn_reliab);
+	remove_proc_entry(CONTEND_INFO_FILE, d_krn_reliab);
+	remove_proc_entry(WAITER_CNTS, d_krn_reliab);
+	remove_proc_entry(CS_DURATION, d_krn_reliab);
+}
+
+static int register_driver(void)
+{
+	create_proc_files();
+	return 0;
+}
+
+static void unregister_driver(void)
+{
+	remove_proc_files();
+}
+
+static ssize_t kernel_reliab_enabled_write(struct file *file, const char __user *buf,
+		size_t count, loff_t *ppos)
+{
+	char buffer[8];
+	int err;
+
+	memset(buffer, 0, sizeof(buffer));
+
+	if (count > sizeof(buffer) - 1)
+		count = sizeof(buffer) - 1;
+
+	if (copy_from_user(buffer, buf, count))
+		return -EFAULT;
+
+	buffer[count] = '\0';
+	err = kstrtoint(strstrip(buffer), 10, &debug_enable_flag);
+	if (err)
+		return err;
+
+	if(debug_enable_flag) {
+		if(init_done)
+			return count;
+		init_done = true;
+		register_driver();
+		init_clear_lock_hold_timer();
+	} else {
+		init_done = false;
+		unregister_driver();
+		exit_clear_lock_hold_timer();
+	}
+
+	return count;
+}
+
+static ssize_t kernel_reliab_enabled_read(struct file *file, char __user *buf,
+		size_t count, loff_t *ppos)
+{
+	char buffer[20];
+	size_t len = 0;
+
+	len = snprintf(buffer, sizeof(buffer), "debug_enable_flag=%d\n", debug_enable_flag);
+
+	return simple_read_from_buffer(buf, count, ppos, buffer, len);
+}
+
+static const struct proc_ops kernel_reliab_enabled_fops = {
+	.proc_write		= kernel_reliab_enabled_write,
+	.proc_read		= kernel_reliab_enabled_read,
+};
+
+static int __init krn_reliab_init(void)
+{
+	struct proc_dir_entry *proc_node;
+
+	d_krn_reliab = proc_mkdir(KRN_RELIAB_DIR, NULL);
+	if (!d_krn_reliab)
+		return -ENOMEM;
+
+	proc_node = proc_create("debug_enable", 0666, d_krn_reliab, &kernel_reliab_enabled_fops);
+	if (!proc_node) {
+		pr_err("failed to create proc node debug_enable_flag\n");
+		goto err_creat_debug_enable_flag;
+	}
+
+	return 0;
+
+err_creat_debug_enable_flag:
+	remove_proc_entry("debug_enable_flag", d_krn_reliab);
+	remove_proc_entry(KRN_RELIAB_DIR, NULL);
+	return 0;
+}
+
+static void __exit krn_reliab_exit(void)
+{
+	exit_clear_lock_hold_timer();
+}
+
+module_init(krn_reliab_init);
+module_exit(krn_reliab_exit);
diff --git a/kernel/locking/sa_common_struct.h b/kernel/locking/sa_common_struct.h
new file mode 100644
index 000000000..1d4b2f995
--- /dev/null
+++ b/kernel/locking/sa_common_struct.h
@@ -0,0 +1,181 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Copyright (C) 2020-2022 Oplus. All rights reserved.
+ */
+
+/*
+this file is splited from the sa_common.h to adapt the OKI,
+IS_ENABLED is not allowed in here, because the macro will not work in OKI.
+*/
+#ifndef _OPLUS_SA_COMMON_STRUCT_H_
+#define _OPLUS_SA_COMMON_STRUCT_H_
+
+#define MAX_CLUSTER            (4)
+
+/*#if IS_ENABLED(CONFIG_OPLUS_FEATURE_CPU_JANKINFO)*/
+
+#define MAX_TASK_COMM_LEN 256
+struct uid_struct {
+	uid_t uid;
+	u64 uid_total_cycle;
+	u64 uid_total_inst;
+	spinlock_t lock;
+	char leader_comm[TASK_COMM_LEN];
+	char cmdline[MAX_TASK_COMM_LEN];
+};
+
+struct  amu_uid_entry {
+	uid_t uid;
+	struct uid_struct *uid_struct;
+	struct hlist_node node;
+};
+
+/*#endif*/
+
+/*#if IS_ENABLED(CONFIG_OPLUS_LOCKING_STRATEGY)*/
+struct locking_info {
+	u64 waittime_stamp;
+	u64 holdtime_stamp;
+	/* Used in torture acquire latency statistic.*/
+	u64 acquire_stamp;
+	/*
+	 * mutex or rwsem optimistic spin start time. Because a task
+	 * can't spin both on mutex and rwsem at one time, use one common
+	 * threshold time is OK.
+	 */
+	u64 opt_spin_start_time;
+	struct task_struct *holder;
+	u32 waittype;
+	bool ux_contrib;
+	/*
+	 * Whether task is ux when it's going to be added to mutex or
+	 * rwsem waiter list. It helps us check whether there is ux
+	 * task on mutex or rwsem waiter list. Also, a task can't be
+	 * added to both mutex and rwsem at one time, so use one common
+	 * field is OK.
+	 */
+	bool is_block_ux;
+	u32 kill_flag;
+	/* for cfs enqueue smoothly.*/
+	struct list_head node;
+	struct task_struct *owner;
+	struct list_head lock_head;
+	u64 clear_seq;
+	atomic_t lock_depth;
+};
+/*#endif*/
+
+/* Please add your own members of task_struct here :) */
+struct oplus_task_struct {
+	/* CONFIG_OPLUS_FEATURE_SCHED_ASSIST */
+	struct rb_node ux_entry;
+	struct rb_node exec_time_node;
+	struct task_struct *task;
+	atomic64_t inherit_ux;
+	u64 enqueue_time;
+	u64 inherit_ux_start;
+	/* u64 sum_exec_baseline; */
+	u64 total_exec;
+	u64 vruntime;
+	u64 preset_vruntime;
+	s64 cfs_delta;
+	int ux_state;
+	u8 ux_depth;
+	s8 ux_priority;
+	s8 ux_nice;
+	unsigned long im_flag;
+	pid_t affinity_pid;
+	pid_t affinity_tgid;
+	unsigned long state;
+/*#if IS_ENABLED(CONFIG_OPLUS_FEATURE_ABNORMAL_FLAG)*/
+	int abnormal_flag;
+/*#endif*/
+	/* CONFIG_OPLUS_FEATURE_SCHED_SPREAD */
+	int lb_state;
+	int ld_flag:1;
+	/* CONFIG_OPLUS_FEATURE_TASK_LOAD */
+	int is_update_runtime:1;
+	int target_process;
+	u64 wake_tid;
+	u64 running_start_time;
+	bool update_running_start_time;
+	u64 exec_calc_runtime;
+	cpumask_t cpus_requested;
+/*#if IS_ENABLED(CONFIG_OPLUS_FEATURE_CPU_JANKINFO)*/
+	u64 block_start_time;
+/*#endif*/
+	/* CONFIG_OPLUS_FEATURE_FRAME_BOOST */
+	struct list_head fbg_list;
+	raw_spinlock_t fbg_list_entry_lock;
+	bool fbg_running; /* task belongs to a group, and in running */
+	u16 fbg_state;
+	s8 preferred_cluster_id;
+	s8 fbg_depth;
+	u64 last_wake_ts;
+	int fbg_cur_group;
+/*#ifdef CONFIG_LOCKING_PROTECT*/
+	unsigned long locking_start_time;
+	struct list_head locking_entry;
+	int locking_depth;
+	int lk_tick_hit;
+/*#endif*/
+
+/*#if IS_ENABLED(CONFIG_OPLUS_LOCKING_STRATEGY)*/
+	struct locking_info lkinfo;
+/*#endif*/
+/*#if IS_ENABLED(CONFIG_OPLUS_FEATURE_FDLEAK_CHECK)*/
+	u8 fdleak_flag;
+/*#endif*/
+
+/*#if IS_ENABLED(CONFIG_OPLUS_FEATURE_LOADBALANCE)*/
+	/* for loadbalance */
+	struct plist_node rtb;		/* rt boost task */
+
+	/*
+	 * The following variables are used to calculate the time
+	 * a task spends in the running/runnable state.
+	 */
+	u64 snap_run_delay;
+	unsigned long snap_pcount;
+/*#endif*/
+
+/*#if IS_ENABLED(CONFIG_OPLUS_FEATURE_PIPELINE)*/
+	atomic_t pipeline_cpu;
+/*#endif*/
+
+	/* for oplus secure guard */
+	int sg_flag;
+	int sg_scno;
+	uid_t sg_uid;
+	uid_t sg_euid;
+	gid_t sg_gid;
+	gid_t sg_egid;
+/*#if IS_ENABLED(CONFIG_ARM64_AMU_EXTN) && IS_ENABLED(CONFIG_OPLUS_FEATURE_CPU_JANKINFO)*/
+	struct uid_struct *uid_struct;
+	u64 amu_instruct;
+	u64 amu_cycle;
+/*#endif*/
+	/* for binder ux */
+	int binder_async_ux_enable;
+	bool binder_async_ux_sts;
+	int binder_thread_mode;
+	struct binder_node *binder_thread_node;
+	/*
+	 * for vip binder:
+	 * vip_thread_policy_max_threads:for the max number of vip threads except binder_max_threads.
+	 * vip_save_threads:the vip thread saved in binder_max_threads
+	 */
+	int vip_thread_policy_max_threads;
+	int vip_save_threads;
+} ____cacheline_aligned;
+
+/*#if IS_ENABLED(CONFIG_OPLUS_FEATURE_LOADBALANCE)*/
+#define INVALID_PID						(-1)
+struct oplus_lb {
+	/* used for active_balance to record the running task. */
+	pid_t pid;
+};
+/*#endif*/
+
+#endif /* _OPLUS_SA_COMMON_STRUCT_H_ */
+
-- 
2.51.0

